{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3dee9ef-952f-480b-8242-536cf68f3d72",
   "metadata": {},
   "source": [
    "\n",
    "<h1>Tranformers<h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55bebde8-7c69-4464-a342-dc8006e4691d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.20.1-cp312-cp312-win_amd64.whl.metadata (6.2 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.5.1-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\jairo mendoza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jairo mendoza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jairo mendoza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\jairo mendoza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (2.1.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\jairo mendoza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jairo mendoza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading torch-2.5.1-cp312-cp312-win_amd64.whl (203.0 MB)\n",
      "   ---------------------------------------- 0.0/203.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/203.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/203.0 MB 1.3 MB/s eta 0:02:38\n",
      "   ---------------------------------------- 0.8/203.0 MB 1.5 MB/s eta 0:02:19\n",
      "   ---------------------------------------- 1.3/203.0 MB 1.8 MB/s eta 0:01:55\n",
      "   ---------------------------------------- 1.6/203.0 MB 1.8 MB/s eta 0:01:53\n",
      "   ---------------------------------------- 1.8/203.0 MB 1.8 MB/s eta 0:01:52\n",
      "   ---------------------------------------- 2.1/203.0 MB 1.7 MB/s eta 0:02:02\n",
      "    --------------------------------------- 2.6/203.0 MB 1.7 MB/s eta 0:02:00\n",
      "    --------------------------------------- 2.9/203.0 MB 1.7 MB/s eta 0:02:00\n",
      "    --------------------------------------- 3.4/203.0 MB 1.7 MB/s eta 0:01:58\n",
      "    --------------------------------------- 3.7/203.0 MB 1.6 MB/s eta 0:02:03\n",
      "    --------------------------------------- 3.9/203.0 MB 1.6 MB/s eta 0:02:06\n",
      "    --------------------------------------- 4.2/203.0 MB 1.6 MB/s eta 0:02:04\n",
      "    --------------------------------------- 4.7/203.0 MB 1.7 MB/s eta 0:01:59\n",
      "    --------------------------------------- 5.0/203.0 MB 1.6 MB/s eta 0:02:02\n",
      "   - -------------------------------------- 5.2/203.0 MB 1.6 MB/s eta 0:02:05\n",
      "   - -------------------------------------- 5.5/203.0 MB 1.6 MB/s eta 0:02:04\n",
      "   - -------------------------------------- 5.8/203.0 MB 1.6 MB/s eta 0:02:04\n",
      "   - -------------------------------------- 6.3/203.0 MB 1.6 MB/s eta 0:02:03\n",
      "   - -------------------------------------- 6.6/203.0 MB 1.6 MB/s eta 0:02:02\n",
      "   - -------------------------------------- 6.8/203.0 MB 1.6 MB/s eta 0:02:02\n",
      "   - -------------------------------------- 7.1/203.0 MB 1.6 MB/s eta 0:02:03\n",
      "   - -------------------------------------- 7.3/203.0 MB 1.6 MB/s eta 0:02:05\n",
      "   - -------------------------------------- 7.9/203.0 MB 1.6 MB/s eta 0:02:03\n",
      "   - -------------------------------------- 8.1/203.0 MB 1.6 MB/s eta 0:02:04\n",
      "   - -------------------------------------- 8.4/203.0 MB 1.6 MB/s eta 0:02:03\n",
      "   - -------------------------------------- 8.9/203.0 MB 1.6 MB/s eta 0:02:01\n",
      "   - -------------------------------------- 9.4/203.0 MB 1.6 MB/s eta 0:01:59\n",
      "   - -------------------------------------- 10.0/203.0 MB 1.7 MB/s eta 0:01:57\n",
      "   -- ------------------------------------- 10.2/203.0 MB 1.7 MB/s eta 0:01:57\n",
      "   -- ------------------------------------- 10.7/203.0 MB 1.7 MB/s eta 0:01:56\n",
      "   -- ------------------------------------- 11.0/203.0 MB 1.7 MB/s eta 0:01:55\n",
      "   -- ------------------------------------- 11.5/203.0 MB 1.7 MB/s eta 0:01:54\n",
      "   -- ------------------------------------- 11.5/203.0 MB 1.7 MB/s eta 0:01:54\n",
      "   -- ------------------------------------- 11.5/203.0 MB 1.7 MB/s eta 0:01:54\n",
      "   -- ------------------------------------- 11.5/203.0 MB 1.7 MB/s eta 0:01:54\n",
      "   -- ------------------------------------- 11.5/203.0 MB 1.7 MB/s eta 0:01:54\n",
      "   -- ------------------------------------- 11.5/203.0 MB 1.7 MB/s eta 0:01:54\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 12.1/203.0 MB 1.1 MB/s eta 0:02:53\n",
      "   -- ------------------------------------- 12.1/203.0 MB 1.1 MB/s eta 0:02:53\n",
      "   -- ------------------------------------- 12.3/203.0 MB 1.1 MB/s eta 0:02:54\n",
      "   -- ------------------------------------- 12.6/203.0 MB 1.1 MB/s eta 0:02:54\n",
      "   -- ------------------------------------- 13.1/203.0 MB 1.1 MB/s eta 0:02:52\n",
      "   -- ------------------------------------- 13.4/203.0 MB 1.1 MB/s eta 0:02:50\n",
      "   -- ------------------------------------- 13.6/203.0 MB 1.1 MB/s eta 0:02:49\n",
      "   -- ------------------------------------- 14.2/203.0 MB 1.1 MB/s eta 0:02:47\n",
      "   -- ------------------------------------- 14.4/203.0 MB 1.1 MB/s eta 0:02:45\n",
      "   -- ------------------------------------- 14.7/203.0 MB 1.1 MB/s eta 0:02:45\n",
      "   -- ------------------------------------- 14.9/203.0 MB 1.1 MB/s eta 0:02:44\n",
      "   --- ------------------------------------ 15.5/203.0 MB 1.2 MB/s eta 0:02:42\n",
      "   --- ------------------------------------ 15.7/203.0 MB 1.2 MB/s eta 0:02:41\n",
      "   --- ------------------------------------ 16.3/203.0 MB 1.2 MB/s eta 0:02:38\n",
      "   --- ------------------------------------ 16.8/203.0 MB 1.2 MB/s eta 0:02:35\n",
      "   --- ------------------------------------ 17.0/203.0 MB 1.2 MB/s eta 0:02:34\n",
      "   --- ------------------------------------ 17.3/203.0 MB 1.2 MB/s eta 0:02:33\n",
      "   --- ------------------------------------ 17.8/203.0 MB 1.2 MB/s eta 0:02:32\n",
      "   --- ------------------------------------ 18.1/203.0 MB 1.2 MB/s eta 0:02:31\n",
      "   --- ------------------------------------ 18.4/203.0 MB 1.2 MB/s eta 0:02:31\n",
      "   --- ------------------------------------ 18.6/203.0 MB 1.2 MB/s eta 0:02:30\n",
      "   --- ------------------------------------ 18.9/203.0 MB 1.2 MB/s eta 0:02:30\n",
      "   --- ------------------------------------ 19.1/203.0 MB 1.2 MB/s eta 0:02:29\n",
      "   --- ------------------------------------ 19.7/203.0 MB 1.2 MB/s eta 0:02:28\n",
      "   --- ------------------------------------ 19.9/203.0 MB 1.2 MB/s eta 0:02:28\n",
      "   --- ------------------------------------ 20.2/203.0 MB 1.2 MB/s eta 0:02:27\n",
      "   ---- ----------------------------------- 20.7/203.0 MB 1.3 MB/s eta 0:02:25\n",
      "   ---- ----------------------------------- 21.0/203.0 MB 1.3 MB/s eta 0:02:25\n",
      "   ---- ----------------------------------- 21.5/203.0 MB 1.3 MB/s eta 0:02:23\n",
      "   ---- ----------------------------------- 21.8/203.0 MB 1.3 MB/s eta 0:02:22\n",
      "   ---- ----------------------------------- 22.0/203.0 MB 1.3 MB/s eta 0:02:22\n",
      "   ---- ----------------------------------- 22.3/203.0 MB 1.3 MB/s eta 0:02:22\n",
      "   ---- ----------------------------------- 22.8/203.0 MB 1.3 MB/s eta 0:02:20\n",
      "   ---- ----------------------------------- 23.1/203.0 MB 1.3 MB/s eta 0:02:20\n",
      "   ---- ----------------------------------- 23.3/203.0 MB 1.3 MB/s eta 0:02:20\n",
      "   ---- ----------------------------------- 23.6/203.0 MB 1.3 MB/s eta 0:02:20\n",
      "   ---- ----------------------------------- 23.9/203.0 MB 1.3 MB/s eta 0:02:19\n",
      "   ---- ----------------------------------- 24.1/203.0 MB 1.3 MB/s eta 0:02:19\n",
      "   ---- ----------------------------------- 24.4/203.0 MB 1.3 MB/s eta 0:02:19\n",
      "   ---- ----------------------------------- 24.6/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ---- ----------------------------------- 25.2/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 25.4/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 25.7/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 26.0/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 26.2/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 26.2/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 26.5/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 26.7/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 27.0/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 27.3/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 27.8/203.0 MB 1.3 MB/s eta 0:02:17\n",
      "   ----- ---------------------------------- 28.0/203.0 MB 1.3 MB/s eta 0:02:16\n",
      "   ----- ---------------------------------- 28.3/203.0 MB 1.3 MB/s eta 0:02:16\n",
      "   ----- ---------------------------------- 28.3/203.0 MB 1.3 MB/s eta 0:02:16\n",
      "   ----- ---------------------------------- 28.6/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 28.6/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 28.8/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 28.8/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 28.8/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 28.8/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 28.8/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 29.1/203.0 MB 1.2 MB/s eta 0:02:24\n",
      "   ----- ---------------------------------- 29.4/203.0 MB 1.2 MB/s eta 0:02:23\n",
      "   ----- ---------------------------------- 29.9/203.0 MB 1.2 MB/s eta 0:02:22\n",
      "   ----- ---------------------------------- 30.4/203.0 MB 1.2 MB/s eta 0:02:20\n",
      "   ------ --------------------------------- 30.7/203.0 MB 1.2 MB/s eta 0:02:20\n",
      "   ------ --------------------------------- 30.9/203.0 MB 1.2 MB/s eta 0:02:20\n",
      "   ------ --------------------------------- 31.2/203.0 MB 1.2 MB/s eta 0:02:19\n",
      "   ------ --------------------------------- 31.5/203.0 MB 1.2 MB/s eta 0:02:19\n",
      "   ------ --------------------------------- 32.0/203.0 MB 1.2 MB/s eta 0:02:18\n",
      "   ------ --------------------------------- 32.2/203.0 MB 1.2 MB/s eta 0:02:18\n",
      "   ------ --------------------------------- 32.5/203.0 MB 1.2 MB/s eta 0:02:17\n",
      "   ------ --------------------------------- 33.0/203.0 MB 1.3 MB/s eta 0:02:16\n",
      "   ------ --------------------------------- 33.3/203.0 MB 1.3 MB/s eta 0:02:16\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 34.1/203.0 MB 1.1 MB/s eta 0:02:30\n",
      "   ------ --------------------------------- 34.1/203.0 MB 1.1 MB/s eta 0:02:30\n",
      "   ------ --------------------------------- 34.3/203.0 MB 1.1 MB/s eta 0:02:32\n",
      "   ------ --------------------------------- 34.6/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------ --------------------------------- 34.9/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------ --------------------------------- 35.1/203.0 MB 1.1 MB/s eta 0:02:32\n",
      "   ------ --------------------------------- 35.4/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 35.7/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 35.9/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 36.2/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 36.7/203.0 MB 1.1 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 37.0/203.0 MB 1.1 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 37.2/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 37.5/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 37.7/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 38.0/203.0 MB 1.1 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 38.3/203.0 MB 1.1 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 38.5/203.0 MB 1.1 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 38.8/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 39.3/203.0 MB 1.1 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 39.3/203.0 MB 1.1 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 39.6/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 39.6/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 39.6/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 39.6/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 39.6/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 39.8/203.0 MB 1.0 MB/s eta 0:02:42\n",
      "   ------- -------------------------------- 39.8/203.0 MB 1.0 MB/s eta 0:02:42\n",
      "   ------- -------------------------------- 39.8/203.0 MB 1.0 MB/s eta 0:02:42\n",
      "   ------- -------------------------------- 39.8/203.0 MB 1.0 MB/s eta 0:02:42\n",
      "   ------- ------------------------------- 40.1/203.0 MB 972.5 kB/s eta 0:02:48\n",
      "   ------- ------------------------------- 40.4/203.0 MB 963.2 kB/s eta 0:02:49\n",
      "   ------- ------------------------------- 40.6/203.0 MB 997.0 kB/s eta 0:02:43\n",
      "   -------- ------------------------------- 40.9/203.0 MB 1.0 MB/s eta 0:02:42\n",
      "   -------- ------------------------------- 41.4/203.0 MB 1.0 MB/s eta 0:02:41\n",
      "   -------- ------------------------------- 41.7/203.0 MB 1.0 MB/s eta 0:02:41\n",
      "   -------- ------------------------------- 41.9/203.0 MB 1.0 MB/s eta 0:02:40\n",
      "   -------- ------------------------------- 42.2/203.0 MB 1.1 MB/s eta 0:02:26\n",
      "   -------- ------------------------------- 42.5/203.0 MB 1.1 MB/s eta 0:02:26\n",
      "   -------- ------------------------------- 42.7/203.0 MB 1.1 MB/s eta 0:02:25\n",
      "   -------- ------------------------------- 43.0/203.0 MB 1.1 MB/s eta 0:02:25\n",
      "   -------- ------------------------------- 43.3/203.0 MB 1.1 MB/s eta 0:02:24\n",
      "   -------- ------------------------------- 43.5/203.0 MB 1.1 MB/s eta 0:02:24\n",
      "   -------- ------------------------------- 43.8/203.0 MB 1.1 MB/s eta 0:02:24\n",
      "   -------- ------------------------------- 44.0/203.0 MB 1.1 MB/s eta 0:02:24\n",
      "   -------- ------------------------------- 44.3/203.0 MB 1.1 MB/s eta 0:02:23\n",
      "   -------- ------------------------------- 44.6/203.0 MB 1.1 MB/s eta 0:02:23\n",
      "   -------- ------------------------------- 44.8/203.0 MB 1.1 MB/s eta 0:02:23\n",
      "   -------- ------------------------------- 45.1/203.0 MB 1.1 MB/s eta 0:02:22\n",
      "   -------- ------------------------------- 45.4/203.0 MB 1.1 MB/s eta 0:02:22\n",
      "   --------- ------------------------------ 45.9/203.0 MB 1.1 MB/s eta 0:02:20\n",
      "   --------- ------------------------------ 46.1/203.0 MB 1.1 MB/s eta 0:02:20\n",
      "   --------- ------------------------------ 46.4/203.0 MB 1.1 MB/s eta 0:02:19\n",
      "   --------- ------------------------------ 46.7/203.0 MB 1.1 MB/s eta 0:02:19\n",
      "   --------- ------------------------------ 47.2/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   --------- ------------------------------ 47.4/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   --------- ------------------------------ 47.7/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   --------- ------------------------------ 48.0/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   --------- ------------------------------ 48.5/203.0 MB 1.1 MB/s eta 0:02:17\n",
      "   --------- ------------------------------ 48.8/203.0 MB 1.1 MB/s eta 0:02:17\n",
      "   --------- ------------------------------ 49.3/203.0 MB 1.1 MB/s eta 0:02:16\n",
      "   --------- ------------------------------ 49.5/203.0 MB 1.1 MB/s eta 0:02:16\n",
      "   --------- ------------------------------ 49.8/203.0 MB 1.1 MB/s eta 0:02:17\n",
      "   --------- ------------------------------ 50.1/203.0 MB 1.1 MB/s eta 0:02:17\n",
      "   --------- ------------------------------ 50.3/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   --------- ------------------------------ 50.6/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 50.9/203.0 MB 1.1 MB/s eta 0:02:17\n",
      "   ---------- ----------------------------- 51.4/203.0 MB 1.1 MB/s eta 0:02:17\n",
      "   ---------- ----------------------------- 51.6/203.0 MB 1.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 51.9/203.0 MB 1.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 52.2/203.0 MB 1.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 52.4/203.0 MB 1.1 MB/s eta 0:02:15\n",
      "   ---------- ----------------------------- 52.7/203.0 MB 1.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 53.2/203.0 MB 1.1 MB/s eta 0:02:15\n",
      "   ---------- ----------------------------- 53.5/203.0 MB 1.1 MB/s eta 0:02:15\n",
      "   ---------- ----------------------------- 53.7/203.0 MB 1.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 54.0/203.0 MB 1.1 MB/s eta 0:02:15\n",
      "   ---------- ----------------------------- 54.3/203.0 MB 1.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 54.3/203.0 MB 1.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 54.5/203.0 MB 1.1 MB/s eta 0:02:17\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ---------------------------- 55.1/203.0 MB 972.5 kB/s eta 0:02:33\n",
      "   ---------- ---------------------------- 55.1/203.0 MB 972.5 kB/s eta 0:02:33\n",
      "   ---------- ---------------------------- 55.3/203.0 MB 967.7 kB/s eta 0:02:33\n",
      "   ---------- ---------------------------- 55.6/203.0 MB 968.8 kB/s eta 0:02:33\n",
      "   ---------- ---------------------------- 55.8/203.0 MB 968.3 kB/s eta 0:02:33\n",
      "   ---------- ---------------------------- 55.8/203.0 MB 968.3 kB/s eta 0:02:33\n",
      "   ---------- ---------------------------- 56.1/203.0 MB 962.2 kB/s eta 0:02:33\n",
      "   ---------- ---------------------------- 56.4/203.0 MB 955.4 kB/s eta 0:02:34\n",
      "   ---------- ---------------------------- 56.6/203.0 MB 953.4 kB/s eta 0:02:34\n",
      "   ---------- ---------------------------- 56.6/203.0 MB 953.4 kB/s eta 0:02:34\n",
      "   ---------- ---------------------------- 56.9/203.0 MB 956.2 kB/s eta 0:02:33\n",
      "   ---------- ---------------------------- 57.1/203.0 MB 957.4 kB/s eta 0:02:33\n",
      "   ----------- --------------------------- 57.4/203.0 MB 961.0 kB/s eta 0:02:32\n",
      "   ----------- --------------------------- 57.7/203.0 MB 963.7 kB/s eta 0:02:31\n",
      "   ----------- --------------------------- 57.7/203.0 MB 963.7 kB/s eta 0:02:31\n",
      "   ----------- --------------------------- 57.7/203.0 MB 963.7 kB/s eta 0:02:31\n",
      "   ----------- --------------------------- 57.9/203.0 MB 981.6 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 57.9/203.0 MB 981.6 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 58.2/203.0 MB 969.9 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 58.2/203.0 MB 969.9 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 58.2/203.0 MB 969.9 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 58.2/203.0 MB 969.9 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 58.5/203.0 MB 929.2 kB/s eta 0:02:36\n",
      "   ----------- --------------------------- 58.7/203.0 MB 921.3 kB/s eta 0:02:37\n",
      "   ----------- --------------------------- 58.7/203.0 MB 921.3 kB/s eta 0:02:37\n",
      "   ----------- --------------------------- 59.0/203.0 MB 915.0 kB/s eta 0:02:38\n",
      "   ----------- --------------------------- 59.0/203.0 MB 915.0 kB/s eta 0:02:38\n",
      "   ----------- --------------------------- 59.2/203.0 MB 909.7 kB/s eta 0:02:39\n",
      "   ----------- --------------------------- 59.5/203.0 MB 894.1 kB/s eta 0:02:41\n",
      "   ----------- --------------------------- 59.5/203.0 MB 894.1 kB/s eta 0:02:41\n",
      "   ----------- --------------------------- 59.8/203.0 MB 884.9 kB/s eta 0:02:42\n",
      "   ----------- --------------------------- 59.8/203.0 MB 884.9 kB/s eta 0:02:42\n",
      "   ----------- --------------------------- 60.0/203.0 MB 970.2 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 60.3/203.0 MB 968.1 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 60.3/203.0 MB 968.1 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 60.6/203.0 MB 966.1 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 60.8/203.0 MB 966.8 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 61.1/203.0 MB 967.6 kB/s eta 0:02:27\n",
      "   ----------- --------------------------- 61.1/203.0 MB 967.6 kB/s eta 0:02:27\n",
      "   ----------- --------------------------- 61.3/203.0 MB 962.4 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 61.3/203.0 MB 962.4 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 61.6/203.0 MB 958.5 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 61.6/203.0 MB 958.5 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 61.6/203.0 MB 958.5 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 61.9/203.0 MB 941.5 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 61.9/203.0 MB 941.5 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 61.9/203.0 MB 941.5 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 61.9/203.0 MB 941.5 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 61.9/203.0 MB 941.5 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 61.9/203.0 MB 941.5 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 62.1/203.0 MB 909.7 kB/s eta 0:02:35\n",
      "   ----------- --------------------------- 62.1/203.0 MB 909.7 kB/s eta 0:02:35\n",
      "   ----------- --------------------------- 62.1/203.0 MB 909.7 kB/s eta 0:02:35\n",
      "   ----------- --------------------------- 62.4/203.0 MB 895.0 kB/s eta 0:02:38\n",
      "   ------------ -------------------------- 62.7/203.0 MB 891.3 kB/s eta 0:02:38\n",
      "   ------------ -------------------------- 62.7/203.0 MB 891.3 kB/s eta 0:02:38\n",
      "   ------------ -------------------------- 62.9/203.0 MB 883.0 kB/s eta 0:02:39\n",
      "   ------------ -------------------------- 63.2/203.0 MB 882.6 kB/s eta 0:02:39\n",
      "   ------------ -------------------------- 63.4/203.0 MB 877.5 kB/s eta 0:02:40\n",
      "   ------------ -------------------------- 63.7/203.0 MB 877.5 kB/s eta 0:02:39\n",
      "   ------------ -------------------------- 64.2/203.0 MB 885.3 kB/s eta 0:02:37\n",
      "   ------------ -------------------------- 64.5/203.0 MB 885.3 kB/s eta 0:02:37\n",
      "   ------------ -------------------------- 64.7/203.0 MB 885.8 kB/s eta 0:02:37\n",
      "   ------------ -------------------------- 65.0/203.0 MB 883.0 kB/s eta 0:02:37\n",
      "   ------------ -------------------------- 65.0/203.0 MB 883.0 kB/s eta 0:02:37\n",
      "   ------------ -------------------------- 65.5/203.0 MB 875.7 kB/s eta 0:02:38\n",
      "   ------------ -------------------------- 65.8/203.0 MB 884.4 kB/s eta 0:02:36\n",
      "   ------------ -------------------------- 66.1/203.0 MB 889.5 kB/s eta 0:02:35\n",
      "   ------------ -------------------------- 66.3/203.0 MB 891.3 kB/s eta 0:02:34\n",
      "   ------------ -------------------------- 66.8/203.0 MB 923.6 kB/s eta 0:02:28\n",
      "   ------------ -------------------------- 67.1/203.0 MB 926.6 kB/s eta 0:02:27\n",
      "   ------------ -------------------------- 67.4/203.0 MB 929.1 kB/s eta 0:02:27\n",
      "   ------------ -------------------------- 67.6/203.0 MB 931.6 kB/s eta 0:02:26\n",
      "   ------------- ------------------------- 67.9/203.0 MB 948.0 kB/s eta 0:02:23\n",
      "   ------------- ------------------------- 68.4/203.0 MB 956.1 kB/s eta 0:02:21\n",
      "   ------------- ------------------------- 68.7/203.0 MB 959.5 kB/s eta 0:02:21\n",
      "   ------------- ------------------------- 68.9/203.0 MB 960.9 kB/s eta 0:02:20\n",
      "   ------------- ------------------------- 69.2/203.0 MB 965.7 kB/s eta 0:02:19\n",
      "   ------------- ------------------------- 69.7/203.0 MB 972.5 kB/s eta 0:02:18\n",
      "   ------------- ------------------------- 70.0/203.0 MB 972.5 kB/s eta 0:02:17\n",
      "   ------------- ------------------------- 70.5/203.0 MB 979.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 71.0/203.0 MB 984.3 kB/s eta 0:02:15\n",
      "   ------------- ------------------------- 71.3/203.0 MB 986.4 kB/s eta 0:02:14\n",
      "   ------------- ------------------------- 71.6/203.0 MB 988.4 kB/s eta 0:02:14\n",
      "   ------------- ------------------------- 71.6/203.0 MB 988.4 kB/s eta 0:02:14\n",
      "   ------------- ------------------------- 71.8/203.0 MB 983.3 kB/s eta 0:02:14\n",
      "   ------------- ------------------------- 71.8/203.0 MB 983.3 kB/s eta 0:02:14\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.6/203.0 MB 681.9 kB/s eta 0:03:12\n",
      "   ------------- ------------------------- 72.9/203.0 MB 673.2 kB/s eta 0:03:14\n",
      "   ------------- ------------------------- 72.9/203.0 MB 673.2 kB/s eta 0:03:14\n",
      "   -------------- ------------------------ 73.1/203.0 MB 665.5 kB/s eta 0:03:16\n",
      "   -------------- ------------------------ 73.1/203.0 MB 665.5 kB/s eta 0:03:16\n",
      "   -------------- ------------------------ 73.4/203.0 MB 656.4 kB/s eta 0:03:18\n",
      "   -------------- ------------------------ 73.7/203.0 MB 652.1 kB/s eta 0:03:19\n",
      "   -------------- ------------------------ 73.9/203.0 MB 651.0 kB/s eta 0:03:19\n",
      "   -------------- ------------------------ 74.2/203.0 MB 657.1 kB/s eta 0:03:17\n",
      "   -------------- ------------------------ 74.4/203.0 MB 662.6 kB/s eta 0:03:15\n",
      "   -------------- ------------------------ 75.0/203.0 MB 726.1 kB/s eta 0:02:57\n",
      "   -------------- ------------------------ 75.2/203.0 MB 731.1 kB/s eta 0:02:55\n",
      "   -------------- ------------------------ 75.8/203.0 MB 744.2 kB/s eta 0:02:52\n",
      "   -------------- ------------------------ 76.3/203.0 MB 755.0 kB/s eta 0:02:48\n",
      "   -------------- ------------------------ 76.5/203.0 MB 760.9 kB/s eta 0:02:47\n",
      "   -------------- ------------------------ 77.1/203.0 MB 771.8 kB/s eta 0:02:44\n",
      "   -------------- ------------------------ 77.1/203.0 MB 771.8 kB/s eta 0:02:44\n",
      "   -------------- ------------------------ 77.3/203.0 MB 771.7 kB/s eta 0:02:43\n",
      "   -------------- ------------------------ 77.6/203.0 MB 776.2 kB/s eta 0:02:42\n",
      "   -------------- ------------------------ 77.9/203.0 MB 778.9 kB/s eta 0:02:41\n",
      "   --------------- ----------------------- 78.1/203.0 MB 783.2 kB/s eta 0:02:40\n",
      "   --------------- ----------------------- 78.4/203.0 MB 786.3 kB/s eta 0:02:39\n",
      "   --------------- ----------------------- 78.6/203.0 MB 789.3 kB/s eta 0:02:38\n",
      "   --------------- ----------------------- 78.9/203.0 MB 792.2 kB/s eta 0:02:37\n",
      "   --------------- ----------------------- 79.2/203.0 MB 796.0 kB/s eta 0:02:36\n",
      "   --------------- ----------------------- 79.7/203.0 MB 800.6 kB/s eta 0:02:35\n",
      "   --------------- ----------------------- 80.0/203.0 MB 805.2 kB/s eta 0:02:33\n",
      "   --------------- ----------------------- 80.2/203.0 MB 811.1 kB/s eta 0:02:32\n",
      "   --------------- ----------------------- 80.2/203.0 MB 811.1 kB/s eta 0:02:32\n",
      "   --------------- ----------------------- 80.7/203.0 MB 809.0 kB/s eta 0:02:32\n",
      "   --------------- ----------------------- 81.3/203.0 MB 821.4 kB/s eta 0:02:29\n",
      "   --------------- ----------------------- 81.5/203.0 MB 823.1 kB/s eta 0:02:28\n",
      "   --------------- ----------------------- 81.8/203.0 MB 824.4 kB/s eta 0:02:28\n",
      "   --------------- ----------------------- 82.1/203.0 MB 826.1 kB/s eta 0:02:27\n",
      "   --------------- ----------------------- 82.3/203.0 MB 824.4 kB/s eta 0:02:27\n",
      "   --------------- ----------------------- 82.8/203.0 MB 843.7 kB/s eta 0:02:23\n",
      "   --------------- ----------------------- 83.1/203.0 MB 849.0 kB/s eta 0:02:22\n",
      "   ---------------- ---------------------- 83.6/203.0 MB 857.2 kB/s eta 0:02:20\n",
      "   ---------------- ---------------------- 83.9/203.0 MB 870.9 kB/s eta 0:02:17\n",
      "   ---------------- ---------------------- 84.1/203.0 MB 875.6 kB/s eta 0:02:16\n",
      "   ---------------- ---------------------- 84.7/203.0 MB 895.7 kB/s eta 0:02:13\n",
      "   ---------------- ---------------------- 84.9/203.0 MB 898.9 kB/s eta 0:02:12\n",
      "   ---------------- ---------------------- 84.9/203.0 MB 898.9 kB/s eta 0:02:12\n",
      "   ---------------- ---------------------- 85.2/203.0 MB 896.9 kB/s eta 0:02:12\n",
      "   ---------------- ---------------------- 85.5/203.0 MB 900.0 kB/s eta 0:02:11\n",
      "   ---------------- ---------------------- 85.5/203.0 MB 900.0 kB/s eta 0:02:11\n",
      "   ---------------- ---------------------- 85.5/203.0 MB 900.0 kB/s eta 0:02:11\n",
      "   ---------------- ---------------------- 85.5/203.0 MB 900.0 kB/s eta 0:02:11\n",
      "   ---------------- ---------------------- 85.5/203.0 MB 900.0 kB/s eta 0:02:11\n",
      "   ---------------- ---------------------- 85.5/203.0 MB 900.0 kB/s eta 0:02:11\n",
      "   ---------------- ---------------------- 85.7/203.0 MB 877.0 kB/s eta 0:02:14\n",
      "   ---------------- ---------------------- 85.7/203.0 MB 877.0 kB/s eta 0:02:14\n",
      "   ---------------- ---------------------- 85.7/203.0 MB 877.0 kB/s eta 0:02:14\n",
      "   ---------------- ---------------------- 85.7/203.0 MB 877.0 kB/s eta 0:02:14\n",
      "   ---------------- ---------------------- 86.0/203.0 MB 866.4 kB/s eta 0:02:16\n",
      "   ---------------- ---------------------- 86.2/203.0 MB 868.3 kB/s eta 0:02:15\n",
      "   ---------------- ---------------------- 86.8/203.0 MB 875.7 kB/s eta 0:02:13\n",
      "   ---------------- ---------------------- 86.8/203.0 MB 875.7 kB/s eta 0:02:13\n",
      "   ---------------- ---------------------- 87.3/203.0 MB 878.4 kB/s eta 0:02:12\n",
      "   ---------------- ---------------------- 87.6/203.0 MB 885.8 kB/s eta 0:02:11\n",
      "   ---------------- ---------------------- 87.8/203.0 MB 890.9 kB/s eta 0:02:10\n",
      "   ---------------- ---------------------- 88.3/203.0 MB 900.5 kB/s eta 0:02:08\n",
      "   ----------------- --------------------- 88.6/203.0 MB 905.2 kB/s eta 0:02:07\n",
      "   ----------------- --------------------- 89.1/203.0 MB 930.6 kB/s eta 0:02:03\n",
      "   ----------------- --------------------- 89.4/203.0 MB 935.0 kB/s eta 0:02:02\n",
      "   ----------------- --------------------- 89.9/203.0 MB 945.8 kB/s eta 0:02:00\n",
      "   ----------------- --------------------- 90.4/203.0 MB 954.0 kB/s eta 0:01:59\n",
      "   ----------------- --------------------- 90.7/203.0 MB 990.6 kB/s eta 0:01:54\n",
      "   ----------------- --------------------- 91.0/203.0 MB 994.3 kB/s eta 0:01:53\n",
      "   ----------------- --------------------- 91.2/203.0 MB 998.0 kB/s eta 0:01:53\n",
      "   ------------------ --------------------- 91.8/203.0 MB 1.0 MB/s eta 0:01:51\n",
      "   ------------------ --------------------- 92.0/203.0 MB 1.0 MB/s eta 0:01:51\n",
      "   ------------------ --------------------- 92.3/203.0 MB 1.0 MB/s eta 0:01:50\n",
      "   ------------------ --------------------- 92.8/203.0 MB 1.0 MB/s eta 0:01:48\n",
      "   ------------------ --------------------- 93.3/203.0 MB 1.0 MB/s eta 0:01:46\n",
      "   ------------------ --------------------- 93.6/203.0 MB 1.0 MB/s eta 0:01:45\n",
      "   ------------------ --------------------- 94.1/203.0 MB 1.1 MB/s eta 0:01:44\n",
      "   ------------------ --------------------- 94.4/203.0 MB 1.1 MB/s eta 0:01:43\n",
      "   ------------------ --------------------- 94.9/203.0 MB 1.1 MB/s eta 0:01:42\n",
      "   ------------------ --------------------- 95.4/203.0 MB 1.1 MB/s eta 0:01:40\n",
      "   ------------------ --------------------- 96.2/203.0 MB 1.1 MB/s eta 0:01:38\n",
      "   ------------------- -------------------- 96.5/203.0 MB 1.1 MB/s eta 0:01:38\n",
      "   ------------------- -------------------- 97.0/203.0 MB 1.1 MB/s eta 0:01:37\n",
      "   ------------------- -------------------- 97.5/203.0 MB 1.1 MB/s eta 0:01:36\n",
      "   ------------------- -------------------- 97.8/203.0 MB 1.1 MB/s eta 0:01:35\n",
      "   ------------------- -------------------- 98.0/203.0 MB 1.1 MB/s eta 0:01:35\n",
      "   ------------------- -------------------- 98.3/203.0 MB 1.1 MB/s eta 0:01:35\n",
      "   ------------------- -------------------- 98.8/203.0 MB 1.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 99.1/203.0 MB 1.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 99.4/203.0 MB 1.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 99.6/203.0 MB 1.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 99.9/203.0 MB 1.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 100.4/203.0 MB 1.1 MB/s eta 0:01:32\n",
      "   ------------------- -------------------- 100.7/203.0 MB 1.1 MB/s eta 0:01:32\n",
      "   ------------------- -------------------- 100.9/203.0 MB 1.1 MB/s eta 0:01:32\n",
      "   ------------------- -------------------- 101.4/203.0 MB 1.1 MB/s eta 0:01:31\n",
      "   -------------------- ------------------- 101.7/203.0 MB 1.1 MB/s eta 0:01:31\n",
      "   -------------------- ------------------- 102.0/203.0 MB 1.1 MB/s eta 0:01:31\n",
      "   -------------------- ------------------- 102.2/203.0 MB 1.1 MB/s eta 0:01:31\n",
      "   -------------------- ------------------- 102.8/203.0 MB 1.1 MB/s eta 0:01:29\n",
      "   -------------------- ------------------- 103.0/203.0 MB 1.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 103.0/203.0 MB 1.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 103.3/203.0 MB 1.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 103.3/203.0 MB 1.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 103.3/203.0 MB 1.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 103.3/203.0 MB 1.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 103.3/203.0 MB 1.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 103.3/203.0 MB 1.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 103.3/203.0 MB 1.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 103.5/203.0 MB 1.1 MB/s eta 0:01:35\n",
      "   -------------------- ------------------- 103.8/203.0 MB 1.2 MB/s eta 0:01:26\n",
      "   -------------------- ------------------- 104.1/203.0 MB 1.2 MB/s eta 0:01:26\n",
      "   -------------------- ------------------- 104.6/203.0 MB 1.2 MB/s eta 0:01:25\n",
      "   -------------------- ------------------- 104.9/203.0 MB 1.2 MB/s eta 0:01:24\n",
      "   -------------------- ------------------- 105.4/203.0 MB 1.2 MB/s eta 0:01:23\n",
      "   -------------------- ------------------- 105.6/203.0 MB 1.2 MB/s eta 0:01:23\n",
      "   -------------------- ------------------- 106.4/203.0 MB 1.2 MB/s eta 0:01:21\n",
      "   --------------------- ------------------ 106.7/203.0 MB 1.2 MB/s eta 0:01:21\n",
      "   --------------------- ------------------ 107.2/203.0 MB 1.2 MB/s eta 0:01:19\n",
      "   --------------------- ------------------ 108.0/203.0 MB 1.2 MB/s eta 0:01:18\n",
      "   --------------------- ------------------ 108.5/203.0 MB 1.2 MB/s eta 0:01:17\n",
      "   --------------------- ------------------ 109.1/203.0 MB 1.2 MB/s eta 0:01:16\n",
      "   --------------------- ------------------ 109.6/203.0 MB 1.3 MB/s eta 0:01:15\n",
      "   --------------------- ------------------ 110.1/203.0 MB 1.3 MB/s eta 0:01:14\n",
      "   --------------------- ------------------ 110.6/203.0 MB 1.4 MB/s eta 0:01:06\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   ---------------------- ----------------- 111.9/203.0 MB 1.4 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 111.9/203.0 MB 1.4 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 112.2/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 112.2/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 112.5/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 112.5/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 112.5/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 112.7/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 112.7/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 113.0/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 113.0/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 113.2/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 113.2/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 113.5/203.0 MB 1.3 MB/s eta 0:01:09\n",
      "   ---------------------- ----------------- 113.5/203.0 MB 1.3 MB/s eta 0:01:09\n",
      "   ---------------------- ----------------- 113.8/203.0 MB 1.3 MB/s eta 0:01:10\n",
      "   ---------------------- ----------------- 113.8/203.0 MB 1.3 MB/s eta 0:01:10\n",
      "   ---------------------- ----------------- 113.8/203.0 MB 1.3 MB/s eta 0:01:10\n",
      "   ---------------------- ----------------- 113.8/203.0 MB 1.3 MB/s eta 0:01:10\n",
      "   ---------------------- ----------------- 113.8/203.0 MB 1.3 MB/s eta 0:01:10\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.3/203.0 MB 1.1 MB/s eta 0:01:24\n",
      "   ---------------------- ----------------- 114.6/203.0 MB 1.0 MB/s eta 0:01:25\n",
      "   ---------------------- ----------------- 114.8/203.0 MB 1.0 MB/s eta 0:01:25\n",
      "   ---------------------- ----------------- 115.1/203.0 MB 1.0 MB/s eta 0:01:24\n",
      "   ---------------------- ----------------- 115.6/203.0 MB 1.0 MB/s eta 0:01:24\n",
      "   ---------------------- ----------------- 115.9/203.0 MB 1.1 MB/s eta 0:01:23\n",
      "   ---------------------- ----------------- 116.4/203.0 MB 1.1 MB/s eta 0:01:23\n",
      "   ---------------------- ----------------- 116.7/203.0 MB 1.1 MB/s eta 0:01:22\n",
      "   ----------------------- ---------------- 116.9/203.0 MB 1.1 MB/s eta 0:01:21\n",
      "   ----------------------- ---------------- 116.9/203.0 MB 1.1 MB/s eta 0:01:21\n",
      "   ----------------------- ---------------- 116.9/203.0 MB 1.1 MB/s eta 0:01:21\n",
      "   ----------------------- ---------------- 116.9/203.0 MB 1.1 MB/s eta 0:01:21\n",
      "   ----------------------- ---------------- 117.2/203.0 MB 1.1 MB/s eta 0:01:20\n",
      "   ----------------------- ---------------- 117.2/203.0 MB 1.1 MB/s eta 0:01:20\n",
      "   ----------------------- ---------------- 117.2/203.0 MB 1.1 MB/s eta 0:01:20\n",
      "   ----------------------- ---------------- 117.7/203.0 MB 1.1 MB/s eta 0:01:20\n",
      "   ----------------------- ---------------- 118.0/203.0 MB 1.1 MB/s eta 0:01:18\n",
      "   ----------------------- ---------------- 118.2/203.0 MB 1.1 MB/s eta 0:01:18\n",
      "   ----------------------- ---------------- 118.8/203.0 MB 1.1 MB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 119.0/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 119.3/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 119.5/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 119.8/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 119.8/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.1/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.3/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ---------------------- --------------- 120.8/203.0 MB 980.7 kB/s eta 0:01:24\n",
      "   ---------------------- --------------- 120.8/203.0 MB 980.7 kB/s eta 0:01:24\n",
      "   ---------------------- --------------- 121.1/203.0 MB 963.2 kB/s eta 0:01:26\n",
      "   ---------------------- --------------- 121.4/203.0 MB 946.7 kB/s eta 0:01:27\n",
      "   ---------------------- --------------- 121.6/203.0 MB 937.9 kB/s eta 0:01:27\n",
      "   ---------------------- --------------- 121.9/203.0 MB 935.4 kB/s eta 0:01:27\n",
      "   ---------------------- --------------- 121.9/203.0 MB 935.4 kB/s eta 0:01:27\n",
      "   ---------------------- --------------- 122.2/203.0 MB 920.4 kB/s eta 0:01:28\n",
      "   ---------------------- --------------- 122.2/203.0 MB 920.4 kB/s eta 0:01:28\n",
      "   ---------------------- --------------- 122.2/203.0 MB 920.4 kB/s eta 0:01:28\n",
      "   ---------------------- --------------- 122.2/203.0 MB 920.4 kB/s eta 0:01:28\n",
      "   ---------------------- --------------- 122.2/203.0 MB 920.4 kB/s eta 0:01:28\n",
      "   ---------------------- --------------- 122.2/203.0 MB 920.4 kB/s eta 0:01:28\n",
      "   ---------------------- --------------- 122.2/203.0 MB 920.4 kB/s eta 0:01:28\n",
      "   ---------------------- --------------- 122.2/203.0 MB 920.4 kB/s eta 0:01:28\n",
      "   ---------------------- --------------- 122.4/203.0 MB 803.9 kB/s eta 0:01:41\n",
      "   ---------------------- --------------- 122.7/203.0 MB 796.4 kB/s eta 0:01:41\n",
      "   ---------------------- --------------- 122.7/203.0 MB 796.4 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 123.2/203.0 MB 791.0 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 123.2/203.0 MB 791.0 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 123.7/203.0 MB 786.4 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 124.0/203.0 MB 782.2 kB/s eta 0:01:42\n",
      "   ----------------------- -------------- 124.3/203.0 MB 786.9 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 124.5/203.0 MB 779.7 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 124.8/203.0 MB 773.8 kB/s eta 0:01:42\n",
      "   ----------------------- -------------- 125.0/203.0 MB 778.1 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 125.3/203.0 MB 772.6 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 125.6/203.0 MB 769.8 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 125.6/203.0 MB 769.8 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 125.8/203.0 MB 761.4 kB/s eta 0:01:42\n",
      "   ----------------------- -------------- 126.4/203.0 MB 769.4 kB/s eta 0:01:40\n",
      "   ----------------------- -------------- 126.6/203.0 MB 809.4 kB/s eta 0:01:35\n",
      "   ----------------------- -------------- 126.9/203.0 MB 812.8 kB/s eta 0:01:34\n",
      "   ----------------------- -------------- 127.4/203.0 MB 822.6 kB/s eta 0:01:32\n",
      "   ----------------------- -------------- 127.7/203.0 MB 828.1 kB/s eta 0:01:32\n",
      "   ----------------------- -------------- 127.9/203.0 MB 831.7 kB/s eta 0:01:31\n",
      "   ------------------------ ------------- 128.5/203.0 MB 840.6 kB/s eta 0:01:29\n",
      "   ------------------------ ------------- 128.7/203.0 MB 846.4 kB/s eta 0:01:28\n",
      "   ------------------------ ------------- 128.7/203.0 MB 846.4 kB/s eta 0:01:28\n",
      "   ------------------------ ------------- 128.7/203.0 MB 846.4 kB/s eta 0:01:28\n",
      "   ------------------------ ------------- 129.2/203.0 MB 831.4 kB/s eta 0:01:29\n",
      "   ------------------------ ------------- 129.5/203.0 MB 824.4 kB/s eta 0:01:30\n",
      "   ------------------------ ------------- 129.8/203.0 MB 821.8 kB/s eta 0:01:30\n",
      "   ------------------------ ------------- 130.0/203.0 MB 816.0 kB/s eta 0:01:30\n",
      "   ------------------------ ------------- 130.3/203.0 MB 813.5 kB/s eta 0:01:30\n",
      "   ------------------------ ------------- 130.5/203.0 MB 804.7 kB/s eta 0:01:31\n",
      "   ------------------------ ------------- 130.8/203.0 MB 796.8 kB/s eta 0:01:31\n",
      "   ------------------------ ------------- 131.1/203.0 MB 788.9 kB/s eta 0:01:32\n",
      "   ------------------------ ------------- 131.6/203.0 MB 778.5 kB/s eta 0:01:32\n",
      "   ------------------------ ------------- 131.9/203.0 MB 779.3 kB/s eta 0:01:32\n",
      "   ------------------------ ------------- 132.4/203.0 MB 770.6 kB/s eta 0:01:32\n",
      "   ------------------------ ------------- 132.9/203.0 MB 770.5 kB/s eta 0:01:32\n",
      "   ------------------------ ------------- 133.4/203.0 MB 770.5 kB/s eta 0:01:31\n",
      "   ------------------------- ------------ 134.0/203.0 MB 769.8 kB/s eta 0:01:30\n",
      "   ------------------------- ------------ 134.5/203.0 MB 769.8 kB/s eta 0:01:30\n",
      "   ------------------------- ------------ 135.0/203.0 MB 830.0 kB/s eta 0:01:22\n",
      "   ------------------------- ------------ 135.5/203.0 MB 841.6 kB/s eta 0:01:21\n",
      "   ------------------------- ------------ 136.1/203.0 MB 853.5 kB/s eta 0:01:19\n",
      "   ------------------------- ------------ 136.3/203.0 MB 858.9 kB/s eta 0:01:18\n",
      "   ------------------------- ------------ 136.8/203.0 MB 868.7 kB/s eta 0:01:17\n",
      "   ------------------------- ------------ 137.1/203.0 MB 873.5 kB/s eta 0:01:16\n",
      "   ------------------------- ------------ 137.6/203.0 MB 883.0 kB/s eta 0:01:15\n",
      "   ------------------------- ------------ 137.9/203.0 MB 884.9 kB/s eta 0:01:14\n",
      "   ------------------------- ------------ 138.1/203.0 MB 890.0 kB/s eta 0:01:13\n",
      "   ------------------------- ------------ 138.4/203.0 MB 893.1 kB/s eta 0:01:13\n",
      "   ------------------------- ------------ 138.7/203.0 MB 891.9 kB/s eta 0:01:13\n",
      "   ------------------------- ------------ 138.7/203.0 MB 891.9 kB/s eta 0:01:13\n",
      "   ------------------------- ------------ 138.7/203.0 MB 891.9 kB/s eta 0:01:13\n",
      "   ------------------------- ------------ 138.7/203.0 MB 891.9 kB/s eta 0:01:13\n",
      "   ------------------------- ------------ 138.7/203.0 MB 891.9 kB/s eta 0:01:13\n",
      "   ------------------------- ------------ 138.7/203.0 MB 891.9 kB/s eta 0:01:13\n",
      "   ------------------------- ------------ 138.7/203.0 MB 891.9 kB/s eta 0:01:13\n",
      "   -------------------------- ----------- 138.9/203.0 MB 867.3 kB/s eta 0:01:14\n",
      "   -------------------------- ----------- 139.2/203.0 MB 877.9 kB/s eta 0:01:13\n",
      "   -------------------------- ----------- 139.7/203.0 MB 888.1 kB/s eta 0:01:12\n",
      "   -------------------------- ----------- 140.2/203.0 MB 905.0 kB/s eta 0:01:10\n",
      "   -------------------------- ----------- 140.5/203.0 MB 911.0 kB/s eta 0:01:09\n",
      "   -------------------------- ----------- 141.0/203.0 MB 918.5 kB/s eta 0:01:08\n",
      "   -------------------------- ----------- 141.6/203.0 MB 933.1 kB/s eta 0:01:06\n",
      "   -------------------------- ----------- 142.1/203.0 MB 944.7 kB/s eta 0:01:05\n",
      "   -------------------------- ----------- 142.6/203.0 MB 983.7 kB/s eta 0:01:02\n",
      "   ---------------------------- ----------- 143.4/203.0 MB 1.0 MB/s eta 0:01:00\n",
      "   ---------------------------- ----------- 144.2/203.0 MB 1.0 MB/s eta 0:00:58\n",
      "   ---------------------------- ----------- 144.7/203.0 MB 1.0 MB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 145.5/203.0 MB 1.1 MB/s eta 0:00:55\n",
      "   ---------------------------- ----------- 145.8/203.0 MB 1.2 MB/s eta 0:00:48\n",
      "   ---------------------------- ----------- 146.3/203.0 MB 1.2 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 146.8/203.0 MB 1.2 MB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 147.3/203.0 MB 1.2 MB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 147.8/203.0 MB 1.2 MB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 148.1/203.0 MB 1.3 MB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 148.6/203.0 MB 1.3 MB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 148.9/203.0 MB 1.3 MB/s eta 0:00:43\n",
      "   ----------------------------- ---------- 149.2/203.0 MB 1.3 MB/s eta 0:00:43\n",
      "   ----------------------------- ---------- 149.7/203.0 MB 1.3 MB/s eta 0:00:43\n",
      "   ----------------------------- ---------- 149.9/203.0 MB 1.3 MB/s eta 0:00:42\n",
      "   ----------------------------- ---------- 150.5/203.0 MB 1.3 MB/s eta 0:00:42\n",
      "   ----------------------------- ---------- 150.7/203.0 MB 1.3 MB/s eta 0:00:42\n",
      "   ----------------------------- ---------- 150.7/203.0 MB 1.3 MB/s eta 0:00:42\n",
      "   ----------------------------- ---------- 151.0/203.0 MB 1.3 MB/s eta 0:00:41\n",
      "   ----------------------------- ---------- 151.5/203.0 MB 1.3 MB/s eta 0:00:41\n",
      "   ----------------------------- ---------- 151.8/203.0 MB 1.3 MB/s eta 0:00:41\n",
      "   ----------------------------- ---------- 152.0/203.0 MB 1.3 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 152.6/203.0 MB 1.3 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 152.6/203.0 MB 1.3 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 152.6/203.0 MB 1.3 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 152.6/203.0 MB 1.3 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 152.8/203.0 MB 1.3 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 152.8/203.0 MB 1.3 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 152.8/203.0 MB 1.3 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 153.1/203.0 MB 1.2 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 153.1/203.0 MB 1.2 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 153.1/203.0 MB 1.2 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 153.1/203.0 MB 1.2 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 153.1/203.0 MB 1.2 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 153.4/203.0 MB 1.2 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 153.4/203.0 MB 1.2 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 153.6/203.0 MB 1.2 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 154.1/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 154.1/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 154.4/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 154.9/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 154.9/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.2/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.2/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.2/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 156.0/203.0 MB 1.1 MB/s eta 0:00:42\n",
      "   ------------------------------ --------- 156.2/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 156.2/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 156.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 156.8/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 157.0/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------ --------- 157.0/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------ --------- 157.3/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------ --------- 157.3/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.5/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.5/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.8/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.8/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.8/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.8/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.8/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.8/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.8/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.8/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 158.1/203.0 MB 1.1 MB/s eta 0:00:41\n",
      "   ------------------------------- -------- 158.1/203.0 MB 1.1 MB/s eta 0:00:41\n",
      "   ------------------------------- -------- 158.1/203.0 MB 1.1 MB/s eta 0:00:41\n",
      "   ------------------------------- -------- 158.3/203.0 MB 1.1 MB/s eta 0:00:42\n",
      "   ------------------------------- -------- 158.3/203.0 MB 1.1 MB/s eta 0:00:42\n",
      "   ------------------------------- -------- 158.3/203.0 MB 1.1 MB/s eta 0:00:42\n",
      "   ------------------------------- -------- 158.3/203.0 MB 1.1 MB/s eta 0:00:42\n",
      "   ------------------------------- -------- 158.6/203.0 MB 1.1 MB/s eta 0:00:42\n",
      "   ------------------------------- -------- 158.6/203.0 MB 1.1 MB/s eta 0:00:42\n",
      "   ------------------------------- -------- 158.6/203.0 MB 1.1 MB/s eta 0:00:42\n",
      "   ------------------------------- -------- 158.6/203.0 MB 1.1 MB/s eta 0:00:42\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ----------------------------- -------- 159.1/203.0 MB 341.7 kB/s eta 0:02:09\n",
      "   ----------------------------- -------- 159.1/203.0 MB 341.7 kB/s eta 0:02:09\n",
      "   ----------------------------- -------- 159.1/203.0 MB 341.7 kB/s eta 0:02:09\n",
      "   ----------------------------- -------- 159.4/203.0 MB 315.6 kB/s eta 0:02:19\n",
      "   ----------------------------- -------- 159.4/203.0 MB 315.6 kB/s eta 0:02:19\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.9/203.0 MB 222.0 kB/s eta 0:03:15\n",
      "   ----------------------------- -------- 159.9/203.0 MB 222.0 kB/s eta 0:03:15\n",
      "   ----------------------------- -------- 160.2/203.0 MB 219.6 kB/s eta 0:03:16\n",
      "   ----------------------------- -------- 160.2/203.0 MB 219.6 kB/s eta 0:03:16\n",
      "   ------------------------------ ------- 160.4/203.0 MB 210.4 kB/s eta 0:03:23\n",
      "   ------------------------------ ------- 160.4/203.0 MB 210.4 kB/s eta 0:03:23\n",
      "   ------------------------------ ------- 160.7/203.0 MB 210.0 kB/s eta 0:03:22\n",
      "   ------------------------------ ------- 161.0/203.0 MB 201.8 kB/s eta 0:03:29\n",
      "   ------------------------------ ------- 161.2/203.0 MB 202.3 kB/s eta 0:03:27\n",
      "   ------------------------------ ------- 161.5/203.0 MB 206.2 kB/s eta 0:03:22\n",
      "   ------------------------------ ------- 161.7/203.0 MB 214.1 kB/s eta 0:03:13\n",
      "   ------------------------------ ------- 162.0/203.0 MB 221.7 kB/s eta 0:03:06\n",
      "   ------------------------------ ------- 162.5/203.0 MB 236.3 kB/s eta 0:02:52\n",
      "   ------------------------------ ------- 162.5/203.0 MB 236.3 kB/s eta 0:02:52\n",
      "   ------------------------------ ------- 163.1/203.0 MB 257.7 kB/s eta 0:02:36\n",
      "   ------------------------------ ------- 163.1/203.0 MB 257.7 kB/s eta 0:02:36\n",
      "   ------------------------------ ------- 163.3/203.0 MB 264.6 kB/s eta 0:02:31\n",
      "   ------------------------------ ------- 163.6/203.0 MB 271.5 kB/s eta 0:02:26\n",
      "   ------------------------------ ------- 163.6/203.0 MB 271.5 kB/s eta 0:02:26\n",
      "   ------------------------------ ------- 163.8/203.0 MB 275.8 kB/s eta 0:02:23\n",
      "   ------------------------------ ------- 163.8/203.0 MB 275.8 kB/s eta 0:02:23\n",
      "   ------------------------------ ------- 164.1/203.0 MB 281.1 kB/s eta 0:02:19\n",
      "   ------------------------------ ------- 164.4/203.0 MB 302.6 kB/s eta 0:02:08\n",
      "   ------------------------------ ------- 164.4/203.0 MB 302.6 kB/s eta 0:02:08\n",
      "   ------------------------------ ------- 164.6/203.0 MB 308.6 kB/s eta 0:02:05\n",
      "   ------------------------------ ------- 164.9/203.0 MB 313.9 kB/s eta 0:02:02\n",
      "   ------------------------------ ------- 164.9/203.0 MB 313.9 kB/s eta 0:02:02\n",
      "   ------------------------------ ------- 165.2/203.0 MB 318.4 kB/s eta 0:02:00\n",
      "   ------------------------------ ------- 165.4/203.0 MB 324.7 kB/s eta 0:01:56\n",
      "   ------------------------------ ------- 165.4/203.0 MB 324.7 kB/s eta 0:01:56\n",
      "   ------------------------------- ------ 165.9/203.0 MB 337.9 kB/s eta 0:01:50\n",
      "   ------------------------------- ------ 166.2/203.0 MB 344.9 kB/s eta 0:01:47\n",
      "   ------------------------------- ------ 166.5/203.0 MB 351.4 kB/s eta 0:01:45\n",
      "   ------------------------------- ------ 166.7/203.0 MB 353.2 kB/s eta 0:01:43\n",
      "   ------------------------------- ------ 167.0/203.0 MB 360.3 kB/s eta 0:01:41\n",
      "   ------------------------------- ------ 167.2/203.0 MB 362.2 kB/s eta 0:01:39\n",
      "   ------------------------------- ------ 167.8/203.0 MB 376.1 kB/s eta 0:01:34\n",
      "   ------------------------------- ------ 168.0/203.0 MB 376.9 kB/s eta 0:01:33\n",
      "   ------------------------------- ------ 168.6/203.0 MB 385.9 kB/s eta 0:01:30\n",
      "   ------------------------------- ------ 168.6/203.0 MB 385.9 kB/s eta 0:01:30\n",
      "   ------------------------------- ------ 168.6/203.0 MB 385.9 kB/s eta 0:01:30\n",
      "   ------------------------------- ------ 168.6/203.0 MB 385.9 kB/s eta 0:01:30\n",
      "   ------------------------------- ------ 168.6/203.0 MB 385.9 kB/s eta 0:01:30\n",
      "   ------------------------------- ------ 168.6/203.0 MB 385.9 kB/s eta 0:01:30\n",
      "   ------------------------------- ------ 168.8/203.0 MB 369.7 kB/s eta 0:01:33\n",
      "   ------------------------------- ------ 168.8/203.0 MB 369.7 kB/s eta 0:01:33\n",
      "   ------------------------------- ------ 169.1/203.0 MB 384.2 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.1/203.0 MB 384.2 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.1/203.0 MB 384.2 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.3/203.0 MB 384.1 kB/s eta 0:01:28\n",
      "   ------------------------------- ------ 169.3/203.0 MB 384.1 kB/s eta 0:01:28\n",
      "   ------------------------------- ------ 169.3/203.0 MB 384.1 kB/s eta 0:01:28\n",
      "   ------------------------------- ------ 169.3/203.0 MB 384.1 kB/s eta 0:01:28\n",
      "   ------------------------------- ------ 169.3/203.0 MB 384.1 kB/s eta 0:01:28\n",
      "   ------------------------------- ------ 169.3/203.0 MB 384.1 kB/s eta 0:01:28\n",
      "   ------------------------------- ------ 169.3/203.0 MB 384.1 kB/s eta 0:01:28\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 170.1/203.0 MB 500.5 kB/s eta 0:01:06\n",
      "   ------------------------------- ------ 170.4/203.0 MB 503.4 kB/s eta 0:01:05\n",
      "   ------------------------------- ------ 170.7/203.0 MB 510.1 kB/s eta 0:01:04\n",
      "   ------------------------------- ------ 170.9/203.0 MB 516.4 kB/s eta 0:01:03\n",
      "   -------------------------------- ----- 171.2/203.0 MB 525.0 kB/s eta 0:01:01\n",
      "   -------------------------------- ----- 171.4/203.0 MB 531.7 kB/s eta 0:01:00\n",
      "   -------------------------------- ----- 171.7/203.0 MB 540.1 kB/s eta 0:00:59\n",
      "   -------------------------------- ----- 172.0/203.0 MB 545.9 kB/s eta 0:00:57\n",
      "   -------------------------------- ----- 172.2/203.0 MB 549.7 kB/s eta 0:00:57\n",
      "   -------------------------------- ----- 172.5/203.0 MB 555.2 kB/s eta 0:00:56\n",
      "   -------------------------------- ----- 172.5/203.0 MB 555.2 kB/s eta 0:00:56\n",
      "   -------------------------------- ----- 172.8/203.0 MB 559.2 kB/s eta 0:00:55\n",
      "   -------------------------------- ----- 173.0/203.0 MB 564.2 kB/s eta 0:00:54\n",
      "   -------------------------------- ----- 173.3/203.0 MB 569.4 kB/s eta 0:00:53\n",
      "   -------------------------------- ----- 173.5/203.0 MB 572.8 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 173.5/203.0 MB 572.8 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 173.5/203.0 MB 572.8 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 173.5/203.0 MB 572.8 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 173.8/203.0 MB 567.7 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 173.8/203.0 MB 567.7 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 174.1/203.0 MB 566.5 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 174.3/203.0 MB 572.4 kB/s eta 0:00:51\n",
      "   -------------------------------- ----- 174.6/203.0 MB 577.8 kB/s eta 0:00:50\n",
      "   -------------------------------- ----- 174.9/203.0 MB 581.5 kB/s eta 0:00:49\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.4/203.0 MB 543.8 kB/s eta 0:00:51\n",
      "   -------------------------------- ----- 175.4/203.0 MB 543.8 kB/s eta 0:00:51\n",
      "   -------------------------------- ----- 175.4/203.0 MB 543.8 kB/s eta 0:00:51\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.9/203.0 MB 447.5 kB/s eta 0:01:01\n",
      "   -------------------------------- ----- 176.2/203.0 MB 439.0 kB/s eta 0:01:02\n",
      "   --------------------------------- ---- 176.4/203.0 MB 439.7 kB/s eta 0:01:01\n",
      "   --------------------------------- ---- 176.7/203.0 MB 440.1 kB/s eta 0:01:00\n",
      "   --------------------------------- ---- 176.9/203.0 MB 446.3 kB/s eta 0:00:59\n",
      "   --------------------------------- ---- 177.2/203.0 MB 451.0 kB/s eta 0:00:58\n",
      "   --------------------------------- ---- 177.7/203.0 MB 463.4 kB/s eta 0:00:55\n",
      "   --------------------------------- ---- 178.0/203.0 MB 467.0 kB/s eta 0:00:54\n",
      "   --------------------------------- ---- 178.5/203.0 MB 481.1 kB/s eta 0:00:51\n",
      "   --------------------------------- ---- 178.8/203.0 MB 484.1 kB/s eta 0:00:51\n",
      "   --------------------------------- ---- 179.3/203.0 MB 494.2 kB/s eta 0:00:49\n",
      "   --------------------------------- ---- 179.8/203.0 MB 507.3 kB/s eta 0:00:46\n",
      "   --------------------------------- ---- 180.1/203.0 MB 509.5 kB/s eta 0:00:46\n",
      "   --------------------------------- ---- 180.4/203.0 MB 515.8 kB/s eta 0:00:44\n",
      "   --------------------------------- ---- 180.9/203.0 MB 527.6 kB/s eta 0:00:43\n",
      "   --------------------------------- ---- 181.1/203.0 MB 533.6 kB/s eta 0:00:42\n",
      "   --------------------------------- ---- 181.7/203.0 MB 542.0 kB/s eta 0:00:40\n",
      "   ---------------------------------- --- 182.2/203.0 MB 551.4 kB/s eta 0:00:38\n",
      "   ---------------------------------- --- 182.7/203.0 MB 553.1 kB/s eta 0:00:37\n",
      "   ---------------------------------- --- 183.0/203.0 MB 559.5 kB/s eta 0:00:36\n",
      "   ---------------------------------- --- 183.8/203.0 MB 570.4 kB/s eta 0:00:34\n",
      "   ---------------------------------- --- 184.3/203.0 MB 578.2 kB/s eta 0:00:33\n",
      "   ---------------------------------- --- 184.8/203.0 MB 587.3 kB/s eta 0:00:32\n",
      "   ---------------------------------- --- 185.6/203.0 MB 597.0 kB/s eta 0:00:30\n",
      "   ---------------------------------- --- 186.1/203.0 MB 605.5 kB/s eta 0:00:28\n",
      "   ---------------------------------- --- 186.9/203.0 MB 621.7 kB/s eta 0:00:26\n",
      "   ----------------------------------- -- 187.4/203.0 MB 629.8 kB/s eta 0:00:25\n",
      "   ----------------------------------- -- 188.0/203.0 MB 660.6 kB/s eta 0:00:23\n",
      "   ----------------------------------- -- 188.7/203.0 MB 682.2 kB/s eta 0:00:21\n",
      "   ----------------------------------- -- 189.5/203.0 MB 703.1 kB/s eta 0:00:20\n",
      "   ----------------------------------- -- 190.1/203.0 MB 717.1 kB/s eta 0:00:19\n",
      "   ----------------------------------- -- 190.6/203.0 MB 729.8 kB/s eta 0:00:18\n",
      "   ----------------------------------- -- 191.4/203.0 MB 751.0 kB/s eta 0:00:16\n",
      "   ----------------------------------- -- 191.6/203.0 MB 757.8 kB/s eta 0:00:16\n",
      "   ----------------------------------- -- 192.2/203.0 MB 777.6 kB/s eta 0:00:15\n",
      "   ------------------------------------ - 192.4/203.0 MB 781.6 kB/s eta 0:00:14\n",
      "   ------------------------------------ - 192.7/203.0 MB 786.3 kB/s eta 0:00:14\n",
      "   ------------------------------------ - 193.2/203.0 MB 796.8 kB/s eta 0:00:13\n",
      "   ------------------------------------ - 193.7/203.0 MB 838.4 kB/s eta 0:00:12\n",
      "   ------------------------------------ - 194.0/203.0 MB 844.3 kB/s eta 0:00:11\n",
      "   ------------------------------------ - 194.2/203.0 MB 847.4 kB/s eta 0:00:11\n",
      "   ------------------------------------ - 194.8/203.0 MB 857.2 kB/s eta 0:00:10\n",
      "   ------------------------------------ - 195.0/203.0 MB 862.4 kB/s eta 0:00:10\n",
      "   ------------------------------------ - 195.3/203.0 MB 862.2 kB/s eta 0:00:09\n",
      "   ------------------------------------ - 195.8/203.0 MB 935.2 kB/s eta 0:00:08\n",
      "   ------------------------------------ - 196.1/203.0 MB 941.0 kB/s eta 0:00:08\n",
      "   ------------------------------------ - 196.3/203.0 MB 944.5 kB/s eta 0:00:08\n",
      "   ------------------------------------ - 196.9/203.0 MB 952.1 kB/s eta 0:00:07\n",
      "   ------------------------------------ - 197.1/203.0 MB 954.5 kB/s eta 0:00:07\n",
      "   ------------------------------------ - 197.4/203.0 MB 956.9 kB/s eta 0:00:06\n",
      "   ------------------------------------ - 197.7/203.0 MB 957.2 kB/s eta 0:00:06\n",
      "   ------------------------------------ - 197.7/203.0 MB 957.2 kB/s eta 0:00:06\n",
      "   -------------------------------------  197.9/203.0 MB 955.9 kB/s eta 0:00:06\n",
      "   -------------------------------------  198.2/203.0 MB 956.7 kB/s eta 0:00:06\n",
      "   -------------------------------------  198.7/203.0 MB 963.2 kB/s eta 0:00:05\n",
      "   ---------------------------------------  199.0/203.0 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------------  199.2/203.0 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------------  199.2/203.0 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------------  199.5/203.0 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------------  199.8/203.0 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------------  200.0/203.0 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------------------  200.5/203.0 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------------------  201.1/203.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  201.3/203.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  201.6/203.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  201.9/203.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  202.1/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.6/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 203.0/203.0 MB 1.0 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/6.2 MB 2.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.3/6.2 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.6/6.2 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.1/6.2 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.6/6.2 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.1/6.2 MB 2.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.7/6.2 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 3.9/6.2 MB 2.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 4.5/6.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.7/6.2 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.0/6.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.5/6.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.20.1-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.6 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.8/1.6 MB 2.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.3/1.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.5.1-cp312-cp312-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.8/2.4 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.3/2.4 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 262.1/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, fsspec, filelock, torch, torchvision, torchaudio\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.10.0 mpmath-1.3.0 sympy-1.13.1 torch-2.5.1 torchaudio-2.5.1 torchvision-0.20.1 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --timeout 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "01d10c41-bbdf-4aae-9119-a371637b19c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f82bfb9430>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch  # Importa la librera principal de PyTorch, que es utilizada para construir y entrenar modelos de redes neuronales, y realizar operaciones tensoriales.\n",
    "\n",
    "import torch.nn as nn  # Importa el submdulo `nn` de PyTorch, que contiene clases para construir redes neuronales, como capas (e.g., Linear, Conv2d) y modelos predefinidos.\n",
    "\n",
    "import torch.nn.functional as F  # Importa el submdulo `functional` de PyTorch, que contiene funciones tiles para redes neuronales, como activaciones (ReLU, Sigmoid) y funciones de prdida (CrossEntropy, MSELoss).\n",
    "\n",
    "import torch.optim as optim  # Importa el submdulo `optim` de PyTorch, que proporciona varios optimizadores (como SGD, Adam) para actualizar los parmetros del modelo durante el entrenamiento.\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader  # Importa herramientas esenciales para trabajar con datos en PyTorch:\n",
    "# - `Dataset`: Es la clase base para crear conjuntos de datos personalizados, donde defines cmo acceder a los datos (implementando los mtodos `__getitem__` y `__len__`).\n",
    "# - `DataLoader`: Es utilizado para cargar datos en lotes (batches) durante el entrenamiento de redes neuronales, facilitando el procesamiento eficiente de grandes volmenes de datos.\n",
    "\n",
    "from collections import Counter  # Importa la clase `Counter` de la librera `collections`, que se utiliza para contar la frecuencia de elementos en una lista o conjunto de datos, til en anlisis de texto o para datos en general.\n",
    "\n",
    "import math  # Importa la librera estndar `math` que proporciona funciones matemticas avanzadas como logaritmos, trigonometra, constantes matemticas (por ejemplo, `pi`), etc.\n",
    "\n",
    "import numpy as np  # Importa la librera `numpy` para realizar operaciones eficientes con arrays multidimensionales y lgebra lineal.\n",
    "\n",
    "import re  # Importa la librera `re` para trabajar con expresiones regulares, que se utiliza para encontrar patrones y realizar bsquedas en texto.\n",
    "\n",
    "torch.manual_seed(23)  # Establece una semilla para la generacin de nmeros aleatorios en PyTorch, con el valor `23`. Esto asegura que los resultados sean reproducibles cada vez que se ejecute el cdigo, garantizando consistencia en la inicializacin de pesos y en el muestreo aleatorio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b530e45-19d9-48f6-be3d-ed27ee6ccddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Asigna el dispositivo de computacin en el que se ejecutar el modelo. Si hay una GPU disponible (a travs de CUDA), se utilizar la GPU ('cuda'). \n",
    "#Si no hay GPU disponible, se utilizar la CPU ('cpu').\n",
    "\n",
    "#print(device)  # Imprime el dispositivo que se ha asignado, es decir, 'cuda' si la GPU est disponible o 'cpu' si solo se puede usar la CPU.\n",
    "\n",
    "device = torch.device(\"cpu\")  # Asegrate de usar la CPU\n",
    "model = model.to(device)  # Mueve el modelo a la CPU\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fce31f70-7fd6-4b7c-a214-18eb5d7ca357",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 50  # Define la longitud mxima de una secuencia de entrada (en este caso, 128 elementos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d2161d7-de1a-4093-9f2b-e3906ddea564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    La clase `PositionalEmbedding` genera una matriz de codificacin posicional \n",
    "    que se aade a los embeddings de las palabras. Esta codificacin posicional \n",
    "    permite que el modelo Transformer capture informacin sobre el orden de los tokens en la secuencia.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_seq_len = MAX_SEQ_LEN):\n",
    "        \"\"\"\n",
    "        Inicializa la codificacin posicional.\n",
    "        Parmetros:\n",
    "        - d_model: La dimensionalidad del embedding de cada token (tambin usado en el Transformer).\n",
    "        - max_seq_len: La longitud mxima de la secuencia de entrada (por defecto, MAX_SEQ_LEN).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Crear la matriz de embeddings posicionales, con tamao (max_seq_len, d_model)\n",
    "        self.pos_embed_matrix = torch.zeros(max_seq_len, d_model, device=device)\n",
    "        \n",
    "        # Generar las posiciones de los tokens, un tensor de tamao (max_seq_len, 1)\n",
    "        token_pos = torch.arange(0, max_seq_len, dtype = torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Calcular el trmino de divisin para cada dimensin del embedding\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() \n",
    "                             * (-math.log(10000.0)/d_model))\n",
    "        \n",
    "        # Aplicar las funciones seno y coseno a las posiciones\n",
    "        self.pos_embed_matrix[:, 0::2] = torch.sin(token_pos * div_term)\n",
    "        self.pos_embed_matrix[:, 1::2] = torch.cos(token_pos * div_term)\n",
    "        \n",
    "        # Reorganizar la matriz para la suma posterior con el embedding de entrada\n",
    "        self.pos_embed_matrix = self.pos_embed_matrix.unsqueeze(0).transpose(0,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Realiza la operacin de suma entre el embedding de entrada y la codificacin posicional.\n",
    "        Parmetros:\n",
    "        - x: El tensor de entrada con forma (seq_len, batch_size, d_model).\n",
    "        \"\"\"\n",
    "        return x + self.pos_embed_matrix[:x.size(0), :]\n",
    "\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    La clase `MultiHeadAttention` implementa la atencin multi-cabeza del Transformer. \n",
    "    La atencin multi-cabeza permite que el modelo enfoque en diferentes partes de la secuencia \n",
    "    de entrada de forma simultnea.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model = 512, num_heads = 8):\n",
    "        \"\"\"\n",
    "        Inicializa la atencin multi-cabeza.\n",
    "        Parmetros:\n",
    "        - d_model: Dimensionalidad del embedding de cada token.\n",
    "        - num_heads: Nmero de cabezas de atencin.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Asegura que el tamao del modelo sea divisible por el nmero de cabezas\n",
    "        assert d_model % num_heads == 0, 'Embedding size not compatible with num heads'\n",
    "        \n",
    "        # Dimensiones de la representacin de cada cabeza\n",
    "        self.d_v = d_model // num_heads\n",
    "        self.d_k = self.d_v\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Definicin de las matrices de transformacin para Q (consulta), K (clave) y V (valor)\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask = None):\n",
    "        \"\"\"\n",
    "        Realiza la operacin de atencin multi-cabeza.\n",
    "        Parmetros:\n",
    "        - Q: Tensor de consultas con forma [batch_size, seq_len, d_model].\n",
    "        - K: Tensor de claves con forma [batch_size, seq_len, d_model].\n",
    "        - V: Tensor de valores con forma [batch_size, seq_len, d_model].\n",
    "        - mask: Mscara para evitar que ciertos tokens se vean durante la atencin.\n",
    "        \"\"\"\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # Aplicar las transformaciones lineales a Q, K y V y luego dividirlas en varias cabezas\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Aplicar la atencin de producto punto escalado\n",
    "        weighted_values, attention = self.scale_dot_product(Q, K, V, mask)\n",
    "        \n",
    "        # Reorganizar la salida de las cabezas de atencin\n",
    "        weighted_values = weighted_values.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        \n",
    "        # Aplicar la transformacin final\n",
    "        weighted_values = self.W_o(weighted_values)\n",
    "        \n",
    "        return weighted_values, attention\n",
    "        \n",
    "    def scale_dot_product(self, Q, K, V, mask = None):\n",
    "        \"\"\"\n",
    "        Realiza el producto punto escalado entre las consultas y las claves, \n",
    "        aplicando la mscara si es necesario.\n",
    "        Parmetros:\n",
    "        - Q: Consultas.\n",
    "        - K: Claves.\n",
    "        - V: Valores.\n",
    "        - mask: Mscara para aplicar.\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)  # Aplica la mscara\n",
    "        attention = F.softmax(scores, dim=-1)  # Aplica softmax\n",
    "        weighted_values = torch.matmul(attention, V)  # Calcula el valor ponderado\n",
    "        \n",
    "        return weighted_values, attention\n",
    "\n",
    "\n",
    "class PositionFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    La clase `PositionFeedForward` es una red feed-forward completamente conectada \n",
    "    que se aplica de forma independiente a cada posicin de la secuencia.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        \"\"\"\n",
    "        Inicializa la red feed-forward.\n",
    "        Parmetros:\n",
    "        - d_model: Dimensionalidad del embedding de cada token.\n",
    "        - d_ff: Dimensionalidad de la capa interna de la red feed-forward.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)  # Capa lineal de entrada\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)  # Capa lineal de salida\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Realiza el paso hacia adelante de la red feed-forward.\n",
    "        Parmetros:\n",
    "        - x: Tensor de entrada con forma [batch_size, seq_len, d_model].\n",
    "        \"\"\"\n",
    "        return self.linear2(F.relu(self.linear1(x)))  # Activacin ReLU\n",
    "\n",
    "\n",
    "class EncoderSubLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    La clase `EncoderSubLayer` define una subcapa del encoder del Transformer, \n",
    "    que consiste en una capa de atencin seguida de una red feed-forward.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout = 0.1):\n",
    "        \"\"\"\n",
    "        Inicializa la subcapa del encoder.\n",
    "        Parmetros:\n",
    "        - d_model: Dimensionalidad del embedding de cada token.\n",
    "        - num_heads: Nmero de cabezas de atencin.\n",
    "        - d_ff: Dimensionalidad de la red feed-forward.\n",
    "        - dropout: Tasa de dropout para regularizacin.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)  # Atencin multi-cabeza\n",
    "        self.ffn = PositionFeedForward(d_model, d_ff)  # Red feed-forward\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # Normalizacin de capa\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)  # Dropout para regularizacin\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        \"\"\"\n",
    "        Realiza el paso hacia adelante de la subcapa del encoder.\n",
    "        Parmetros:\n",
    "        - x: Tensor de entrada.\n",
    "        - mask: Mscara para la atencin.\n",
    "        \"\"\"\n",
    "        attention_score, _ = self.self_attn(x, x, x, mask)  # Atencin de la entrada\n",
    "        x = x + self.dropout1(attention_score)  # Residual y dropout\n",
    "        x = self.norm1(x)  # Normalizacin\n",
    "        x = x + self.dropout2(self.ffn(x))  # Red feed-forward y residual\n",
    "        return self.norm2(x)  # Normalizacin final\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    La clase `Encoder` contiene varias capas de `EncoderSubLayer`, \n",
    "    cada una de las cuales realiza atencin y procesamiento feed-forward.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Inicializa el encoder.\n",
    "        Parmetros:\n",
    "        - d_model: Dimensionalidad del embedding de cada token.\n",
    "        - num_heads: Nmero de cabezas de atencin.\n",
    "        - d_ff: Dimensionalidad de la red feed-forward.\n",
    "        - num_layers: Nmero de capas en el encoder.\n",
    "        - dropout: Tasa de dropout.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)  # Normalizacin final del encoder\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Realiza el paso hacia adelante del encoder.\n",
    "        Parmetros:\n",
    "        - x: Tensor de entrada.\n",
    "        - mask: Mscara para la atencin.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class DecoderSubLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    La clase `DecoderSubLayer` define una subcapa del decoder, que incluye\n",
    "    atencin self-attention, cross-attention (atencin entre encoder y decoder),\n",
    "    y una red feed-forward.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Inicializa la subcapa del decoder.\n",
    "        Parmetros:\n",
    "        - d_model: Dimensionalidad del embedding de cada token.\n",
    "        - num_heads: Nmero de cabezas de atencin.\n",
    "        - d_ff: Dimensionalidad de la red feed-forward.\n",
    "        - dropout: Tasa de dropout.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)  # Atencin self\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)  # Atencin entre encoder y decoder\n",
    "        self.feed_forward = PositionFeedForward(d_model, d_ff)  # Red feed-forward\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # Normalizacin\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)  # Dropout\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, target_mask=None, encoder_mask=None):\n",
    "        \"\"\"\n",
    "        Realiza el paso hacia adelante de la subcapa del decoder.\n",
    "        Parmetros:\n",
    "        - x: Tensor de entrada (target).\n",
    "        - encoder_output: Salida del encoder (la que el decoder usa como contexto).\n",
    "        - target_mask: Mscara para la atencin del target.\n",
    "        - encoder_mask: Mscara para la atencin del encoder.\n",
    "        \"\"\"\n",
    "        attention_score, _ = self.self_attn(x, x, x, target_mask)  # Self-attention\n",
    "        x = x + self.dropout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        encoder_attn, _ = self.cross_attn(x, encoder_output, encoder_output, encoder_mask)  # Cross-attention\n",
    "        x = x + self.dropout2(encoder_attn)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        ff_output = self.feed_forward(x)  # Red feed-forward\n",
    "        x = x + self.dropout3(ff_output)\n",
    "        return self.norm3(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    La clase `Decoder` contiene varias capas de `DecoderSubLayer`, \n",
    "    cada una de las cuales realiza atencin self-attention, cross-attention \n",
    "    y procesamiento feed-forward.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Inicializa el decoder.\n",
    "        Parmetros:\n",
    "        - d_model: Dimensionalidad del embedding de cada token.\n",
    "        - num_heads: Nmero de cabezas de atencin.\n",
    "        - d_ff: Dimensionalidad de la red feed-forward.\n",
    "        - num_layers: Nmero de capas en el decoder.\n",
    "        - dropout: Tasa de dropout.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)  # Normalizacin final del decoder\n",
    "\n",
    "    def forward(self, x, encoder_output, target_mask, encoder_mask):\n",
    "        \"\"\"\n",
    "        Realiza el paso hacia adelante del decoder.\n",
    "        Parmetros:\n",
    "        - x: Tensor de entrada (target).\n",
    "        - encoder_output: Salida del encoder.\n",
    "        - target_mask: Mscara para la atencin del target.\n",
    "        - encoder_mask: Mscara para la atencin del encoder.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, target_mask, encoder_mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7d47fef4-c64b-4610-8181-109453077f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):  \n",
    "    # Definicin de la clase Transformer, que hereda de nn.Module para construir un modelo de red neuronal.\n",
    "    # Esta clase representa la arquitectura completa del Transformer: un modelo de atencin con encoder-decoder.\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers,\n",
    "                 input_vocab_size, target_vocab_size, \n",
    "                 max_len=MAX_SEQ_LEN, dropout=0.1):\n",
    "        # El mtodo constructor inicializa los componentes del Transformer.\n",
    "        # Parmetros:\n",
    "        # - d_model: Dimensionalidad de los vectores de caractersticas.\n",
    "        # - num_heads: Nmero de cabezas de atencin en cada capa de atencin multi-cabeza.\n",
    "        # - d_ff: Dimensionalidad de la red completamente conectada en el modelo.\n",
    "        # - num_layers: Nmero de capas en el encoder y decoder.\n",
    "        # - input_vocab_size: Tamao del vocabulario de entrada.\n",
    "        # - target_vocab_size: Tamao del vocabulario de salida (target).\n",
    "        # - max_len: Longitud mxima de las secuencias de entrada y salida.\n",
    "        # - dropout: Tasa de dropout para regularizacin durante el entrenamiento.\n",
    "        \n",
    "        super().__init__()  # Llama al constructor de la clase base nn.Module.\n",
    "        \n",
    "        # Definicin de las capas del modelo.\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)  \n",
    "        # Capa de embedding para la entrada (vocabulario de entrada -> d_model).\n",
    "        \n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        # Capa de embedding para la salida (vocabulario de salida -> d_model).\n",
    "        \n",
    "        self.pos_embedding = PositionalEmbedding(d_model, max_len)\n",
    "        # Capa para codificar las posiciones de los tokens en las secuencias de entrada y salida.\n",
    "        \n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        # Definicin del encoder con el nmero de capas (num_layers), atencin multi-cabeza (num_heads) y otros parmetros.\n",
    "        \n",
    "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        # Definicin del decoder con los mismos parmetros que el encoder.\n",
    "        \n",
    "        self.output_layer = nn.Linear(d_model, target_vocab_size)\n",
    "        # Capa lineal que convierte la salida del decoder a las predicciones finales (tamao vocabulario de salida).\n",
    "        \n",
    "    def forward(self, source, target):\n",
    "        # Mtodo `forward` define cmo se realiza el paso hacia adelante a travs del modelo.\n",
    "        # Parmetros:\n",
    "        # - source: Secuencia de entrada (source).\n",
    "        # - target: Secuencia de salida esperada (target).\n",
    "        \n",
    "        # Generacin de las mscaras para el encoder y decoder.\n",
    "        source_mask, target_mask = self.mask(source, target)\n",
    "        \n",
    "        # Embedding y codificacin posicional para la entrada (source).\n",
    "        source = self.encoder_embedding(source) * math.sqrt(self.encoder_embedding.embedding_dim)\n",
    "        # Realiza el embedding de la entrada y escala por la raz cuadrada de la dimensin del embedding.\n",
    "        \n",
    "        source = self.pos_embedding(source)  \n",
    "        # Aade la codificacin posicional a la entrada.\n",
    "        \n",
    "        # Paso del encoder.\n",
    "        encoder_output = self.encoder(source, source_mask)\n",
    "        \n",
    "        # Embedding y codificacin posicional para la salida (target).\n",
    "        target = self.decoder_embedding(target) * math.sqrt(self.decoder_embedding.embedding_dim)\n",
    "        # Realiza el embedding de la salida y escala de manera similar.\n",
    "        \n",
    "        target = self.pos_embedding(target)  \n",
    "        # Aade la codificacin posicional a la salida.\n",
    "        \n",
    "        # Paso del decoder.\n",
    "        output = self.decoder(target, encoder_output, target_mask, source_mask)\n",
    "        # El decoder recibe la salida del encoder y las mscaras correspondientes para generar la salida.\n",
    "        \n",
    "        return self.output_layer(output)\n",
    "        # La salida del decoder se pasa por una capa lineal que produce la prediccin final para cada token en la secuencia de salida.\n",
    "\n",
    "    \n",
    "    def mask(self, source, target):\n",
    "        # Mtodo para crear las mscaras para el encoder y decoder.\n",
    "        # Las mscaras se utilizan para evitar que el modelo vea tokens futuros durante el entrenamiento (en el caso del decoder).\n",
    "        \n",
    "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)  \n",
    "        # Mscara para la entrada (source): crea una mscara para los tokens no nulos (diferentes de 0).\n",
    "        \n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(2)  \n",
    "        # Mscara para la salida (target): crea una mscara similar para la secuencia de salida.\n",
    "        \n",
    "        size = target.size(1)  \n",
    "        # Obtiene el tamao de la secuencia de salida.\n",
    "        \n",
    "        no_mask = torch.tril(torch.ones((1, size, size), device=device)).bool()  \n",
    "        # Crea una mscara triangular inferior para el decoder, asegurndose de que cada posicin solo pueda \"ver\" las posiciones anteriores (para evitar mirar futuros tokens).\n",
    "        \n",
    "        target_mask = target_mask & no_mask\n",
    "        # Aplica la mscara triangular inferior a la mscara de la salida (target_mask).\n",
    "        \n",
    "        return source_mask, target_mask  \n",
    "        # Devuelve las mscaras para la entrada y la salida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2df51453-d98b-4b5c-a522-a092edb01309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicin de parmetros de entrada y salida para el modelo\n",
    "seq_len_source = 10  # Longitud de la secuencia de entrada (source).\n",
    "seq_len_target = 10  # Longitud de la secuencia de salida (target).\n",
    "batch_size = 2  # Nmero de ejemplos en cada lote (batch).\n",
    "input_vocab_size = 50  # Tamao del vocabulario para la secuencia de entrada (source). 50\n",
    "target_vocab_size = 50  # Tamao del vocabulario para la secuencia de salida (target). 50\n",
    "\n",
    "# Generacin de las secuencias de entrada (source) y salida (target)\n",
    "# Las secuencias de entrada y salida son vectores de enteros que representan ndices\n",
    "# de palabras en los respectivos vocabularios.\n",
    "# `torch.randint` genera nmeros enteros aleatorios en el rango [1, vocab_size).\n",
    "source = torch.randint(1, input_vocab_size, (batch_size, seq_len_source))  # Secuencias de entrada (source)\n",
    "target = torch.randint(1, target_vocab_size, (batch_size, seq_len_target))  # Secuencias de salida (target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "76e5908d-f8b0-4e65-a337-76abf8b4c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicin de hiperparmetros para el modelo Transformer\n",
    "d_model = 512 # Dimensionalidad de los vectores de caractersticas (embeddings) de entrada y salida.  #512\n",
    "               # Este valor define el tamao de la representacin interna de cada token.\n",
    "num_heads = 8  # Nmero de cabezas en la atencin multi-cabeza (multi-head attention).\n",
    "               # Esto permite al modelo aprender diferentes representaciones para cada token en distintas subespacios de caractersticas.\n",
    "d_ff = 2048  # Tamao de la capa de alimentacin directa (feed-forward layer) interna. #2048\n",
    "             # Es el nmero de neuronas en la capa oculta de la red de alimentacin directa.\n",
    "num_layers = 6  # Nmero de capas en el encoder y decoder del Transformer.\n",
    "               # A mayor nmero de capas, mayor capacidad de modelado, pero tambin mayor complejidad computacional.\n",
    "\n",
    "# Inicializacin del modelo Transformer\n",
    "# Se crea una instancia del modelo Transformer con los hiperparmetros definidos previamente.\n",
    "# Este modelo es capaz de procesar secuencias de entrada y salida usando un mecanismo de atencin\n",
    "# multi-cabeza y una red de alimentacin directa.\n",
    "model = Transformer(d_model, num_heads, d_ff, num_layers,\n",
    "                  input_vocab_size, target_vocab_size, \n",
    "                  max_len=MAX_SEQ_LEN, dropout=0.1)\n",
    "\n",
    "# Mover el modelo al dispositivo adecuado (CPU o GPU)\n",
    "# Esto garantiza que el modelo se ejecute en el dispositivo donde la variable `device` haya sido configurada (puede ser 'cuda' si hay GPU disponible o 'cpu' si no).\n",
    "model = model.to(device)\n",
    "\n",
    "# Mover las secuencias de entrada (source) y salida (target) al mismo dispositivo que el modelo.\n",
    "# Esto asegura que las secuencias de entrada y salida estn en el dispositivo correcto para el procesamiento\n",
    "# durante la etapa de entrenamiento o inferencia.\n",
    "source = source.to(device)  # Mueve las secuencias de entrada (source) al dispositivo\n",
    "target = target.to(device)  # Mueve las secuencias de salida (target) al dispositivo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eb9d67dd-563f-4eaa-8b4c-18458e9ac36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(source, target)  # Realiza una pasada hacia adelante a travs del modelo Transformer, generando las predicciones de salida para cada token en la secuencia 'target' basada en las secuencias de entrada 'source'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "19715a6f-eef6-4e01-966e-50f060ff45d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.shape torch.Size([2, 10, 50])\n"
     ]
    }
   ],
   "source": [
    "# Expected output shape -> [batch, seq_len_target, target_vocab_size] i.e. [2, 10, 50]\n",
    "print(f'output.shape {output.shape}')  # Imprime las dimensiones del tensor de salida, mostrando la forma de la salida del modelo (batch_size, seq_len_target, target_vocab_size).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1894bfc-c9f5-4a15-9498-a1b69d09ee8e",
   "metadata": {},
   "source": [
    "<h1>Translation Eng-Span</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bd170b79-77c5-47d2-be87-adc26d50e96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0                                                 1        2   \\\n",
      "0  1276                              Let's try something.     2481   \n",
      "1  1277                            I have to go to sleep.     2482   \n",
      "2  1280  Today is June 18th and it is Muiriel's birthday!     2485   \n",
      "3  1280  Today is June 18th and it is Muiriel's birthday!  1130137   \n",
      "4  1282                                Muiriel is 20 now.     2487   \n",
      "\n",
      "                                                  3    4    5    6    7    8   \\\n",
      "0                                  Intentemos algo!  NaN  NaN  NaN  NaN  NaN   \n",
      "1                           Tengo que irme a dormir.  NaN  NaN  NaN  NaN  NaN   \n",
      "2  Hoy es 18 de junio y es el cumpleaos de Muir...  NaN  NaN  NaN  NaN  NaN   \n",
      "3  Hoy es el 18 de junio y es el cumpleaos de M...  NaN  NaN  NaN  NaN  NaN   \n",
      "4                      Ahora, Muiriel tiene 20 aos.  NaN  NaN  NaN  NaN  NaN   \n",
      "\n",
      "    9    10  \n",
      "0  NaN  NaN  \n",
      "1  NaN  NaN  \n",
      "2  NaN  NaN  \n",
      "3  NaN  NaN  \n",
      "4  NaN  NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Definimos la ruta del archivo CSV que contiene las frases en ingls y espaol\n",
    "PATH = 'ing-esp.csv'  # Asegrate de que la ruta sea correcta\n",
    "\n",
    "# Intentamos leer el archivo CSV especificando el delimitador correcto (punto y coma ';')\n",
    "# Leemos solo las columnas que nos interesan (1 para ingls y 3 para espaol)\n",
    "df = pd.read_csv(PATH, sep=';', encoding='ISO-8859-1', header=None, dtype=str, low_memory=False, on_bad_lines='skip')\n",
    "\n",
    "# Verificamos las primeras filas para entender la estructura del DataFrame\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "deddd76c-c886-4389-b5f9-e8319e015393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos solo las columnas 1 (ingls) y 3 (espaol)\n",
    "eng_spa_cols = df.iloc[:, [1, 3]]\n",
    "\n",
    "# Eliminamos cualquier columna vaca (si existiera alguna columna extra)\n",
    "eng_spa_cols = eng_spa_cols.dropna(axis=1, how='all')\n",
    "\n",
    "# Calculamos la longitud de las frases en ingls\n",
    "eng_spa_cols['length'] = eng_spa_cols.iloc[:, 0].str.len()\n",
    "\n",
    "# Ordenamos por la longitud de las frases en ingls\n",
    "eng_spa_cols = eng_spa_cols.sort_values(by='length')\n",
    "\n",
    "# Eliminamos la columna 'length' ya que no es necesaria para el archivo final\n",
    "eng_spa_cols = eng_spa_cols.drop(columns=['length'])\n",
    "\n",
    "# Especificamos la ruta de salida para el archivo\n",
    "output_file_path = 'textoIngleEspaniol.txt'\n",
    "\n",
    "# Guardamos el DataFrame resultante en un archivo de texto (tabulado y sin ndice ni encabezados)\n",
    "eng_spa_cols.to_csv(output_file_path, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0a0f803a-ce96-4fa5-af94-a4dce6ba38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'textoIngleEspaniol.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2bf8a06d-d2fe-4445-b05b-0fcb1a60cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrimos el archivo en modo de lectura ('r') y con codificacin 'utf-8' para manejar correctamente los caracteres especiales.\n",
    "# La variable 'PATH' debe contener la ruta del archivo que estamos leyendo.\n",
    "with open(PATH, 'r', encoding='utf-8') as f:\n",
    "    # Leemos todas las lneas del archivo y las almacenamos en una lista llamada 'lines'.\n",
    "    # Cada elemento de la lista 'lines' ser una cadena de texto que corresponde a una lnea del archivo.\n",
    "    lines = f.readlines()  # `readlines()` lee todo el contenido del archivo y lo divide por lneas.\n",
    "\n",
    "# Ahora procesamos cada lnea en 'lines', y por cada lnea:\n",
    "# 1. Eliminamos cualquier espacio en blanco o salto de lnea extra con 'strip()'.\n",
    "# 2. Dividimos la lnea en dos partes usando el delimitador de tabulacin ('\\t') con 'split()'.\n",
    "#    Esto crear una lista con dos elementos: la primera frase (en ingls) y la segunda (en espaol).\n",
    "# 3. Filtramos las lneas que no contienen el delimitador '\\t', asegurndonos de que solo procesamos las lneas con pares de frases.\n",
    "eng_spa_pairs = [line.strip().split('\\t') for line in lines if '\\t' in line]\n",
    "\n",
    "# 'eng_spa_pairs' ser ahora una lista de listas donde cada sublista contiene dos elementos:\n",
    "# 1. La primera posicin es la frase en ingls.\n",
    "# 2. La segunda posicin es la traduccin en espaol.\n",
    "# Esto se utilizar para tareas como entrenamiento de un modelo de traduccin, donde cada par de frases (ingls, espaol) es un ejemplo de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e7fda871-a9f4-47dd-807e-003c1d56cf49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['So?', 'Y?'],\n",
       " ['OK.', 'rale!'],\n",
       " ['Hi!', 'Hola!'],\n",
       " ['Go.', 'Ve.'],\n",
       " ['Hi.', 'Hola!'],\n",
       " ['No.', 'No.'],\n",
       " ['Go.', 'Vete.'],\n",
       " ['Ah!', 'Anda!'],\n",
       " ['Go.', 'Vyase.'],\n",
       " ['No!', 'No!']]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora que hemos procesado las lneas y tenemos 'eng_spa_pairs', vamos a tomar los primeros 10 pares de frases\n",
    "# para verificar que el procesamiento de los datos se haya realizado correctamente.\n",
    "# Esto es til para inspeccionar una pequea muestra de los datos antes de utilizarla para entrenar un modelo.\n",
    "eng_spa_pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eab40ddf-cf89-4632-805b-5e3e348b08d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So?', 'OK.', 'Hi!', 'Go.', 'Hi.', 'No.', 'Go.', 'Ah!', 'Go.', 'No!']\n",
      "['Y?', 'rale!', 'Hola!', 'Ve.', 'Hola!', 'No.', 'Vete.', 'Anda!', 'Vyase.', 'No!']\n"
     ]
    }
   ],
   "source": [
    "# Primero, verificamos que cada par de frases tiene exactamente dos elementos\n",
    "# Para evitar que se generen errores de ndice fuera de rango\n",
    "eng_spa_pairs = [pair for pair in eng_spa_pairs if len(pair) == 2]\n",
    "\n",
    "# Ahora podemos proceder a extraer las frases en ingls y espaol sin problema\n",
    "eng_sentences = [pair[0] for pair in eng_spa_pairs]  # Frases en ingls (primer elemento de cada par)\n",
    "spa_sentences = [pair[1] for pair in eng_spa_pairs]  # Frases en espaol (segundo elemento de cada par)\n",
    "\n",
    "# Verificamos los primeros 10 elementos de las listas para asegurarnos de que todo est correcto\n",
    "print(eng_sentences[:10])\n",
    "print(spa_sentences[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9d39a6de-4567-4f88-a2b1-a66cb39c65c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # 1. Convertir todo el texto a minsculas y eliminar espacios al principio y al final.\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    # 2. Reemplazar mltiples espacios seguidos por un solo espacio.\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    \n",
    "    # 3. Normalizar las vocales con tilde a su forma sin tilde (por ejemplo, \"\" a \"a\").\n",
    "    sentence = re.sub(r\"[]+\", \"a\", sentence)\n",
    "    sentence = re.sub(r\"[]+\", \"e\", sentence)\n",
    "    sentence = re.sub(r\"[]+\", \"i\", sentence)\n",
    "    sentence = re.sub(r\"[]+\", \"o\", sentence)\n",
    "    sentence = re.sub(r\"[]+\", \"u\", sentence)\n",
    "    \n",
    "    # 4. Eliminar todos los caracteres que no son letras del alfabeto (como signos de puntuacin, nmeros, etc.)\n",
    "    sentence = re.sub(r\"[^a-z]+\", \" \", sentence)\n",
    "    \n",
    "    # 5. Volver a eliminar cualquier espacio extra al principio o al final.\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    # 6. Aadir las etiquetas <sos> al principio y <eos> al final de la frase.\n",
    "    sentence = '<sos> ' + sentence + ' <eos>'\n",
    "    \n",
    "    # 7. Devolver la frase procesada.\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c0eb545a-8e60-483f-ab05-e69423e4e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 'Hola @ cmo ests? 123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7466cdd8-0076-44b1-a769-89f768fdd60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola @ cmo ests? 123\n",
      "<sos> hola como estas <eos>\n"
     ]
    }
   ],
   "source": [
    "# Imprime la variable 's1' en su forma original.\n",
    "# La variable 's1' contiene una cadena de texto (string) que puede tener diferentes caracteres especiales, espacios extras, o incluso signos de puntuacin.\n",
    "# Al utilizar 'print(s1)', estamos visualizando cmo es la cadena antes de cualquier tipo de preprocesamiento.\n",
    "print(s1)\n",
    "\n",
    "# Aplica la funcin 'preprocess_sentence' a la cadena 's1' y luego imprime el resultado.\n",
    "# La funcin 'preprocess_sentence' toma la cadena 's1', la procesa para convertirla a minsculas, \n",
    "# eliminar caracteres no alfabticos, eliminar espacios innecesarios, normalizar las vocales con tildes, y agregar etiquetas especiales al inicio y fin.\n",
    "# El resultado es una versin ms \"limpia\" de la cadena original, que es ms adecuada para tareas de procesamiento de lenguaje natural.\n",
    "# Se imprime la cadena procesada para que se pueda comparar con la forma original de 's1'.\n",
    "print(preprocess_sentence(s1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "662c8bb3-afce-460e-9775-8e31619a4c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica la funcin 'preprocess_sentence' a cada una de las frases en la lista 'eng_sentences'.\n",
    "# La lista 'eng_sentences' contiene frases en ingls, y queremos preprocesar cada una de esas frases antes de usarlas en un modelo de procesamiento de lenguaje.\n",
    "# Esto asegura que todas las frases en ingls sean limpiadas, normalizadas y formateadas de manera consistente (por ejemplo, convirtindolas a minsculas, eliminando caracteres especiales y agregando las etiquetas <sos> y <eos>).\n",
    "# Usamos una lista por comprensin para aplicar la funcin 'preprocess_sentence' a cada frase de 'eng_sentences'.\n",
    "eng_sentences = [preprocess_sentence(sentence) for sentence in eng_sentences]\n",
    "\n",
    "# Aplica la misma funcin 'preprocess_sentence' a cada una de las frases en la lista 'spa_sentences'.\n",
    "# La lista 'spa_sentences' contiene frases en espaol, y de igual forma, necesitamos preprocesarlas para hacerlas aptas para su uso en el modelo.\n",
    "# Al igual que en el caso de las frases en ingls, se aplica la funcin de preprocesamiento a cada frase en 'spa_sentences' para limpiarlas y estructurarlas correctamente.\n",
    "spa_sentences = [preprocess_sentence(sentence) for sentence in spa_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "161af0b2-1dc9-4dbc-bc29-8a55f4cb7b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> y <eos>',\n",
       " '<sos> orale <eos>',\n",
       " '<sos> hola <eos>',\n",
       " '<sos> ve <eos>',\n",
       " '<sos> hola <eos>',\n",
       " '<sos> no <eos>',\n",
       " '<sos> vete <eos>',\n",
       " '<sos> anda <eos>',\n",
       " '<sos> vayase <eos>',\n",
       " '<sos> no <eos>']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spa_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "66ddff21-f2d6-4610-a15b-71031d528aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicin de la funcin 'build_vocab' que se encarga de construir el vocabulario\n",
    "# a partir de una lista de oraciones. El vocabulario es esencial para convertir\n",
    "# las palabras en sus correspondientes ndices (y viceversa), lo cual es necesario\n",
    "# para trabajar con modelos de aprendizaje automtico.\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    # 'sentences' es una lista de oraciones (frases) que puede contener texto en ingls, espaol\n",
    "    # o cualquier otro idioma, dependiendo del contexto de uso de la funcin.\n",
    "    \n",
    "    # 1. Se extraen todas las palabras de todas las oraciones, de manera que cada palabra\n",
    "    #    se convierte en un nico elemento en una lista. Para ello, primero recorremos cada\n",
    "    #    oracin, luego separamos cada oracin en palabras usando 'split()', y agregamos\n",
    "    #    todas las palabras a la lista 'words'.\n",
    "    words = [word for sentence in sentences for word in sentence.split()]\n",
    "    \n",
    "    # 2. Usamos un objeto 'Counter' para contar cuntas veces aparece cada palabra en la lista 'words'.\n",
    "    #    'word_count' es un diccionario que contiene como claves las palabras, y como valores,\n",
    "    #    la cantidad de veces que cada palabra aparece en todas las oraciones.\n",
    "    word_count = Counter(words)\n",
    "    \n",
    "    # 3. Ordenamos el diccionario de frecuencias 'word_count' de mayor a menor, es decir,\n",
    "    #    las palabras ms frecuentes aparecern primero en la lista ordenada.\n",
    "    #    'sorted_word_counts' es una lista de tuplas donde el primer elemento es la palabra\n",
    "    #    y el segundo es el conteo de ocurrencias de esa palabra.\n",
    "    sorted_word_counts = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # 4. Creamos el diccionario 'word2idx' que mapea cada palabra a un ndice nico.\n",
    "    #    El ndice comienza en 2 porque los ndices 0 y 1 se reservan para los tokens especiales '<pad>' y '<unk>'.\n",
    "    #    Usamos 'enumerate' para asignar un ndice a cada palabra ordenada, comenzando desde el ndice 2.\n",
    "    word2idx = {word: idx for idx, (word, _) in enumerate(sorted_word_counts, 2)}\n",
    "    \n",
    "    # 5. Asignamos los ndices 0 y 1 a los tokens especiales:\n",
    "    #    - '<pad>': Se usa para rellenar secuencias a un tamao constante (padding).\n",
    "    #    - '<unk>': Se usa para representar palabras desconocidas (out-of-vocabulary).\n",
    "    word2idx['<pad>'] = 0\n",
    "    word2idx['<unk>'] = 1\n",
    "    \n",
    "    # 6. Creamos un diccionario 'idx2word' que es el inverso de 'word2idx', es decir,\n",
    "    #    mapea ndices a palabras. Esto nos permite recuperar una palabra a partir de su ndice.\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    \n",
    "    # 7. Finalmente, la funcin devuelve dos diccionarios:\n",
    "    #    - 'word2idx': Mapea palabras a ndices (para usar en el modelo).\n",
    "    #    - 'idx2word': Mapea ndices a palabras (til para la interpretacin de resultados).\n",
    "    return word2idx, idx2word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "487e6fed-b683-4c2a-8c8f-f5646947a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos los vocabularios para las frases en ingls y espaol usando la funcin 'build_vocab'.\n",
    "# 'build_vocab' toma como entrada una lista de oraciones y devuelve dos diccionarios:\n",
    "# - 'word2idx': Mapea las palabras a ndices numricos.\n",
    "# - 'idx2word': Mapea los ndices numricos a palabras.\n",
    "# En este caso, estamos construyendo los vocabularios para las frases en ingls ('eng_sentences') y espaol ('spa_sentences').\n",
    "\n",
    "# Construccin del vocabulario para las frases en ingls.\n",
    "eng_word2idx, eng_idx2word = build_vocab(eng_sentences)\n",
    "# - 'eng_word2idx' ser un diccionario que asigna un ndice nico a cada palabra en las frases en ingls.\n",
    "# - 'eng_idx2word' ser el diccionario inverso, que permite obtener la palabra a partir de su ndice.\n",
    "\n",
    "# Construccin del vocabulario para las frases en espaol.\n",
    "spa_word2idx, spa_idx2word = build_vocab(spa_sentences)\n",
    "# - 'spa_word2idx' ser un diccionario similar al anterior, pero para las frases en espaol.\n",
    "# - 'spa_idx2word' ser el diccionario inverso de 'spa_word2idx', para convertir ndices en palabras.\n",
    "\n",
    "# Determinamos el tamao del vocabulario de ingls y espaol (nmero de palabras nicas en cada idioma).\n",
    "eng_vocab_size = len(eng_word2idx)\n",
    "# - 'eng_vocab_size' almacenar el nmero total de palabras nicas (incluyendo los tokens especiales <pad>, <unk>) en el vocabulario de ingls.\n",
    "\n",
    "spa_vocab_size = len(spa_word2idx)\n",
    "# - 'spa_vocab_size' almacenar el nmero total de palabras nicas (incluyendo los tokens especiales <pad>, <unk>) en el vocabulario de espaol.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4bdb6cbb-1c65-4d4c-9a5b-7bdab3deeb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27464 46617\n"
     ]
    }
   ],
   "source": [
    "print(eng_vocab_size, spa_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9338a663-7d34-4292-8957-69e0f98a2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la clase 'EngSpaDataset' que hereda de 'Dataset'.\n",
    "# Esta clase se utiliza para crear un conjunto de datos personalizado (dataset) en PyTorch.\n",
    "# El objetivo de esta clase es proporcionar una forma estructurada de acceder a las frases en ingls y espaol\n",
    "# y convertir las palabras en ndices numricos usando los vocabularios previamente construidos.\n",
    "\n",
    "class EngSpaDataset(Dataset):\n",
    "    # El constructor (__init__) se encarga de inicializar el conjunto de datos.\n",
    "    def __init__(self, eng_sentences, spa_sentences, eng_word2idx, spa_word2idx):\n",
    "        # 'eng_sentences' son las frases en ingls.\n",
    "        # 'spa_sentences' son las frases en espaol.\n",
    "        # 'eng_word2idx' es el diccionario que mapea las palabras en ingls a ndices.\n",
    "        # 'spa_word2idx' es el diccionario que mapea las palabras en espaol a ndices.\n",
    "        self.eng_sentences = eng_sentences  # Almacenamos las frases en ingls.\n",
    "        self.spa_sentences = spa_sentences  # Almacenamos las frases en espaol.\n",
    "        self.eng_word2idx = eng_word2idx  # Almacenamos el vocabulario de ingls.\n",
    "        self.spa_word2idx = spa_word2idx  # Almacenamos el vocabulario de espaol.\n",
    "        \n",
    "    # El mtodo __len__ devuelve el tamao del conjunto de datos.\n",
    "    # En este caso, el nmero de oraciones en ingls (y en espaol, que son iguales).\n",
    "    def __len__(self):\n",
    "        # Devuelve la cantidad de frases en el conjunto de datos.\n",
    "        return len(self.eng_sentences)\n",
    "    \n",
    "    # El mtodo __getitem__ obtiene un par de frases (ingls, espaol) dado un ndice 'idx'.\n",
    "    # Este mtodo ser utilizado por el DataLoader de PyTorch para acceder a los datos de manera eficiente.\n",
    "    def __getitem__(self, idx):\n",
    "        # 'eng_sentence' es la oracin en ingls en el ndice 'idx'.\n",
    "        eng_sentence = self.eng_sentences[idx]\n",
    "        # 'spa_sentence' es la oracin en espaol en el ndice 'idx'.\n",
    "        spa_sentence = self.spa_sentences[idx]\n",
    "        \n",
    "        # Convertimos la oracin en ingls en una lista de ndices, donde cada palabra se reemplaza por su ndice en el vocabulario.\n",
    "        # Si una palabra no se encuentra en el vocabulario, se utiliza el ndice del token desconocido (<unk>).\n",
    "        eng_idxs = [self.eng_word2idx.get(word, self.eng_word2idx['<unk>']) for word in eng_sentence.split()]\n",
    "        \n",
    "        # Convertimos la oracin en espaol en una lista de ndices, de manera similar a lo hecho con el ingls.\n",
    "        # Si una palabra no se encuentra en el vocabulario, se utiliza el ndice del token desconocido (<unk>).\n",
    "        spa_idxs = [self.spa_word2idx.get(word, self.spa_word2idx['<unk>']) for word in spa_sentence.split()]\n",
    "        \n",
    "        # Retornamos las oraciones convertidas en ndices como tensores de PyTorch.\n",
    "        # Los tensores son estructuras de datos que PyTorch puede procesar.\n",
    "        return torch.tensor(eng_idxs), torch.tensor(spa_idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7e0b638f-2d59-441b-a8d2-b2e516d803a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la funcin 'collate_fn', que es utilizada para procesar un lote de datos en el DataLoader.\n",
    "# Esta funcin se encarga de manejar el procesamiento de datos dentro del lote antes de ser alimentados al modelo.\n",
    "# Principalmente, realiza dos tareas importantes: \n",
    "# - Recortar o limitar las secuencias a una longitud mxima.\n",
    "# - Realizar padding (relleno) para asegurar que todas las secuencias dentro del lote tengan la misma longitud.\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 'batch' es una lista de tuplas que contienen las oraciones en ingls y espaol (en ndices).\n",
    "    # Cada elemento en 'batch' es una tupla con una secuencia en ingls y una secuencia en espaol.\n",
    "    \n",
    "    # 'eng_batch' y 'spa_batch' contienen las secuencias de ingls y espaol respectivamente.\n",
    "    # Usamos zip(*batch) para separar las oraciones de ingls y espaol en dos listas separadas.\n",
    "    eng_batch, spa_batch = zip(*batch)\n",
    "\n",
    "    # Para cada secuencia en ingls, recortamos a 'MAX_SEQ_LEN' y usamos '.clone().detach()' para evitar que los cambios\n",
    "    # en el tensor afecten al original. Esto tambin asegura que el tensor es independiente de cualquier grfico de computacin.\n",
    "    eng_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in eng_batch]\n",
    "    \n",
    "    # Para cada secuencia en espaol, tambin la recortamos a 'MAX_SEQ_LEN' y la clonamos para desvincularla.\n",
    "    spa_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in spa_batch]\n",
    "    \n",
    "    # Usamos 'torch.nn.utils.rnn.pad_sequence' para rellenar las secuencias en ingls con ceros (padding_value=0) \n",
    "    # hasta la longitud mxima en el lote. 'batch_first=True' asegura que la dimensin del batch sea la primera dimensin.\n",
    "    eng_batch = torch.nn.utils.rnn.pad_sequence(eng_batch, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Hacemos lo mismo para las secuencias en espaol, asegurando que tengan el mismo tamao en el lote.\n",
    "    spa_batch = torch.nn.utils.rnn.pad_sequence(spa_batch, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Retornamos el lote de secuencias en ingls y espaol despus de haber realizado el padding.\n",
    "    return eng_batch, spa_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ee01557c-dddd-43fa-9c81-519fae3c1288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la funcin 'train', que se encarga de entrenar el modelo durante un nmero determinado de pocas.\n",
    "# El entrenamiento se realiza usando el optimizador y la funcin de prdida proporcionados.\n",
    "# Durante cada poca, el modelo se ajusta a los datos de entrenamiento en funcin de la prdida y el gradiente calculados.\n",
    "# La funcin recibe los siguientes parmetros:\n",
    "# - 'model': El modelo que estamos entrenando.\n",
    "# - 'dataloader': El DataLoader que proporciona los lotes de datos (eng_batch, spa_batch).\n",
    "# - 'loss_function': La funcin de prdida que se utilizar para calcular el error entre la prediccin del modelo y la salida esperada.\n",
    "# - 'optimiser': El optimizador que ajustar los parmetros del modelo.\n",
    "# - 'epochs': El nmero de pocas durante las cuales el modelo ser entrenado.\n",
    "\n",
    "def train(model, dataloader, loss_function, optimiser, epochs):\n",
    "    # Configuramos el modelo en modo de entrenamiento.\n",
    "    # 'model.train()' indica que el modelo se encuentra en el estado de entrenamiento, activando comportamientos como el dropout.\n",
    "    model.train()\n",
    "    \n",
    "    # Recorremos las pocas del entrenamiento.\n",
    "    for epoch in range(epochs):\n",
    "        # Inicializamos una variable para acumular la prdida total de la poca.\n",
    "        total_loss = 0 \n",
    "        \n",
    "        # Iteramos sobre los lotes del DataLoader.\n",
    "        # En cada iteracin, 'eng_batch' es el lote de secuencias en ingls y 'spa_batch' es el lote de secuencias en espaol.\n",
    "        for i, (eng_batch, spa_batch) in enumerate(dataloader):\n",
    "            # Movemos los lotes al dispositivo (CPU o GPU) donde se encuentra el modelo.\n",
    "            eng_batch = eng_batch.to(device)\n",
    "            spa_batch = spa_batch.to(device)\n",
    "            \n",
    "            # Preprocesamiento del decoder:\n",
    "            # Separamos el lote de salida del espaol en dos partes:\n",
    "            # - 'target_input' es la entrada al decoder (todas las palabras excepto el ltimo token de cada secuencia).\n",
    "            # - 'target_output' es la salida esperada del decoder (todas las palabras excepto el primero de cada secuencia).\n",
    "            target_input = spa_batch[:, :-1]\n",
    "            target_output = spa_batch[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            # Ponemos a cero los gradientes acumulados de las iteraciones anteriores para evitar que se acumulen en el siguiente paso.\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            # Realizamos una pasada hacia adelante a travs del modelo con el batch de ingls y el input del decoder.\n",
    "            output = model(eng_batch, target_input)\n",
    "            \n",
    "            # Aplanamos la salida del modelo para que tenga la forma necesaria para calcular la prdida.\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            \n",
    "            # Calculamos la prdida entre la salida del modelo y la salida esperada (target_output).\n",
    "            loss = loss_function(output, target_output)\n",
    "            \n",
    "            # Calculamos los gradientes a partir de la prdida.\n",
    "            loss.backward()\n",
    "            \n",
    "            # Actualizamos los parmetros del modelo usando el optimizador.\n",
    "            optimiser.step()\n",
    "            \n",
    "            # Sumamos la prdida de esta iteracin a la prdida total.\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Calculamos la prdida promedio de la poca.\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        \n",
    "        # Imprimimos la prdida promedio por cada poca.\n",
    "        print(f'Epoch: {epoch}/{epochs}, Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "06a0b778-9467-4f53-a95a-c16bce30c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el tamao del lote (batch) para el entrenamiento. En este caso, el tamao del lote es 64.\n",
    "BATCH_SIZE = 8 #64\n",
    "\n",
    "# Creamos una instancia del dataset que contiene las frases en ingls y espaol, junto con sus respectivos diccionarios de ndices (eng_word2idx y spa_word2idx).\n",
    "# 'EngSpaDataset' es una clase que hereda de 'Dataset' y est diseada para gestionar pares de frases en ingls y espaol,\n",
    "# as como la conversin de esas frases en listas de ndices de palabras (usando los diccionarios 'eng_word2idx' y 'spa_word2idx').\n",
    "dataset = EngSpaDataset(eng_sentences, spa_sentences, eng_word2idx, spa_word2idx)\n",
    "\n",
    "# Creamos un DataLoader, que es una herramienta que se encarga de cargar los datos en lotes para el entrenamiento.\n",
    "# 'DataLoader' toma como entrada el dataset, el tamao del lote (BATCH_SIZE), si se deben barajar los datos (shuffle=True),\n",
    "# y una funcin personalizada de 'collate_fn' que prepara los lotes para el entrenamiento.\n",
    "# El DataLoader permite iterar sobre el dataset en pequeos lotes, facilitando el entrenamiento del modelo.\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1a3ec03f-2e62-475f-82b5-dec0adc537ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una instancia del modelo Transformer utilizando los hiperparmetros especificados.\n",
    "\n",
    "# 'd_model=512': Establece la dimensionalidad de los vectores de caractersticas en el modelo Transformer. \n",
    "# Esto significa que cada token ser representado por un vector de 512 dimensiones, lo que ayuda al modelo a capturar relaciones complejas entre las palabras. \n",
    "# Un valor comnmente usado es 512, que proporciona un buen equilibrio entre poder de representacin y eficiencia computacional.\n",
    "\n",
    "# 'num_heads=8': Define el nmero de \"cabezas\" en la atencin multi-cabeza (multi-head attention). \n",
    "# Cada cabeza de atencin aprende una representacin diferente de cada token, permitiendo al modelo capturar distintas relaciones y patrones. \n",
    "# En este caso, el modelo tendr 8 cabezas de atencin, lo que mejora su capacidad de modelado.\n",
    "\n",
    "# 'd_ff=2048': Establece el tamao de la capa de alimentacin directa (feed-forward layer). \n",
    "# Esta capa toma las salidas de la atencin multi-cabeza y las transforma. El valor de 2048 significa que la capa tendr 2048 neuronas, lo que aumenta la capacidad del modelo para aprender transformaciones complejas.\n",
    "\n",
    "# 'num_layers=6': Define el nmero de capas tanto en el encoder como en el decoder. \n",
    "# Esto indica cuntas veces se aplicar la atencin multi-cabeza y la red de alimentacin directa en cada uno de estos componentes. \n",
    "# Con 6 capas, el modelo tendr una capacidad mayor de capturar patrones complejos, aunque tambin aumenta el costo computacional.\n",
    "\n",
    "# 'input_vocab_size=eng_vocab_size': Especifica el tamao del vocabulario de entrada, que corresponde al nmero de palabras nicas en las frases en ingls. \n",
    "# 'eng_vocab_size' es el tamao del vocabulario de las frases en ingls, y este valor se utiliza para definir la capa de embedding de entrada del modelo.\n",
    "\n",
    "# 'target_vocab_size=spa_vocab_size': Define el tamao del vocabulario de salida, correspondiente al nmero de palabras nicas en las frases en espaol. \n",
    "# 'spa_vocab_size' es el tamao del vocabulario de las frases en espaol, y se usa para la capa de embedding en el decoder, as como para la capa de salida del modelo.\n",
    "\n",
    "# 'max_len=MAX_SEQ_LEN': Establece la longitud mxima de las secuencias que el modelo puede procesar. \n",
    "# 'MAX_SEQ_LEN' es una constante que define cuntos tokens (palabras o subpalabras) como mximo el modelo aceptar en las secuencias de entrada y salida. \n",
    "# Esto asegura que el modelo pueda manejar secuencias de longitud variable sin quedar limitado.\n",
    "\n",
    "# 'dropout=0.1': Define la tasa de dropout que se aplica para la regularizacin durante el entrenamiento. \n",
    "# En este caso, el valor de 0.1 significa que el modelo ignorar aleatoriamente el 10% de las conexiones durante el entrenamiento para prevenir el sobreajuste (overfitting).\n",
    "\n",
    "model = Transformer(d_model=128, num_heads=4, d_ff=512, num_layers=2, #d_model=512  d_ff=2048 num_layer=6\n",
    "                    input_vocab_size=eng_vocab_size, target_vocab_size=spa_vocab_size,\n",
    "                    max_len=MAX_SEQ_LEN, dropout=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f7a6bfa1-2cf2-4d1b-89fe-bd8d981b2db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movemos el modelo al dispositivo especificado (CPU o GPU). Esto asegura que las operaciones del modelo se realicen en el dispositivo que se haya configurado previamente (por ejemplo, 'cuda' si hay una GPU disponible o 'cpu' si no).\n",
    "# El dispositivo se define antes en el cdigo y se usa para asegurar que tanto el modelo como los datos estn en el mismo lugar para ser procesados.\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Definimos la funcin de prdida (loss function). \n",
    "# En este caso, utilizamos 'CrossEntropyLoss', que es adecuada para tareas de clasificacin mltiple como la traduccin de secuencias.\n",
    "# 'ignore_index=0' le indica a la funcin de prdida que ignore las posiciones de los tokens de padding, es decir, los ndices con valor 0.\n",
    "# Esto es importante porque el padding no debe afectar al clculo de la prdida, ya que no corresponde a una palabra o token real.\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Definimos el optimizador que se usar durante el entrenamiento. En este caso, usamos el optimizador 'Adam', que es un optimizador muy popular y eficiente.\n",
    "# 'model.parameters()' obtiene todos los parmetros del modelo que se actualizarn durante el entrenamiento.\n",
    "# 'lr=0.0001' especifica la tasa de aprendizaje (learning rate), que controla qu tan rpido el optimizador ajusta los parmetros durante el entrenamiento. \n",
    "# En este caso, la tasa de aprendizaje es pequea para hacer ajustes finos en los parmetros durante el entrenamiento.\n",
    "\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bfde355d-9e15-4a9b-9842-d6c2472c5820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Batch 0/33266 - Loss: 10.9242\n",
      "Batch 100/33266 - Loss: 9.0613\n",
      "Batch 200/33266 - Loss: 7.3750\n",
      "Batch 300/33266 - Loss: 7.1300\n",
      "Batch 400/33266 - Loss: 7.0763\n",
      "Batch 500/33266 - Loss: 6.1902\n",
      "Batch 600/33266 - Loss: 5.9182\n",
      "Batch 700/33266 - Loss: 7.1199\n",
      "Batch 800/33266 - Loss: 6.4075\n",
      "Batch 900/33266 - Loss: 6.2202\n",
      "Batch 1000/33266 - Loss: 6.1031\n",
      "Batch 1100/33266 - Loss: 6.1848\n",
      "Batch 1200/33266 - Loss: 5.7866\n",
      "Batch 1300/33266 - Loss: 6.1444\n",
      "Batch 1400/33266 - Loss: 5.4407\n",
      "Batch 1500/33266 - Loss: 5.0448\n",
      "Batch 1600/33266 - Loss: 6.5918\n",
      "Batch 1700/33266 - Loss: 5.5889\n",
      "Batch 1800/33266 - Loss: 5.7957\n",
      "Batch 1900/33266 - Loss: 5.4905\n",
      "Batch 2000/33266 - Loss: 6.2380\n",
      "Batch 2100/33266 - Loss: 6.4696\n",
      "Batch 2200/33266 - Loss: 6.1172\n",
      "Batch 2300/33266 - Loss: 4.9554\n",
      "Batch 2400/33266 - Loss: 5.5033\n",
      "Batch 2500/33266 - Loss: 5.2688\n",
      "Batch 2600/33266 - Loss: 5.8008\n",
      "Batch 2700/33266 - Loss: 5.4825\n",
      "Batch 2800/33266 - Loss: 6.4351\n",
      "Batch 2900/33266 - Loss: 5.6467\n",
      "Batch 3000/33266 - Loss: 5.2075\n",
      "Batch 3100/33266 - Loss: 5.8220\n",
      "Batch 3200/33266 - Loss: 5.7164\n",
      "Batch 3300/33266 - Loss: 5.6002\n",
      "Batch 3400/33266 - Loss: 4.8707\n",
      "Batch 3500/33266 - Loss: 5.1638\n",
      "Batch 3600/33266 - Loss: 5.5422\n",
      "Batch 3700/33266 - Loss: 5.9154\n",
      "Batch 3800/33266 - Loss: 5.1926\n",
      "Batch 3900/33266 - Loss: 5.9067\n",
      "Batch 4000/33266 - Loss: 5.4142\n",
      "Batch 4100/33266 - Loss: 5.4570\n",
      "Batch 4200/33266 - Loss: 5.1392\n",
      "Batch 4300/33266 - Loss: 5.0570\n",
      "Batch 4400/33266 - Loss: 5.3275\n",
      "Batch 4500/33266 - Loss: 5.1167\n",
      "Batch 4600/33266 - Loss: 5.2351\n",
      "Batch 4700/33266 - Loss: 5.2183\n",
      "Batch 4800/33266 - Loss: 5.0930\n",
      "Batch 4900/33266 - Loss: 5.0306\n",
      "Batch 5000/33266 - Loss: 5.5261\n",
      "Batch 5100/33266 - Loss: 5.7092\n",
      "Batch 5200/33266 - Loss: 5.8981\n",
      "Batch 5300/33266 - Loss: 5.3928\n",
      "Batch 5400/33266 - Loss: 4.7973\n",
      "Batch 5500/33266 - Loss: 5.1518\n",
      "Batch 5600/33266 - Loss: 4.8559\n",
      "Batch 5700/33266 - Loss: 6.3391\n",
      "Batch 5800/33266 - Loss: 5.3425\n",
      "Batch 5900/33266 - Loss: 5.0656\n",
      "Batch 6000/33266 - Loss: 5.9761\n",
      "Batch 6100/33266 - Loss: 5.2252\n",
      "Batch 6200/33266 - Loss: 5.1177\n",
      "Batch 6300/33266 - Loss: 4.9031\n",
      "Batch 6400/33266 - Loss: 5.1666\n",
      "Batch 6500/33266 - Loss: 5.0896\n",
      "Batch 6600/33266 - Loss: 4.9666\n",
      "Batch 6700/33266 - Loss: 5.4438\n",
      "Batch 6800/33266 - Loss: 4.8637\n",
      "Batch 6900/33266 - Loss: 4.6188\n",
      "Batch 7000/33266 - Loss: 5.7294\n",
      "Batch 7100/33266 - Loss: 5.4286\n",
      "Batch 7200/33266 - Loss: 5.0366\n",
      "Batch 7300/33266 - Loss: 4.8344\n",
      "Batch 7400/33266 - Loss: 4.7574\n",
      "Batch 7500/33266 - Loss: 4.6973\n",
      "Batch 7600/33266 - Loss: 5.1413\n",
      "Batch 7700/33266 - Loss: 4.8341\n",
      "Batch 7800/33266 - Loss: 6.0383\n",
      "Batch 7900/33266 - Loss: 4.4321\n",
      "Batch 8000/33266 - Loss: 5.5566\n",
      "Batch 8100/33266 - Loss: 4.1759\n",
      "Batch 8200/33266 - Loss: 4.4813\n",
      "Batch 8300/33266 - Loss: 4.8629\n",
      "Batch 8400/33266 - Loss: 4.5626\n",
      "Batch 8500/33266 - Loss: 5.3841\n",
      "Batch 8600/33266 - Loss: 4.6528\n",
      "Batch 8700/33266 - Loss: 5.2755\n",
      "Batch 8800/33266 - Loss: 4.3647\n",
      "Batch 8900/33266 - Loss: 4.9445\n",
      "Batch 9000/33266 - Loss: 4.1803\n",
      "Batch 9100/33266 - Loss: 5.4052\n",
      "Batch 9200/33266 - Loss: 4.5618\n",
      "Batch 9300/33266 - Loss: 4.5552\n",
      "Batch 9400/33266 - Loss: 5.5408\n",
      "Batch 9500/33266 - Loss: 5.1460\n",
      "Batch 9600/33266 - Loss: 4.5498\n",
      "Batch 9700/33266 - Loss: 5.0316\n",
      "Batch 9800/33266 - Loss: 5.0452\n",
      "Batch 9900/33266 - Loss: 5.3746\n",
      "Batch 10000/33266 - Loss: 4.6481\n",
      "Batch 10100/33266 - Loss: 5.1540\n",
      "Batch 10200/33266 - Loss: 4.4006\n",
      "Batch 10300/33266 - Loss: 4.6448\n",
      "Batch 10400/33266 - Loss: 4.8185\n",
      "Batch 10500/33266 - Loss: 4.8562\n",
      "Batch 10600/33266 - Loss: 5.4934\n",
      "Batch 10700/33266 - Loss: 4.2330\n",
      "Batch 10800/33266 - Loss: 5.0893\n",
      "Batch 10900/33266 - Loss: 4.2054\n",
      "Batch 11000/33266 - Loss: 4.2888\n",
      "Batch 11100/33266 - Loss: 5.9249\n",
      "Batch 11200/33266 - Loss: 4.7505\n",
      "Batch 11300/33266 - Loss: 4.0647\n",
      "Batch 11400/33266 - Loss: 5.2136\n",
      "Batch 11500/33266 - Loss: 3.7803\n",
      "Batch 11600/33266 - Loss: 5.1869\n",
      "Batch 11700/33266 - Loss: 4.4215\n",
      "Batch 11800/33266 - Loss: 5.8088\n",
      "Batch 11900/33266 - Loss: 4.3178\n",
      "Batch 12000/33266 - Loss: 4.1515\n",
      "Batch 12100/33266 - Loss: 5.0245\n",
      "Batch 12200/33266 - Loss: 5.4094\n",
      "Batch 12300/33266 - Loss: 5.5038\n",
      "Batch 12400/33266 - Loss: 5.0863\n",
      "Batch 12500/33266 - Loss: 4.3810\n",
      "Batch 12600/33266 - Loss: 4.5812\n",
      "Batch 12700/33266 - Loss: 4.6105\n",
      "Batch 12800/33266 - Loss: 3.8589\n",
      "Batch 12900/33266 - Loss: 6.1425\n",
      "Batch 13000/33266 - Loss: 3.8066\n",
      "Batch 13100/33266 - Loss: 4.2351\n",
      "Batch 13200/33266 - Loss: 4.5833\n",
      "Batch 13300/33266 - Loss: 4.2852\n",
      "Batch 13400/33266 - Loss: 4.1323\n",
      "Batch 13500/33266 - Loss: 5.7407\n",
      "Batch 13600/33266 - Loss: 4.4800\n",
      "Batch 13700/33266 - Loss: 3.7078\n",
      "Batch 13800/33266 - Loss: 3.4125\n",
      "Batch 13900/33266 - Loss: 4.4853\n",
      "Batch 14000/33266 - Loss: 4.3335\n",
      "Batch 14100/33266 - Loss: 4.1109\n",
      "Batch 14200/33266 - Loss: 3.9883\n",
      "Batch 14300/33266 - Loss: 3.9653\n",
      "Batch 14400/33266 - Loss: 5.2154\n",
      "Batch 14500/33266 - Loss: 4.3175\n",
      "Batch 14600/33266 - Loss: 5.4086\n",
      "Batch 14700/33266 - Loss: 4.8271\n",
      "Batch 14800/33266 - Loss: 6.0086\n",
      "Batch 14900/33266 - Loss: 3.5555\n",
      "Batch 15000/33266 - Loss: 4.6535\n",
      "Batch 15100/33266 - Loss: 3.8412\n",
      "Batch 15200/33266 - Loss: 3.8333\n",
      "Batch 15300/33266 - Loss: 3.5921\n",
      "Batch 15400/33266 - Loss: 4.2289\n",
      "Batch 15500/33266 - Loss: 3.8843\n",
      "Batch 15600/33266 - Loss: 3.4569\n",
      "Batch 15700/33266 - Loss: 5.3978\n",
      "Batch 15800/33266 - Loss: 3.0660\n",
      "Batch 15900/33266 - Loss: 5.0264\n",
      "Batch 16000/33266 - Loss: 4.5776\n",
      "Batch 16100/33266 - Loss: 4.0998\n",
      "Batch 16200/33266 - Loss: 4.4768\n",
      "Batch 16300/33266 - Loss: 5.0496\n",
      "Batch 16400/33266 - Loss: 3.8810\n",
      "Batch 16500/33266 - Loss: 4.3422\n",
      "Batch 16600/33266 - Loss: 4.9451\n",
      "Batch 16700/33266 - Loss: 5.2204\n",
      "Batch 16800/33266 - Loss: 4.6950\n",
      "Batch 16900/33266 - Loss: 5.0993\n",
      "Batch 17000/33266 - Loss: 4.6659\n",
      "Batch 17100/33266 - Loss: 4.0153\n",
      "Batch 17200/33266 - Loss: 4.7819\n",
      "Batch 17300/33266 - Loss: 4.2658\n",
      "Batch 17400/33266 - Loss: 3.7114\n",
      "Batch 17500/33266 - Loss: 4.6079\n",
      "Batch 17600/33266 - Loss: 4.8000\n",
      "Batch 17700/33266 - Loss: 5.0859\n",
      "Batch 17800/33266 - Loss: 4.4061\n",
      "Batch 17900/33266 - Loss: 4.2886\n",
      "Batch 18000/33266 - Loss: 3.2138\n",
      "Batch 18100/33266 - Loss: 4.0271\n",
      "Batch 18200/33266 - Loss: 4.6446\n",
      "Batch 18300/33266 - Loss: 3.7706\n",
      "Batch 18400/33266 - Loss: 4.7983\n",
      "Batch 18500/33266 - Loss: 3.5656\n",
      "Batch 18600/33266 - Loss: 4.5711\n",
      "Batch 18700/33266 - Loss: 3.6740\n",
      "Batch 18800/33266 - Loss: 4.2053\n",
      "Batch 18900/33266 - Loss: 2.8906\n",
      "Batch 19000/33266 - Loss: 4.3853\n",
      "Batch 19100/33266 - Loss: 3.7011\n",
      "Batch 19200/33266 - Loss: 4.8757\n",
      "Batch 19300/33266 - Loss: 4.3975\n",
      "Batch 19400/33266 - Loss: 4.2949\n",
      "Batch 19500/33266 - Loss: 4.7105\n",
      "Batch 19600/33266 - Loss: 4.4971\n",
      "Batch 19700/33266 - Loss: 3.9354\n",
      "Batch 19800/33266 - Loss: 4.4281\n",
      "Batch 19900/33266 - Loss: 4.3868\n",
      "Batch 20000/33266 - Loss: 3.5845\n",
      "Batch 20100/33266 - Loss: 2.4904\n",
      "Batch 20200/33266 - Loss: 4.5337\n",
      "Batch 20300/33266 - Loss: 4.9815\n",
      "Batch 20400/33266 - Loss: 4.1484\n",
      "Batch 20500/33266 - Loss: 4.5510\n",
      "Batch 20600/33266 - Loss: 3.1121\n",
      "Batch 20700/33266 - Loss: 4.5199\n",
      "Batch 20800/33266 - Loss: 3.0317\n",
      "Batch 20900/33266 - Loss: 4.4116\n",
      "Batch 21000/33266 - Loss: 4.2578\n",
      "Batch 21100/33266 - Loss: 3.7864\n",
      "Batch 21200/33266 - Loss: 4.4395\n",
      "Batch 21300/33266 - Loss: 3.9796\n",
      "Batch 21400/33266 - Loss: 4.1444\n",
      "Batch 21500/33266 - Loss: 4.9070\n",
      "Batch 21600/33266 - Loss: 3.9066\n",
      "Batch 21700/33266 - Loss: 4.1370\n",
      "Batch 21800/33266 - Loss: 3.8498\n",
      "Batch 21900/33266 - Loss: 2.8835\n",
      "Batch 22000/33266 - Loss: 5.1905\n",
      "Batch 22100/33266 - Loss: 3.6422\n",
      "Batch 22200/33266 - Loss: 3.7276\n",
      "Batch 22300/33266 - Loss: 3.7537\n",
      "Batch 22400/33266 - Loss: 5.3437\n",
      "Batch 22500/33266 - Loss: 4.6122\n",
      "Batch 22600/33266 - Loss: 3.9800\n",
      "Batch 22700/33266 - Loss: 2.7570\n",
      "Batch 22800/33266 - Loss: 4.0051\n",
      "Batch 22900/33266 - Loss: 3.1108\n",
      "Batch 23000/33266 - Loss: 4.4285\n",
      "Batch 23100/33266 - Loss: 3.9274\n",
      "Batch 23200/33266 - Loss: 4.6021\n",
      "Batch 23300/33266 - Loss: 3.9103\n",
      "Batch 23400/33266 - Loss: 4.1949\n",
      "Batch 23500/33266 - Loss: 4.0632\n",
      "Batch 23600/33266 - Loss: 4.3681\n",
      "Batch 23700/33266 - Loss: 4.3355\n",
      "Batch 23800/33266 - Loss: 3.1346\n",
      "Batch 23900/33266 - Loss: 5.0669\n",
      "Batch 24000/33266 - Loss: 4.9711\n",
      "Batch 24100/33266 - Loss: 3.1368\n",
      "Batch 24200/33266 - Loss: 3.9228\n",
      "Batch 24300/33266 - Loss: 3.3223\n",
      "Batch 24400/33266 - Loss: 3.9458\n",
      "Batch 24500/33266 - Loss: 3.8800\n",
      "Batch 24600/33266 - Loss: 4.0092\n",
      "Batch 24700/33266 - Loss: 3.3291\n",
      "Batch 24800/33266 - Loss: 4.0785\n",
      "Batch 24900/33266 - Loss: 2.9680\n",
      "Batch 25000/33266 - Loss: 4.1317\n",
      "Batch 25100/33266 - Loss: 3.4548\n",
      "Batch 25200/33266 - Loss: 2.7869\n",
      "Batch 25300/33266 - Loss: 3.7979\n",
      "Batch 25400/33266 - Loss: 4.0043\n",
      "Batch 25500/33266 - Loss: 4.2140\n",
      "Batch 25600/33266 - Loss: 4.5097\n",
      "Batch 25700/33266 - Loss: 3.9252\n",
      "Batch 25800/33266 - Loss: 3.7887\n",
      "Batch 25900/33266 - Loss: 3.7451\n",
      "Batch 26000/33266 - Loss: 4.2032\n",
      "Batch 26100/33266 - Loss: 3.7433\n",
      "Batch 26200/33266 - Loss: 5.1716\n",
      "Batch 26300/33266 - Loss: 4.7967\n",
      "Batch 26400/33266 - Loss: 3.7495\n",
      "Batch 26500/33266 - Loss: 3.3747\n",
      "Batch 26600/33266 - Loss: 4.0147\n",
      "Batch 26700/33266 - Loss: 3.2216\n",
      "Batch 26800/33266 - Loss: 3.0907\n",
      "Batch 26900/33266 - Loss: 3.1976\n",
      "Batch 27000/33266 - Loss: 3.6027\n",
      "Batch 27100/33266 - Loss: 3.3241\n",
      "Batch 27200/33266 - Loss: 3.5510\n",
      "Batch 27300/33266 - Loss: 4.7882\n",
      "Batch 27400/33266 - Loss: 3.0103\n",
      "Batch 27500/33266 - Loss: 4.0238\n",
      "Batch 27600/33266 - Loss: 4.0328\n",
      "Batch 27700/33266 - Loss: 3.8695\n",
      "Batch 27800/33266 - Loss: 3.9663\n",
      "Batch 27900/33266 - Loss: 3.4728\n",
      "Batch 28000/33266 - Loss: 4.1686\n",
      "Batch 28100/33266 - Loss: 2.7097\n",
      "Batch 28200/33266 - Loss: 3.2380\n",
      "Batch 28300/33266 - Loss: 3.2572\n",
      "Batch 28400/33266 - Loss: 3.3992\n",
      "Batch 28500/33266 - Loss: 4.1329\n",
      "Batch 28600/33266 - Loss: 4.7049\n",
      "Batch 28700/33266 - Loss: 3.5707\n",
      "Batch 28800/33266 - Loss: 3.8655\n",
      "Batch 28900/33266 - Loss: 4.2714\n",
      "Batch 29000/33266 - Loss: 2.6656\n",
      "Batch 29100/33266 - Loss: 4.4692\n",
      "Batch 29200/33266 - Loss: 3.5652\n",
      "Batch 29300/33266 - Loss: 4.1863\n",
      "Batch 29400/33266 - Loss: 4.1076\n",
      "Batch 29500/33266 - Loss: 5.6849\n",
      "Batch 29600/33266 - Loss: 4.4027\n",
      "Batch 29700/33266 - Loss: 2.7001\n",
      "Batch 29800/33266 - Loss: 4.7466\n",
      "Batch 29900/33266 - Loss: 2.9333\n",
      "Batch 30000/33266 - Loss: 4.0224\n",
      "Batch 30100/33266 - Loss: 3.7753\n",
      "Batch 30200/33266 - Loss: 3.5964\n",
      "Batch 30300/33266 - Loss: 4.4378\n",
      "Batch 30400/33266 - Loss: 3.9412\n",
      "Batch 30500/33266 - Loss: 3.4480\n",
      "Batch 30600/33266 - Loss: 3.7300\n",
      "Batch 30700/33266 - Loss: 4.1584\n",
      "Batch 30800/33266 - Loss: 4.0348\n",
      "Batch 30900/33266 - Loss: 2.9859\n",
      "Batch 31000/33266 - Loss: 4.0990\n",
      "Batch 31100/33266 - Loss: 3.4624\n",
      "Batch 31200/33266 - Loss: 4.4864\n",
      "Batch 31300/33266 - Loss: 3.5909\n",
      "Batch 31400/33266 - Loss: 3.8617\n",
      "Batch 31500/33266 - Loss: 2.6276\n",
      "Batch 31600/33266 - Loss: 3.9063\n",
      "Batch 31700/33266 - Loss: 3.8467\n",
      "Batch 31800/33266 - Loss: 2.8090\n",
      "Batch 31900/33266 - Loss: 4.1608\n",
      "Batch 32000/33266 - Loss: 4.3763\n",
      "Batch 32100/33266 - Loss: 3.1793\n",
      "Batch 32200/33266 - Loss: 3.0790\n",
      "Batch 32300/33266 - Loss: 3.1710\n",
      "Batch 32400/33266 - Loss: 4.6785\n",
      "Batch 32500/33266 - Loss: 3.4673\n",
      "Batch 32600/33266 - Loss: 3.7445\n",
      "Batch 32700/33266 - Loss: 3.4031\n",
      "Batch 32800/33266 - Loss: 2.6746\n",
      "Batch 32900/33266 - Loss: 3.0234\n",
      "Batch 33000/33266 - Loss: 4.5992\n",
      "Batch 33100/33266 - Loss: 3.5419\n",
      "Batch 33200/33266 - Loss: 4.2512\n",
      "End of Epoch 1 - Avg Loss: 4.4769\n",
      "Epoch 2/2\n",
      "Batch 0/33266 - Loss: 4.2512\n",
      "Batch 100/33266 - Loss: 3.3244\n",
      "Batch 200/33266 - Loss: 5.5101\n",
      "Batch 300/33266 - Loss: 3.7010\n",
      "Batch 400/33266 - Loss: 2.8420\n",
      "Batch 500/33266 - Loss: 3.8399\n",
      "Batch 600/33266 - Loss: 3.1281\n",
      "Batch 700/33266 - Loss: 3.5935\n",
      "Batch 800/33266 - Loss: 2.6578\n",
      "Batch 900/33266 - Loss: 3.9879\n",
      "Batch 1000/33266 - Loss: 4.1459\n",
      "Batch 1100/33266 - Loss: 3.5390\n",
      "Batch 1200/33266 - Loss: 5.0442\n",
      "Batch 1300/33266 - Loss: 2.7790\n",
      "Batch 1400/33266 - Loss: 3.1313\n",
      "Batch 1500/33266 - Loss: 3.7094\n",
      "Batch 1600/33266 - Loss: 2.8959\n",
      "Batch 1700/33266 - Loss: 3.1525\n",
      "Batch 1800/33266 - Loss: 3.6002\n",
      "Batch 1900/33266 - Loss: 2.9278\n",
      "Batch 2000/33266 - Loss: 4.4629\n",
      "Batch 2100/33266 - Loss: 2.6107\n",
      "Batch 2200/33266 - Loss: 4.4832\n",
      "Batch 2300/33266 - Loss: 4.1575\n",
      "Batch 2400/33266 - Loss: 3.4802\n",
      "Batch 2500/33266 - Loss: 3.6071\n",
      "Batch 2600/33266 - Loss: 2.8290\n",
      "Batch 2700/33266 - Loss: 3.3719\n",
      "Batch 2800/33266 - Loss: 2.7764\n",
      "Batch 2900/33266 - Loss: 3.9324\n",
      "Batch 3000/33266 - Loss: 3.7244\n",
      "Batch 3100/33266 - Loss: 3.7348\n",
      "Batch 3200/33266 - Loss: 4.3014\n",
      "Batch 3300/33266 - Loss: 4.0706\n",
      "Batch 3400/33266 - Loss: 3.9373\n",
      "Batch 3500/33266 - Loss: 3.1874\n",
      "Batch 3600/33266 - Loss: 3.9465\n",
      "Batch 3700/33266 - Loss: 3.2670\n",
      "Batch 3800/33266 - Loss: 3.5322\n",
      "Batch 3900/33266 - Loss: 3.3868\n",
      "Batch 4000/33266 - Loss: 2.8973\n",
      "Batch 4100/33266 - Loss: 3.1377\n",
      "Batch 4200/33266 - Loss: 3.7823\n",
      "Batch 4300/33266 - Loss: 3.6606\n",
      "Batch 4400/33266 - Loss: 2.7615\n",
      "Batch 4500/33266 - Loss: 3.2922\n",
      "Batch 4600/33266 - Loss: 2.9936\n",
      "Batch 4700/33266 - Loss: 4.7306\n",
      "Batch 4800/33266 - Loss: 2.8205\n",
      "Batch 4900/33266 - Loss: 3.3535\n",
      "Batch 5000/33266 - Loss: 3.7682\n",
      "Batch 5100/33266 - Loss: 4.0891\n",
      "Batch 5200/33266 - Loss: 3.8238\n",
      "Batch 5300/33266 - Loss: 2.7501\n",
      "Batch 5400/33266 - Loss: 3.2538\n",
      "Batch 5500/33266 - Loss: 4.3641\n",
      "Batch 5600/33266 - Loss: 4.4501\n",
      "Batch 5700/33266 - Loss: 3.6142\n",
      "Batch 5800/33266 - Loss: 3.1178\n",
      "Batch 5900/33266 - Loss: 4.2399\n",
      "Batch 6000/33266 - Loss: 3.6664\n",
      "Batch 6100/33266 - Loss: 3.5829\n",
      "Batch 6200/33266 - Loss: 3.8116\n",
      "Batch 6300/33266 - Loss: 4.5293\n",
      "Batch 6400/33266 - Loss: 3.5207\n",
      "Batch 6500/33266 - Loss: 3.2525\n",
      "Batch 6600/33266 - Loss: 2.9350\n",
      "Batch 6700/33266 - Loss: 2.2124\n",
      "Batch 6800/33266 - Loss: 2.4979\n",
      "Batch 6900/33266 - Loss: 3.1909\n",
      "Batch 7000/33266 - Loss: 4.4852\n",
      "Batch 7100/33266 - Loss: 2.9757\n",
      "Batch 7200/33266 - Loss: 2.9686\n",
      "Batch 7300/33266 - Loss: 3.3195\n",
      "Batch 7400/33266 - Loss: 3.3453\n",
      "Batch 7500/33266 - Loss: 3.7022\n",
      "Batch 7600/33266 - Loss: 3.1226\n",
      "Batch 7700/33266 - Loss: 3.6116\n",
      "Batch 7800/33266 - Loss: 4.1223\n",
      "Batch 7900/33266 - Loss: 3.3845\n",
      "Batch 8000/33266 - Loss: 4.0101\n",
      "Batch 8100/33266 - Loss: 2.4417\n",
      "Batch 8200/33266 - Loss: 3.2204\n",
      "Batch 8300/33266 - Loss: 3.3111\n",
      "Batch 8400/33266 - Loss: 4.0225\n",
      "Batch 8500/33266 - Loss: 4.1002\n",
      "Batch 8600/33266 - Loss: 3.1211\n",
      "Batch 8700/33266 - Loss: 3.4977\n",
      "Batch 8800/33266 - Loss: 2.6448\n",
      "Batch 8900/33266 - Loss: 3.4294\n",
      "Batch 9000/33266 - Loss: 4.2368\n",
      "Batch 9100/33266 - Loss: 3.1086\n",
      "Batch 9200/33266 - Loss: 3.8282\n",
      "Batch 9300/33266 - Loss: 3.4999\n",
      "Batch 9400/33266 - Loss: 4.3517\n",
      "Batch 9500/33266 - Loss: 2.6648\n",
      "Batch 9600/33266 - Loss: 1.9395\n",
      "Batch 9700/33266 - Loss: 3.4006\n",
      "Batch 9800/33266 - Loss: 3.6096\n",
      "Batch 9900/33266 - Loss: 2.7679\n",
      "Batch 10000/33266 - Loss: 3.1261\n",
      "Batch 10100/33266 - Loss: 3.3046\n",
      "Batch 10200/33266 - Loss: 3.6061\n",
      "Batch 10300/33266 - Loss: 3.2740\n",
      "Batch 10400/33266 - Loss: 3.2702\n",
      "Batch 10500/33266 - Loss: 4.2660\n",
      "Batch 10600/33266 - Loss: 3.2495\n",
      "Batch 10700/33266 - Loss: 3.6649\n",
      "Batch 10800/33266 - Loss: 3.4634\n",
      "Batch 10900/33266 - Loss: 3.6068\n",
      "Batch 11000/33266 - Loss: 3.5028\n",
      "Batch 11100/33266 - Loss: 3.8587\n",
      "Batch 11200/33266 - Loss: 3.3099\n",
      "Batch 11300/33266 - Loss: 3.2844\n",
      "Batch 11400/33266 - Loss: 2.9759\n",
      "Batch 11500/33266 - Loss: 3.4918\n",
      "Batch 11600/33266 - Loss: 3.0511\n",
      "Batch 11700/33266 - Loss: 2.4156\n",
      "Batch 11800/33266 - Loss: 3.1610\n",
      "Batch 11900/33266 - Loss: 3.1791\n",
      "Batch 12000/33266 - Loss: 4.2503\n",
      "Batch 12100/33266 - Loss: 3.2256\n",
      "Batch 12200/33266 - Loss: 3.2251\n",
      "Batch 12300/33266 - Loss: 3.1789\n",
      "Batch 12400/33266 - Loss: 3.4597\n",
      "Batch 12500/33266 - Loss: 3.7460\n",
      "Batch 12600/33266 - Loss: 2.9115\n",
      "Batch 12700/33266 - Loss: 2.5665\n",
      "Batch 12800/33266 - Loss: 2.8835\n",
      "Batch 12900/33266 - Loss: 3.6349\n",
      "Batch 13000/33266 - Loss: 2.5187\n",
      "Batch 13100/33266 - Loss: 2.5472\n",
      "Batch 13200/33266 - Loss: 5.2902\n",
      "Batch 13300/33266 - Loss: 2.9692\n",
      "Batch 13400/33266 - Loss: 3.1096\n",
      "Batch 13500/33266 - Loss: 3.5300\n",
      "Batch 13600/33266 - Loss: 2.8035\n",
      "Batch 13700/33266 - Loss: 3.2632\n",
      "Batch 13800/33266 - Loss: 3.9149\n",
      "Batch 13900/33266 - Loss: 3.6204\n",
      "Batch 14000/33266 - Loss: 3.2633\n",
      "Batch 14100/33266 - Loss: 2.9251\n",
      "Batch 14200/33266 - Loss: 2.9604\n",
      "Batch 14300/33266 - Loss: 3.1739\n",
      "Batch 14400/33266 - Loss: 5.0089\n",
      "Batch 14500/33266 - Loss: 2.7014\n",
      "Batch 14600/33266 - Loss: 3.1245\n",
      "Batch 14700/33266 - Loss: 3.3687\n",
      "Batch 14800/33266 - Loss: 3.3033\n",
      "Batch 14900/33266 - Loss: 2.5519\n",
      "Batch 15000/33266 - Loss: 3.7523\n",
      "Batch 15100/33266 - Loss: 4.0454\n",
      "Batch 15200/33266 - Loss: 2.7663\n",
      "Batch 15300/33266 - Loss: 3.5987\n",
      "Batch 15400/33266 - Loss: 2.3461\n",
      "Batch 15500/33266 - Loss: 2.5224\n",
      "Batch 15600/33266 - Loss: 2.4761\n",
      "Batch 15700/33266 - Loss: 2.6540\n",
      "Batch 15800/33266 - Loss: 4.1067\n",
      "Batch 15900/33266 - Loss: 3.7253\n",
      "Batch 16000/33266 - Loss: 4.4283\n",
      "Batch 16100/33266 - Loss: 2.6977\n",
      "Batch 16200/33266 - Loss: 2.7528\n",
      "Batch 16300/33266 - Loss: 2.5235\n",
      "Batch 16400/33266 - Loss: 3.7821\n",
      "Batch 16500/33266 - Loss: 4.2161\n",
      "Batch 16600/33266 - Loss: 3.6516\n",
      "Batch 16700/33266 - Loss: 3.2391\n",
      "Batch 16800/33266 - Loss: 3.3311\n",
      "Batch 16900/33266 - Loss: 4.2765\n",
      "Batch 17000/33266 - Loss: 2.6369\n",
      "Batch 17100/33266 - Loss: 3.9147\n",
      "Batch 17200/33266 - Loss: 3.4421\n",
      "Batch 17300/33266 - Loss: 4.1124\n",
      "Batch 17400/33266 - Loss: 3.2527\n",
      "Batch 17500/33266 - Loss: 1.7901\n",
      "Batch 17600/33266 - Loss: 2.3826\n",
      "Batch 17700/33266 - Loss: 3.7381\n",
      "Batch 17800/33266 - Loss: 3.4891\n",
      "Batch 17900/33266 - Loss: 2.9223\n",
      "Batch 18000/33266 - Loss: 2.7489\n",
      "Batch 18100/33266 - Loss: 2.8127\n",
      "Batch 18200/33266 - Loss: 4.1986\n",
      "Batch 18300/33266 - Loss: 3.5988\n",
      "Batch 18400/33266 - Loss: 2.6746\n",
      "Batch 18500/33266 - Loss: 3.4755\n",
      "Batch 18600/33266 - Loss: 4.2796\n",
      "Batch 18700/33266 - Loss: 3.1193\n",
      "Batch 18800/33266 - Loss: 3.0834\n",
      "Batch 18900/33266 - Loss: 3.3852\n",
      "Batch 19000/33266 - Loss: 3.6255\n",
      "Batch 19100/33266 - Loss: 4.1166\n",
      "Batch 19200/33266 - Loss: 3.1697\n",
      "Batch 19300/33266 - Loss: 3.4145\n",
      "Batch 19400/33266 - Loss: 3.5460\n",
      "Batch 19500/33266 - Loss: 2.3424\n",
      "Batch 19600/33266 - Loss: 2.2514\n",
      "Batch 19700/33266 - Loss: 4.0143\n",
      "Batch 19800/33266 - Loss: 2.8787\n",
      "Batch 19900/33266 - Loss: 2.4280\n",
      "Batch 20000/33266 - Loss: 2.8041\n",
      "Batch 20100/33266 - Loss: 3.7328\n",
      "Batch 20200/33266 - Loss: 2.7188\n",
      "Batch 20300/33266 - Loss: 3.2487\n",
      "Batch 20400/33266 - Loss: 4.2294\n",
      "Batch 20500/33266 - Loss: 3.2484\n",
      "Batch 20600/33266 - Loss: 4.3698\n",
      "Batch 20700/33266 - Loss: 2.5107\n",
      "Batch 20800/33266 - Loss: 3.6662\n",
      "Batch 20900/33266 - Loss: 4.4182\n",
      "Batch 21000/33266 - Loss: 2.7067\n",
      "Batch 21100/33266 - Loss: 4.5053\n",
      "Batch 21200/33266 - Loss: 2.1420\n",
      "Batch 21300/33266 - Loss: 3.4325\n",
      "Batch 21400/33266 - Loss: 3.5905\n",
      "Batch 21500/33266 - Loss: 2.0966\n",
      "Batch 21600/33266 - Loss: 3.2598\n",
      "Batch 21700/33266 - Loss: 3.0953\n",
      "Batch 21800/33266 - Loss: 4.4855\n",
      "Batch 21900/33266 - Loss: 2.6490\n",
      "Batch 22000/33266 - Loss: 2.6673\n",
      "Batch 22100/33266 - Loss: 3.7201\n",
      "Batch 22200/33266 - Loss: 3.4110\n",
      "Batch 22300/33266 - Loss: 3.9044\n",
      "Batch 22400/33266 - Loss: 3.1149\n",
      "Batch 22500/33266 - Loss: 4.2822\n",
      "Batch 22600/33266 - Loss: 2.7678\n",
      "Batch 22700/33266 - Loss: 4.0992\n",
      "Batch 22800/33266 - Loss: 3.3347\n",
      "Batch 22900/33266 - Loss: 3.0883\n",
      "Batch 23000/33266 - Loss: 2.9064\n",
      "Batch 23100/33266 - Loss: 2.9788\n",
      "Batch 23200/33266 - Loss: 2.0643\n",
      "Batch 23300/33266 - Loss: 3.6093\n",
      "Batch 23400/33266 - Loss: 2.1845\n",
      "Batch 23500/33266 - Loss: 2.3766\n",
      "Batch 23600/33266 - Loss: 3.5146\n",
      "Batch 23700/33266 - Loss: 2.6296\n",
      "Batch 23800/33266 - Loss: 3.7824\n",
      "Batch 23900/33266 - Loss: 3.9140\n",
      "Batch 24000/33266 - Loss: 2.8170\n",
      "Batch 24100/33266 - Loss: 2.4890\n",
      "Batch 24200/33266 - Loss: 3.0707\n",
      "Batch 24300/33266 - Loss: 2.4126\n",
      "Batch 24400/33266 - Loss: 2.9238\n",
      "Batch 24500/33266 - Loss: 3.2914\n",
      "Batch 24600/33266 - Loss: 3.5639\n",
      "Batch 24700/33266 - Loss: 2.4436\n",
      "Batch 24800/33266 - Loss: 1.8938\n",
      "Batch 24900/33266 - Loss: 3.6021\n",
      "Batch 25000/33266 - Loss: 3.6931\n",
      "Batch 25100/33266 - Loss: 3.0639\n",
      "Batch 25200/33266 - Loss: 3.7878\n",
      "Batch 25300/33266 - Loss: 2.5699\n",
      "Batch 25400/33266 - Loss: 3.5339\n",
      "Batch 25500/33266 - Loss: 3.2771\n",
      "Batch 25600/33266 - Loss: 2.9204\n",
      "Batch 25700/33266 - Loss: 2.6753\n",
      "Batch 25800/33266 - Loss: 3.6282\n",
      "Batch 25900/33266 - Loss: 2.3605\n",
      "Batch 26000/33266 - Loss: 3.5607\n",
      "Batch 26100/33266 - Loss: 3.3652\n",
      "Batch 26200/33266 - Loss: 2.4182\n",
      "Batch 26300/33266 - Loss: 3.1297\n",
      "Batch 26400/33266 - Loss: 3.2077\n",
      "Batch 26500/33266 - Loss: 3.2733\n",
      "Batch 26600/33266 - Loss: 3.1533\n",
      "Batch 26700/33266 - Loss: 2.9704\n",
      "Batch 26800/33266 - Loss: 4.3376\n",
      "Batch 26900/33266 - Loss: 2.6845\n",
      "Batch 27000/33266 - Loss: 3.9499\n",
      "Batch 27100/33266 - Loss: 3.2503\n",
      "Batch 27200/33266 - Loss: 2.8427\n",
      "Batch 27300/33266 - Loss: 2.5486\n",
      "Batch 27400/33266 - Loss: 3.5972\n",
      "Batch 27500/33266 - Loss: 3.6543\n",
      "Batch 27600/33266 - Loss: 3.3523\n",
      "Batch 27700/33266 - Loss: 3.1898\n",
      "Batch 27800/33266 - Loss: 2.4554\n",
      "Batch 27900/33266 - Loss: 4.2816\n",
      "Batch 28000/33266 - Loss: 4.4562\n",
      "Batch 28100/33266 - Loss: 1.8860\n",
      "Batch 28200/33266 - Loss: 3.4804\n",
      "Batch 28300/33266 - Loss: 2.7594\n",
      "Batch 28400/33266 - Loss: 2.4654\n",
      "Batch 28500/33266 - Loss: 1.5487\n",
      "Batch 28600/33266 - Loss: 2.4993\n",
      "Batch 28700/33266 - Loss: 2.9801\n",
      "Batch 28800/33266 - Loss: 3.2617\n",
      "Batch 28900/33266 - Loss: 2.6745\n",
      "Batch 29000/33266 - Loss: 3.5986\n",
      "Batch 29100/33266 - Loss: 3.1429\n",
      "Batch 29200/33266 - Loss: 2.5423\n",
      "Batch 29300/33266 - Loss: 2.8467\n",
      "Batch 29400/33266 - Loss: 3.1509\n",
      "Batch 29500/33266 - Loss: 2.7556\n",
      "Batch 29600/33266 - Loss: 3.6188\n",
      "Batch 29700/33266 - Loss: 2.8943\n",
      "Batch 29800/33266 - Loss: 2.8716\n",
      "Batch 29900/33266 - Loss: 3.1239\n",
      "Batch 30000/33266 - Loss: 3.6688\n",
      "Batch 30100/33266 - Loss: 4.2156\n",
      "Batch 30200/33266 - Loss: 2.1599\n",
      "Batch 30300/33266 - Loss: 3.3416\n",
      "Batch 30400/33266 - Loss: 2.2538\n",
      "Batch 30500/33266 - Loss: 2.9186\n",
      "Batch 30600/33266 - Loss: 3.5388\n",
      "Batch 30700/33266 - Loss: 3.5926\n",
      "Batch 30800/33266 - Loss: 3.5566\n",
      "Batch 30900/33266 - Loss: 2.5231\n",
      "Batch 31000/33266 - Loss: 3.4954\n",
      "Batch 31100/33266 - Loss: 3.0878\n",
      "Batch 31200/33266 - Loss: 3.9850\n",
      "Batch 31300/33266 - Loss: 3.5618\n",
      "Batch 31400/33266 - Loss: 2.2646\n",
      "Batch 31500/33266 - Loss: 3.0573\n",
      "Batch 31600/33266 - Loss: 3.1530\n",
      "Batch 31700/33266 - Loss: 3.5295\n",
      "Batch 31800/33266 - Loss: 2.6930\n",
      "Batch 31900/33266 - Loss: 2.6409\n",
      "Batch 32000/33266 - Loss: 2.3609\n",
      "Batch 32100/33266 - Loss: 3.3305\n",
      "Batch 32200/33266 - Loss: 3.0395\n",
      "Batch 32300/33266 - Loss: 1.8596\n",
      "Batch 32400/33266 - Loss: 3.5967\n",
      "Batch 32500/33266 - Loss: 3.5422\n",
      "Batch 32600/33266 - Loss: 3.8453\n",
      "Batch 32700/33266 - Loss: 2.1611\n",
      "Batch 32800/33266 - Loss: 3.7025\n",
      "Batch 32900/33266 - Loss: 3.4809\n",
      "Batch 33000/33266 - Loss: 3.1461\n",
      "Batch 33100/33266 - Loss: 4.2457\n",
      "Batch 33200/33266 - Loss: 2.9945\n",
      "End of Epoch 2 - Avg Loss: 3.2997\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la funcin 'train' para entrenar el modelo. El entrenamiento se realiza utilizando los siguientes parmetros:\n",
    "# - model: El modelo Transformer previamente definido que se entrenar.\n",
    "# - dataloader: El DataLoader que proporciona los datos en mini-lotes (batches) durante el entrenamiento.\n",
    "# - loss_function: La funcin de prdida que se utilizar para calcular la diferencia entre las predicciones del modelo y las etiquetas verdaderas.\n",
    "# - optimiser: El optimizador que actualiza los parmetros del modelo en funcin de la retropropagacin (backpropagation).\n",
    "# - epochs: El nmero de pocas de entrenamiento. En este caso, entrenaremos el modelo durante 10 pocas.\n",
    "# Durante cada poca, el modelo ser alimentado con los datos de entrada, y se calcularn y actualizarn los parmetros del modelo en base a la prdida.\n",
    "\n",
    "train(model, dataloader, loss_function, optimiser, epochs=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b4ffe033-ab15-4ead-b191-1b299e68d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta funcin convierte una oracin (sentence) en una lista de ndices usando el diccionario 'word2idx'.\n",
    "# La funcin toma cada palabra en la oracin, la busca en el diccionario 'word2idx', y devuelve su ndice correspondiente.\n",
    "# Si una palabra no est en el diccionario, se utiliza el ndice de la palabra desconocida ('<unk>').\n",
    "def sentence_to_indices(sentence, word2idx):\n",
    "    return [word2idx.get(word, word2idx['<unk>']) for word in sentence.split()]\n",
    "\n",
    "# Esta funcin convierte una lista de ndices (indices) en una oracin (sentence) usando el diccionario 'idx2word'.\n",
    "# La funcin toma cada ndice en la lista y lo busca en 'idx2word' para obtener la palabra correspondiente.\n",
    "# Los ndices que corresponden al token de relleno ('<pad>') se ignoran.\n",
    "def indices_to_sentence(indices, idx2word):\n",
    "    return ' '.join([idx2word[idx] for idx in indices if idx in idx2word and idx2word[idx] != '<pad>'])\n",
    "\n",
    "# Esta funcin realiza la traduccin de una oracin de ingls a espaol utilizando el modelo Transformer.\n",
    "# La funcin toma como entrada la oracin en ingls, las palabras en ingls y espaol mapeadas a ndices ('eng_word2idx' y 'spa_idx2word'),\n",
    "# y produce la traduccin en espaol.\n",
    "# La traduccin se realiza en modo de evaluacin (sin entrenamiento) y sigue los siguientes pasos:\n",
    "# 1. Preprocesar la oracin de entrada en ingls.\n",
    "# 2. Convertir la oracin de ingls en una lista de ndices (sentence_to_indices).\n",
    "# 3. Inicializar el tensor de la entrada y el tensor de destino con el token <sos> (Start of Sentence).\n",
    "# 4. Realizar una inferencia paso a paso para generar cada palabra en la traduccin, detenindose cuando se alcanza el token <eos> (End of Sentence).\n",
    "# 5. Convertir los ndices generados a palabras en espaol utilizando el diccionario 'spa_idx2word'.\n",
    "def translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n",
    "    model.eval()  # Ponemos el modelo en modo de evaluacin (sin actualizaciones de parmetros).\n",
    "    \n",
    "    # Preprocesamos la oracin de entrada (minsculas, eliminacin de espacios innecesarios, etc.).\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    # Convertimos la oracin de entrada en ndices utilizando el diccionario de ingls.\n",
    "    input_indices = sentence_to_indices(sentence, eng_word2idx)\n",
    "    \n",
    "    # Convertimos los ndices a un tensor y lo movemos al dispositivo (CPU o GPU).\n",
    "    input_tensor = torch.tensor(input_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    # Inicializamos el tensor de destino con el token de inicio de oracin (<sos>).\n",
    "    tgt_indices = [spa_word2idx['<sos>']]\n",
    "    tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():  # Desactivamos el clculo de gradientes, ya que estamos en modo de inferencia.\n",
    "        # Generamos la traduccin palabra por palabra, hasta alcanzar el mximo de longitud o el token <eos>.\n",
    "        for _ in range(max_len):\n",
    "            # Ejecutamos el modelo para predecir la siguiente palabra en la secuencia.\n",
    "            output = model(input_tensor, tgt_tensor)\n",
    "            \n",
    "            # Eliminamos la dimensin adicional de la salida y obtenemos la prediccin para el siguiente token.\n",
    "            output = output.squeeze(0)\n",
    "            \n",
    "            # Elegimos el token con la probabilidad ms alta (mximo valor en la ltima dimensin).\n",
    "            next_token = output.argmax(dim=-1)[-1].item()\n",
    "            \n",
    "            # Aadimos el siguiente token al tensor de destino.\n",
    "            tgt_indices.append(next_token)\n",
    "            \n",
    "            # Actualizamos el tensor de destino con la secuencia generada hasta ahora.\n",
    "            tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Si el token de salida es <eos>, terminamos la traduccin.\n",
    "            if next_token == spa_word2idx['<eos>']:\n",
    "                break\n",
    "\n",
    "    # Convertimos los ndices generados a palabras en espaol y devolvemos la traduccin como una cadena de texto.\n",
    "    return indices_to_sentence(tgt_indices, spa_idx2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a16f7ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta funcin evala las traducciones de un conjunto de oraciones de prueba utilizando el modelo Transformer.\n",
    "# Toma un modelo entrenado, un conjunto de oraciones en ingls, y los diccionarios que mapean palabras a ndices para ingls y espaol.\n",
    "# La funcin imprime las traducciones generadas para cada oracin de prueba.\n",
    "# Para cada oracin:\n",
    "# 1. Se traduce utilizando la funcin 'translate_sentence'.\n",
    "# 2. Se imprime la oracin original (en ingls) y la traduccin generada (en espaol).\n",
    "def evaluate_translations(model, sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n",
    "    for sentence in sentences:\n",
    "        # Se traduce cada oracin del conjunto de oraciones de prueba utilizando la funcin 'translate_sentence'.\n",
    "        translation = translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len, device)\n",
    "        \n",
    "        # Imprime la oracin original en ingls y su traduccin en espaol.\n",
    "        print(f'Input sentence: {sentence}')\n",
    "        print(f'Traduccin: {translation}')\n",
    "        print()\n",
    "\n",
    "# Estas son algunas oraciones de ejemplo para evaluar el modelo.\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",  # Ejemplo 1\n",
    "    \"I am learning artificial intelligence.\",  # Ejemplo 2\n",
    "    \"Artificial intelligence is great.\",  # Ejemplo 3\n",
    "    \"Good night!\"  # Ejemplo 4\n",
    "]\n",
    "\n",
    "# Se asume que el modelo ha sido entrenado y cargado previamente.\n",
    "# Definimos el dispositivo de ejecucin del modelo, que puede ser 'cuda' si hay una GPU disponible, o 'cpu' si no la hay.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Mover el modelo al dispositivo adecuado (CPU o GPU).\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a748f7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Hello, how are you?\n",
      "Traduccin: <sos> como estas hola <eos>\n",
      "\n",
      "Input sentence: I am learning artificial intelligence.\n",
      "Traduccin: <sos> estoy aprendiendo inteligencia inteligencia inteligencia <eos>\n",
      "\n",
      "Input sentence: Artificial intelligence is great.\n",
      "Traduccin: <sos> la inteligencia es muy grande <eos>\n",
      "\n",
      "Input sentence: Good night!\n",
      "Traduccin: <sos> buenas noches <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate translations\n",
    "evaluate_translations(model, test_sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b3347",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291f357",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
