{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3dee9ef-952f-480b-8242-536cf68f3d72",
   "metadata": {},
   "source": [
    "\n",
    "<h1>Tranformers<h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55bebde8-7c69-4464-a342-dc8006e4691d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.20.1-cp312-cp312-win_amd64.whl.metadata (6.2 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.5.1-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\jairo mendoza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jairo mendoza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jairo mendoza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\jairo mendoza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (2.1.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\jairo mendoza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jairo mendoza\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading torch-2.5.1-cp312-cp312-win_amd64.whl (203.0 MB)\n",
      "   ---------------------------------------- 0.0/203.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/203.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/203.0 MB 1.3 MB/s eta 0:02:38\n",
      "   ---------------------------------------- 0.8/203.0 MB 1.5 MB/s eta 0:02:19\n",
      "   ---------------------------------------- 1.3/203.0 MB 1.8 MB/s eta 0:01:55\n",
      "   ---------------------------------------- 1.6/203.0 MB 1.8 MB/s eta 0:01:53\n",
      "   ---------------------------------------- 1.8/203.0 MB 1.8 MB/s eta 0:01:52\n",
      "   ---------------------------------------- 2.1/203.0 MB 1.7 MB/s eta 0:02:02\n",
      "    --------------------------------------- 2.6/203.0 MB 1.7 MB/s eta 0:02:00\n",
      "    --------------------------------------- 2.9/203.0 MB 1.7 MB/s eta 0:02:00\n",
      "    --------------------------------------- 3.4/203.0 MB 1.7 MB/s eta 0:01:58\n",
      "    --------------------------------------- 3.7/203.0 MB 1.6 MB/s eta 0:02:03\n",
      "    --------------------------------------- 3.9/203.0 MB 1.6 MB/s eta 0:02:06\n",
      "    --------------------------------------- 4.2/203.0 MB 1.6 MB/s eta 0:02:04\n",
      "    --------------------------------------- 4.7/203.0 MB 1.7 MB/s eta 0:01:59\n",
      "    --------------------------------------- 5.0/203.0 MB 1.6 MB/s eta 0:02:02\n",
      "   - -------------------------------------- 5.2/203.0 MB 1.6 MB/s eta 0:02:05\n",
      "   - -------------------------------------- 5.5/203.0 MB 1.6 MB/s eta 0:02:04\n",
      "   - -------------------------------------- 5.8/203.0 MB 1.6 MB/s eta 0:02:04\n",
      "   - -------------------------------------- 6.3/203.0 MB 1.6 MB/s eta 0:02:03\n",
      "   - -------------------------------------- 6.6/203.0 MB 1.6 MB/s eta 0:02:02\n",
      "   - -------------------------------------- 6.8/203.0 MB 1.6 MB/s eta 0:02:02\n",
      "   - -------------------------------------- 7.1/203.0 MB 1.6 MB/s eta 0:02:03\n",
      "   - -------------------------------------- 7.3/203.0 MB 1.6 MB/s eta 0:02:05\n",
      "   - -------------------------------------- 7.9/203.0 MB 1.6 MB/s eta 0:02:03\n",
      "   - -------------------------------------- 8.1/203.0 MB 1.6 MB/s eta 0:02:04\n",
      "   - -------------------------------------- 8.4/203.0 MB 1.6 MB/s eta 0:02:03\n",
      "   - -------------------------------------- 8.9/203.0 MB 1.6 MB/s eta 0:02:01\n",
      "   - -------------------------------------- 9.4/203.0 MB 1.6 MB/s eta 0:01:59\n",
      "   - -------------------------------------- 10.0/203.0 MB 1.7 MB/s eta 0:01:57\n",
      "   -- ------------------------------------- 10.2/203.0 MB 1.7 MB/s eta 0:01:57\n",
      "   -- ------------------------------------- 10.7/203.0 MB 1.7 MB/s eta 0:01:56\n",
      "   -- ------------------------------------- 11.0/203.0 MB 1.7 MB/s eta 0:01:55\n",
      "   -- ------------------------------------- 11.5/203.0 MB 1.7 MB/s eta 0:01:54\n",
      "   -- ------------------------------------- 11.5/203.0 MB 1.7 MB/s eta 0:01:54\n",
      "   -- ------------------------------------- 11.5/203.0 MB 1.7 MB/s eta 0:01:54\n",
      "   -- ------------------------------------- 11.5/203.0 MB 1.7 MB/s eta 0:01:54\n",
      "   -- ------------------------------------- 11.5/203.0 MB 1.7 MB/s eta 0:01:54\n",
      "   -- ------------------------------------- 11.5/203.0 MB 1.7 MB/s eta 0:01:54\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 11.8/203.0 MB 1.5 MB/s eta 0:02:12\n",
      "   -- ------------------------------------- 12.1/203.0 MB 1.1 MB/s eta 0:02:53\n",
      "   -- ------------------------------------- 12.1/203.0 MB 1.1 MB/s eta 0:02:53\n",
      "   -- ------------------------------------- 12.3/203.0 MB 1.1 MB/s eta 0:02:54\n",
      "   -- ------------------------------------- 12.6/203.0 MB 1.1 MB/s eta 0:02:54\n",
      "   -- ------------------------------------- 13.1/203.0 MB 1.1 MB/s eta 0:02:52\n",
      "   -- ------------------------------------- 13.4/203.0 MB 1.1 MB/s eta 0:02:50\n",
      "   -- ------------------------------------- 13.6/203.0 MB 1.1 MB/s eta 0:02:49\n",
      "   -- ------------------------------------- 14.2/203.0 MB 1.1 MB/s eta 0:02:47\n",
      "   -- ------------------------------------- 14.4/203.0 MB 1.1 MB/s eta 0:02:45\n",
      "   -- ------------------------------------- 14.7/203.0 MB 1.1 MB/s eta 0:02:45\n",
      "   -- ------------------------------------- 14.9/203.0 MB 1.1 MB/s eta 0:02:44\n",
      "   --- ------------------------------------ 15.5/203.0 MB 1.2 MB/s eta 0:02:42\n",
      "   --- ------------------------------------ 15.7/203.0 MB 1.2 MB/s eta 0:02:41\n",
      "   --- ------------------------------------ 16.3/203.0 MB 1.2 MB/s eta 0:02:38\n",
      "   --- ------------------------------------ 16.8/203.0 MB 1.2 MB/s eta 0:02:35\n",
      "   --- ------------------------------------ 17.0/203.0 MB 1.2 MB/s eta 0:02:34\n",
      "   --- ------------------------------------ 17.3/203.0 MB 1.2 MB/s eta 0:02:33\n",
      "   --- ------------------------------------ 17.8/203.0 MB 1.2 MB/s eta 0:02:32\n",
      "   --- ------------------------------------ 18.1/203.0 MB 1.2 MB/s eta 0:02:31\n",
      "   --- ------------------------------------ 18.4/203.0 MB 1.2 MB/s eta 0:02:31\n",
      "   --- ------------------------------------ 18.6/203.0 MB 1.2 MB/s eta 0:02:30\n",
      "   --- ------------------------------------ 18.9/203.0 MB 1.2 MB/s eta 0:02:30\n",
      "   --- ------------------------------------ 19.1/203.0 MB 1.2 MB/s eta 0:02:29\n",
      "   --- ------------------------------------ 19.7/203.0 MB 1.2 MB/s eta 0:02:28\n",
      "   --- ------------------------------------ 19.9/203.0 MB 1.2 MB/s eta 0:02:28\n",
      "   --- ------------------------------------ 20.2/203.0 MB 1.2 MB/s eta 0:02:27\n",
      "   ---- ----------------------------------- 20.7/203.0 MB 1.3 MB/s eta 0:02:25\n",
      "   ---- ----------------------------------- 21.0/203.0 MB 1.3 MB/s eta 0:02:25\n",
      "   ---- ----------------------------------- 21.5/203.0 MB 1.3 MB/s eta 0:02:23\n",
      "   ---- ----------------------------------- 21.8/203.0 MB 1.3 MB/s eta 0:02:22\n",
      "   ---- ----------------------------------- 22.0/203.0 MB 1.3 MB/s eta 0:02:22\n",
      "   ---- ----------------------------------- 22.3/203.0 MB 1.3 MB/s eta 0:02:22\n",
      "   ---- ----------------------------------- 22.8/203.0 MB 1.3 MB/s eta 0:02:20\n",
      "   ---- ----------------------------------- 23.1/203.0 MB 1.3 MB/s eta 0:02:20\n",
      "   ---- ----------------------------------- 23.3/203.0 MB 1.3 MB/s eta 0:02:20\n",
      "   ---- ----------------------------------- 23.6/203.0 MB 1.3 MB/s eta 0:02:20\n",
      "   ---- ----------------------------------- 23.9/203.0 MB 1.3 MB/s eta 0:02:19\n",
      "   ---- ----------------------------------- 24.1/203.0 MB 1.3 MB/s eta 0:02:19\n",
      "   ---- ----------------------------------- 24.4/203.0 MB 1.3 MB/s eta 0:02:19\n",
      "   ---- ----------------------------------- 24.6/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ---- ----------------------------------- 25.2/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 25.4/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 25.7/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 26.0/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 26.2/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 26.2/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 26.5/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 26.7/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 27.0/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 27.3/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 27.8/203.0 MB 1.3 MB/s eta 0:02:17\n",
      "   ----- ---------------------------------- 28.0/203.0 MB 1.3 MB/s eta 0:02:16\n",
      "   ----- ---------------------------------- 28.3/203.0 MB 1.3 MB/s eta 0:02:16\n",
      "   ----- ---------------------------------- 28.3/203.0 MB 1.3 MB/s eta 0:02:16\n",
      "   ----- ---------------------------------- 28.6/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 28.6/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 28.8/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 28.8/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 28.8/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 28.8/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 28.8/203.0 MB 1.3 MB/s eta 0:02:18\n",
      "   ----- ---------------------------------- 29.1/203.0 MB 1.2 MB/s eta 0:02:24\n",
      "   ----- ---------------------------------- 29.4/203.0 MB 1.2 MB/s eta 0:02:23\n",
      "   ----- ---------------------------------- 29.9/203.0 MB 1.2 MB/s eta 0:02:22\n",
      "   ----- ---------------------------------- 30.4/203.0 MB 1.2 MB/s eta 0:02:20\n",
      "   ------ --------------------------------- 30.7/203.0 MB 1.2 MB/s eta 0:02:20\n",
      "   ------ --------------------------------- 30.9/203.0 MB 1.2 MB/s eta 0:02:20\n",
      "   ------ --------------------------------- 31.2/203.0 MB 1.2 MB/s eta 0:02:19\n",
      "   ------ --------------------------------- 31.5/203.0 MB 1.2 MB/s eta 0:02:19\n",
      "   ------ --------------------------------- 32.0/203.0 MB 1.2 MB/s eta 0:02:18\n",
      "   ------ --------------------------------- 32.2/203.0 MB 1.2 MB/s eta 0:02:18\n",
      "   ------ --------------------------------- 32.5/203.0 MB 1.2 MB/s eta 0:02:17\n",
      "   ------ --------------------------------- 33.0/203.0 MB 1.3 MB/s eta 0:02:16\n",
      "   ------ --------------------------------- 33.3/203.0 MB 1.3 MB/s eta 0:02:16\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 33.8/203.0 MB 1.3 MB/s eta 0:02:15\n",
      "   ------ --------------------------------- 34.1/203.0 MB 1.1 MB/s eta 0:02:30\n",
      "   ------ --------------------------------- 34.1/203.0 MB 1.1 MB/s eta 0:02:30\n",
      "   ------ --------------------------------- 34.3/203.0 MB 1.1 MB/s eta 0:02:32\n",
      "   ------ --------------------------------- 34.6/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------ --------------------------------- 34.9/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------ --------------------------------- 35.1/203.0 MB 1.1 MB/s eta 0:02:32\n",
      "   ------ --------------------------------- 35.4/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 35.7/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 35.9/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 36.2/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 36.7/203.0 MB 1.1 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 37.0/203.0 MB 1.1 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 37.2/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 37.5/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 37.7/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 38.0/203.0 MB 1.1 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 38.3/203.0 MB 1.1 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 38.5/203.0 MB 1.1 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 38.8/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 39.3/203.0 MB 1.1 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 39.3/203.0 MB 1.1 MB/s eta 0:02:32\n",
      "   ------- -------------------------------- 39.6/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 39.6/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 39.6/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 39.6/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 39.6/203.0 MB 1.1 MB/s eta 0:02:33\n",
      "   ------- -------------------------------- 39.8/203.0 MB 1.0 MB/s eta 0:02:42\n",
      "   ------- -------------------------------- 39.8/203.0 MB 1.0 MB/s eta 0:02:42\n",
      "   ------- -------------------------------- 39.8/203.0 MB 1.0 MB/s eta 0:02:42\n",
      "   ------- -------------------------------- 39.8/203.0 MB 1.0 MB/s eta 0:02:42\n",
      "   ------- ------------------------------- 40.1/203.0 MB 972.5 kB/s eta 0:02:48\n",
      "   ------- ------------------------------- 40.4/203.0 MB 963.2 kB/s eta 0:02:49\n",
      "   ------- ------------------------------- 40.6/203.0 MB 997.0 kB/s eta 0:02:43\n",
      "   -------- ------------------------------- 40.9/203.0 MB 1.0 MB/s eta 0:02:42\n",
      "   -------- ------------------------------- 41.4/203.0 MB 1.0 MB/s eta 0:02:41\n",
      "   -------- ------------------------------- 41.7/203.0 MB 1.0 MB/s eta 0:02:41\n",
      "   -------- ------------------------------- 41.9/203.0 MB 1.0 MB/s eta 0:02:40\n",
      "   -------- ------------------------------- 42.2/203.0 MB 1.1 MB/s eta 0:02:26\n",
      "   -------- ------------------------------- 42.5/203.0 MB 1.1 MB/s eta 0:02:26\n",
      "   -------- ------------------------------- 42.7/203.0 MB 1.1 MB/s eta 0:02:25\n",
      "   -------- ------------------------------- 43.0/203.0 MB 1.1 MB/s eta 0:02:25\n",
      "   -------- ------------------------------- 43.3/203.0 MB 1.1 MB/s eta 0:02:24\n",
      "   -------- ------------------------------- 43.5/203.0 MB 1.1 MB/s eta 0:02:24\n",
      "   -------- ------------------------------- 43.8/203.0 MB 1.1 MB/s eta 0:02:24\n",
      "   -------- ------------------------------- 44.0/203.0 MB 1.1 MB/s eta 0:02:24\n",
      "   -------- ------------------------------- 44.3/203.0 MB 1.1 MB/s eta 0:02:23\n",
      "   -------- ------------------------------- 44.6/203.0 MB 1.1 MB/s eta 0:02:23\n",
      "   -------- ------------------------------- 44.8/203.0 MB 1.1 MB/s eta 0:02:23\n",
      "   -------- ------------------------------- 45.1/203.0 MB 1.1 MB/s eta 0:02:22\n",
      "   -------- ------------------------------- 45.4/203.0 MB 1.1 MB/s eta 0:02:22\n",
      "   --------- ------------------------------ 45.9/203.0 MB 1.1 MB/s eta 0:02:20\n",
      "   --------- ------------------------------ 46.1/203.0 MB 1.1 MB/s eta 0:02:20\n",
      "   --------- ------------------------------ 46.4/203.0 MB 1.1 MB/s eta 0:02:19\n",
      "   --------- ------------------------------ 46.7/203.0 MB 1.1 MB/s eta 0:02:19\n",
      "   --------- ------------------------------ 47.2/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   --------- ------------------------------ 47.4/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   --------- ------------------------------ 47.7/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   --------- ------------------------------ 48.0/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   --------- ------------------------------ 48.5/203.0 MB 1.1 MB/s eta 0:02:17\n",
      "   --------- ------------------------------ 48.8/203.0 MB 1.1 MB/s eta 0:02:17\n",
      "   --------- ------------------------------ 49.3/203.0 MB 1.1 MB/s eta 0:02:16\n",
      "   --------- ------------------------------ 49.5/203.0 MB 1.1 MB/s eta 0:02:16\n",
      "   --------- ------------------------------ 49.8/203.0 MB 1.1 MB/s eta 0:02:17\n",
      "   --------- ------------------------------ 50.1/203.0 MB 1.1 MB/s eta 0:02:17\n",
      "   --------- ------------------------------ 50.3/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   --------- ------------------------------ 50.6/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 50.9/203.0 MB 1.1 MB/s eta 0:02:17\n",
      "   ---------- ----------------------------- 51.4/203.0 MB 1.1 MB/s eta 0:02:17\n",
      "   ---------- ----------------------------- 51.6/203.0 MB 1.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 51.9/203.0 MB 1.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 52.2/203.0 MB 1.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 52.4/203.0 MB 1.1 MB/s eta 0:02:15\n",
      "   ---------- ----------------------------- 52.7/203.0 MB 1.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 53.2/203.0 MB 1.1 MB/s eta 0:02:15\n",
      "   ---------- ----------------------------- 53.5/203.0 MB 1.1 MB/s eta 0:02:15\n",
      "   ---------- ----------------------------- 53.7/203.0 MB 1.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 54.0/203.0 MB 1.1 MB/s eta 0:02:15\n",
      "   ---------- ----------------------------- 54.3/203.0 MB 1.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 54.3/203.0 MB 1.1 MB/s eta 0:02:16\n",
      "   ---------- ----------------------------- 54.5/203.0 MB 1.1 MB/s eta 0:02:17\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ----------------------------- 54.8/203.0 MB 1.1 MB/s eta 0:02:18\n",
      "   ---------- ---------------------------- 55.1/203.0 MB 972.5 kB/s eta 0:02:33\n",
      "   ---------- ---------------------------- 55.1/203.0 MB 972.5 kB/s eta 0:02:33\n",
      "   ---------- ---------------------------- 55.3/203.0 MB 967.7 kB/s eta 0:02:33\n",
      "   ---------- ---------------------------- 55.6/203.0 MB 968.8 kB/s eta 0:02:33\n",
      "   ---------- ---------------------------- 55.8/203.0 MB 968.3 kB/s eta 0:02:33\n",
      "   ---------- ---------------------------- 55.8/203.0 MB 968.3 kB/s eta 0:02:33\n",
      "   ---------- ---------------------------- 56.1/203.0 MB 962.2 kB/s eta 0:02:33\n",
      "   ---------- ---------------------------- 56.4/203.0 MB 955.4 kB/s eta 0:02:34\n",
      "   ---------- ---------------------------- 56.6/203.0 MB 953.4 kB/s eta 0:02:34\n",
      "   ---------- ---------------------------- 56.6/203.0 MB 953.4 kB/s eta 0:02:34\n",
      "   ---------- ---------------------------- 56.9/203.0 MB 956.2 kB/s eta 0:02:33\n",
      "   ---------- ---------------------------- 57.1/203.0 MB 957.4 kB/s eta 0:02:33\n",
      "   ----------- --------------------------- 57.4/203.0 MB 961.0 kB/s eta 0:02:32\n",
      "   ----------- --------------------------- 57.7/203.0 MB 963.7 kB/s eta 0:02:31\n",
      "   ----------- --------------------------- 57.7/203.0 MB 963.7 kB/s eta 0:02:31\n",
      "   ----------- --------------------------- 57.7/203.0 MB 963.7 kB/s eta 0:02:31\n",
      "   ----------- --------------------------- 57.9/203.0 MB 981.6 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 57.9/203.0 MB 981.6 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 58.2/203.0 MB 969.9 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 58.2/203.0 MB 969.9 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 58.2/203.0 MB 969.9 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 58.2/203.0 MB 969.9 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 58.5/203.0 MB 929.2 kB/s eta 0:02:36\n",
      "   ----------- --------------------------- 58.7/203.0 MB 921.3 kB/s eta 0:02:37\n",
      "   ----------- --------------------------- 58.7/203.0 MB 921.3 kB/s eta 0:02:37\n",
      "   ----------- --------------------------- 59.0/203.0 MB 915.0 kB/s eta 0:02:38\n",
      "   ----------- --------------------------- 59.0/203.0 MB 915.0 kB/s eta 0:02:38\n",
      "   ----------- --------------------------- 59.2/203.0 MB 909.7 kB/s eta 0:02:39\n",
      "   ----------- --------------------------- 59.5/203.0 MB 894.1 kB/s eta 0:02:41\n",
      "   ----------- --------------------------- 59.5/203.0 MB 894.1 kB/s eta 0:02:41\n",
      "   ----------- --------------------------- 59.8/203.0 MB 884.9 kB/s eta 0:02:42\n",
      "   ----------- --------------------------- 59.8/203.0 MB 884.9 kB/s eta 0:02:42\n",
      "   ----------- --------------------------- 60.0/203.0 MB 970.2 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 60.3/203.0 MB 968.1 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 60.3/203.0 MB 968.1 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 60.6/203.0 MB 966.1 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 60.8/203.0 MB 966.8 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 61.1/203.0 MB 967.6 kB/s eta 0:02:27\n",
      "   ----------- --------------------------- 61.1/203.0 MB 967.6 kB/s eta 0:02:27\n",
      "   ----------- --------------------------- 61.3/203.0 MB 962.4 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 61.3/203.0 MB 962.4 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 61.6/203.0 MB 958.5 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 61.6/203.0 MB 958.5 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 61.6/203.0 MB 958.5 kB/s eta 0:02:28\n",
      "   ----------- --------------------------- 61.9/203.0 MB 941.5 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 61.9/203.0 MB 941.5 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 61.9/203.0 MB 941.5 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 61.9/203.0 MB 941.5 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 61.9/203.0 MB 941.5 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 61.9/203.0 MB 941.5 kB/s eta 0:02:30\n",
      "   ----------- --------------------------- 62.1/203.0 MB 909.7 kB/s eta 0:02:35\n",
      "   ----------- --------------------------- 62.1/203.0 MB 909.7 kB/s eta 0:02:35\n",
      "   ----------- --------------------------- 62.1/203.0 MB 909.7 kB/s eta 0:02:35\n",
      "   ----------- --------------------------- 62.4/203.0 MB 895.0 kB/s eta 0:02:38\n",
      "   ------------ -------------------------- 62.7/203.0 MB 891.3 kB/s eta 0:02:38\n",
      "   ------------ -------------------------- 62.7/203.0 MB 891.3 kB/s eta 0:02:38\n",
      "   ------------ -------------------------- 62.9/203.0 MB 883.0 kB/s eta 0:02:39\n",
      "   ------------ -------------------------- 63.2/203.0 MB 882.6 kB/s eta 0:02:39\n",
      "   ------------ -------------------------- 63.4/203.0 MB 877.5 kB/s eta 0:02:40\n",
      "   ------------ -------------------------- 63.7/203.0 MB 877.5 kB/s eta 0:02:39\n",
      "   ------------ -------------------------- 64.2/203.0 MB 885.3 kB/s eta 0:02:37\n",
      "   ------------ -------------------------- 64.5/203.0 MB 885.3 kB/s eta 0:02:37\n",
      "   ------------ -------------------------- 64.7/203.0 MB 885.8 kB/s eta 0:02:37\n",
      "   ------------ -------------------------- 65.0/203.0 MB 883.0 kB/s eta 0:02:37\n",
      "   ------------ -------------------------- 65.0/203.0 MB 883.0 kB/s eta 0:02:37\n",
      "   ------------ -------------------------- 65.5/203.0 MB 875.7 kB/s eta 0:02:38\n",
      "   ------------ -------------------------- 65.8/203.0 MB 884.4 kB/s eta 0:02:36\n",
      "   ------------ -------------------------- 66.1/203.0 MB 889.5 kB/s eta 0:02:35\n",
      "   ------------ -------------------------- 66.3/203.0 MB 891.3 kB/s eta 0:02:34\n",
      "   ------------ -------------------------- 66.8/203.0 MB 923.6 kB/s eta 0:02:28\n",
      "   ------------ -------------------------- 67.1/203.0 MB 926.6 kB/s eta 0:02:27\n",
      "   ------------ -------------------------- 67.4/203.0 MB 929.1 kB/s eta 0:02:27\n",
      "   ------------ -------------------------- 67.6/203.0 MB 931.6 kB/s eta 0:02:26\n",
      "   ------------- ------------------------- 67.9/203.0 MB 948.0 kB/s eta 0:02:23\n",
      "   ------------- ------------------------- 68.4/203.0 MB 956.1 kB/s eta 0:02:21\n",
      "   ------------- ------------------------- 68.7/203.0 MB 959.5 kB/s eta 0:02:21\n",
      "   ------------- ------------------------- 68.9/203.0 MB 960.9 kB/s eta 0:02:20\n",
      "   ------------- ------------------------- 69.2/203.0 MB 965.7 kB/s eta 0:02:19\n",
      "   ------------- ------------------------- 69.7/203.0 MB 972.5 kB/s eta 0:02:18\n",
      "   ------------- ------------------------- 70.0/203.0 MB 972.5 kB/s eta 0:02:17\n",
      "   ------------- ------------------------- 70.5/203.0 MB 979.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 71.0/203.0 MB 984.3 kB/s eta 0:02:15\n",
      "   ------------- ------------------------- 71.3/203.0 MB 986.4 kB/s eta 0:02:14\n",
      "   ------------- ------------------------- 71.6/203.0 MB 988.4 kB/s eta 0:02:14\n",
      "   ------------- ------------------------- 71.6/203.0 MB 988.4 kB/s eta 0:02:14\n",
      "   ------------- ------------------------- 71.8/203.0 MB 983.3 kB/s eta 0:02:14\n",
      "   ------------- ------------------------- 71.8/203.0 MB 983.3 kB/s eta 0:02:14\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.1/203.0 MB 966.7 kB/s eta 0:02:16\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.4/203.0 MB 839.3 kB/s eta 0:02:36\n",
      "   ------------- ------------------------- 72.6/203.0 MB 681.9 kB/s eta 0:03:12\n",
      "   ------------- ------------------------- 72.9/203.0 MB 673.2 kB/s eta 0:03:14\n",
      "   ------------- ------------------------- 72.9/203.0 MB 673.2 kB/s eta 0:03:14\n",
      "   -------------- ------------------------ 73.1/203.0 MB 665.5 kB/s eta 0:03:16\n",
      "   -------------- ------------------------ 73.1/203.0 MB 665.5 kB/s eta 0:03:16\n",
      "   -------------- ------------------------ 73.4/203.0 MB 656.4 kB/s eta 0:03:18\n",
      "   -------------- ------------------------ 73.7/203.0 MB 652.1 kB/s eta 0:03:19\n",
      "   -------------- ------------------------ 73.9/203.0 MB 651.0 kB/s eta 0:03:19\n",
      "   -------------- ------------------------ 74.2/203.0 MB 657.1 kB/s eta 0:03:17\n",
      "   -------------- ------------------------ 74.4/203.0 MB 662.6 kB/s eta 0:03:15\n",
      "   -------------- ------------------------ 75.0/203.0 MB 726.1 kB/s eta 0:02:57\n",
      "   -------------- ------------------------ 75.2/203.0 MB 731.1 kB/s eta 0:02:55\n",
      "   -------------- ------------------------ 75.8/203.0 MB 744.2 kB/s eta 0:02:52\n",
      "   -------------- ------------------------ 76.3/203.0 MB 755.0 kB/s eta 0:02:48\n",
      "   -------------- ------------------------ 76.5/203.0 MB 760.9 kB/s eta 0:02:47\n",
      "   -------------- ------------------------ 77.1/203.0 MB 771.8 kB/s eta 0:02:44\n",
      "   -------------- ------------------------ 77.1/203.0 MB 771.8 kB/s eta 0:02:44\n",
      "   -------------- ------------------------ 77.3/203.0 MB 771.7 kB/s eta 0:02:43\n",
      "   -------------- ------------------------ 77.6/203.0 MB 776.2 kB/s eta 0:02:42\n",
      "   -------------- ------------------------ 77.9/203.0 MB 778.9 kB/s eta 0:02:41\n",
      "   --------------- ----------------------- 78.1/203.0 MB 783.2 kB/s eta 0:02:40\n",
      "   --------------- ----------------------- 78.4/203.0 MB 786.3 kB/s eta 0:02:39\n",
      "   --------------- ----------------------- 78.6/203.0 MB 789.3 kB/s eta 0:02:38\n",
      "   --------------- ----------------------- 78.9/203.0 MB 792.2 kB/s eta 0:02:37\n",
      "   --------------- ----------------------- 79.2/203.0 MB 796.0 kB/s eta 0:02:36\n",
      "   --------------- ----------------------- 79.7/203.0 MB 800.6 kB/s eta 0:02:35\n",
      "   --------------- ----------------------- 80.0/203.0 MB 805.2 kB/s eta 0:02:33\n",
      "   --------------- ----------------------- 80.2/203.0 MB 811.1 kB/s eta 0:02:32\n",
      "   --------------- ----------------------- 80.2/203.0 MB 811.1 kB/s eta 0:02:32\n",
      "   --------------- ----------------------- 80.7/203.0 MB 809.0 kB/s eta 0:02:32\n",
      "   --------------- ----------------------- 81.3/203.0 MB 821.4 kB/s eta 0:02:29\n",
      "   --------------- ----------------------- 81.5/203.0 MB 823.1 kB/s eta 0:02:28\n",
      "   --------------- ----------------------- 81.8/203.0 MB 824.4 kB/s eta 0:02:28\n",
      "   --------------- ----------------------- 82.1/203.0 MB 826.1 kB/s eta 0:02:27\n",
      "   --------------- ----------------------- 82.3/203.0 MB 824.4 kB/s eta 0:02:27\n",
      "   --------------- ----------------------- 82.8/203.0 MB 843.7 kB/s eta 0:02:23\n",
      "   --------------- ----------------------- 83.1/203.0 MB 849.0 kB/s eta 0:02:22\n",
      "   ---------------- ---------------------- 83.6/203.0 MB 857.2 kB/s eta 0:02:20\n",
      "   ---------------- ---------------------- 83.9/203.0 MB 870.9 kB/s eta 0:02:17\n",
      "   ---------------- ---------------------- 84.1/203.0 MB 875.6 kB/s eta 0:02:16\n",
      "   ---------------- ---------------------- 84.7/203.0 MB 895.7 kB/s eta 0:02:13\n",
      "   ---------------- ---------------------- 84.9/203.0 MB 898.9 kB/s eta 0:02:12\n",
      "   ---------------- ---------------------- 84.9/203.0 MB 898.9 kB/s eta 0:02:12\n",
      "   ---------------- ---------------------- 85.2/203.0 MB 896.9 kB/s eta 0:02:12\n",
      "   ---------------- ---------------------- 85.5/203.0 MB 900.0 kB/s eta 0:02:11\n",
      "   ---------------- ---------------------- 85.5/203.0 MB 900.0 kB/s eta 0:02:11\n",
      "   ---------------- ---------------------- 85.5/203.0 MB 900.0 kB/s eta 0:02:11\n",
      "   ---------------- ---------------------- 85.5/203.0 MB 900.0 kB/s eta 0:02:11\n",
      "   ---------------- ---------------------- 85.5/203.0 MB 900.0 kB/s eta 0:02:11\n",
      "   ---------------- ---------------------- 85.5/203.0 MB 900.0 kB/s eta 0:02:11\n",
      "   ---------------- ---------------------- 85.7/203.0 MB 877.0 kB/s eta 0:02:14\n",
      "   ---------------- ---------------------- 85.7/203.0 MB 877.0 kB/s eta 0:02:14\n",
      "   ---------------- ---------------------- 85.7/203.0 MB 877.0 kB/s eta 0:02:14\n",
      "   ---------------- ---------------------- 85.7/203.0 MB 877.0 kB/s eta 0:02:14\n",
      "   ---------------- ---------------------- 86.0/203.0 MB 866.4 kB/s eta 0:02:16\n",
      "   ---------------- ---------------------- 86.2/203.0 MB 868.3 kB/s eta 0:02:15\n",
      "   ---------------- ---------------------- 86.8/203.0 MB 875.7 kB/s eta 0:02:13\n",
      "   ---------------- ---------------------- 86.8/203.0 MB 875.7 kB/s eta 0:02:13\n",
      "   ---------------- ---------------------- 87.3/203.0 MB 878.4 kB/s eta 0:02:12\n",
      "   ---------------- ---------------------- 87.6/203.0 MB 885.8 kB/s eta 0:02:11\n",
      "   ---------------- ---------------------- 87.8/203.0 MB 890.9 kB/s eta 0:02:10\n",
      "   ---------------- ---------------------- 88.3/203.0 MB 900.5 kB/s eta 0:02:08\n",
      "   ----------------- --------------------- 88.6/203.0 MB 905.2 kB/s eta 0:02:07\n",
      "   ----------------- --------------------- 89.1/203.0 MB 930.6 kB/s eta 0:02:03\n",
      "   ----------------- --------------------- 89.4/203.0 MB 935.0 kB/s eta 0:02:02\n",
      "   ----------------- --------------------- 89.9/203.0 MB 945.8 kB/s eta 0:02:00\n",
      "   ----------------- --------------------- 90.4/203.0 MB 954.0 kB/s eta 0:01:59\n",
      "   ----------------- --------------------- 90.7/203.0 MB 990.6 kB/s eta 0:01:54\n",
      "   ----------------- --------------------- 91.0/203.0 MB 994.3 kB/s eta 0:01:53\n",
      "   ----------------- --------------------- 91.2/203.0 MB 998.0 kB/s eta 0:01:53\n",
      "   ------------------ --------------------- 91.8/203.0 MB 1.0 MB/s eta 0:01:51\n",
      "   ------------------ --------------------- 92.0/203.0 MB 1.0 MB/s eta 0:01:51\n",
      "   ------------------ --------------------- 92.3/203.0 MB 1.0 MB/s eta 0:01:50\n",
      "   ------------------ --------------------- 92.8/203.0 MB 1.0 MB/s eta 0:01:48\n",
      "   ------------------ --------------------- 93.3/203.0 MB 1.0 MB/s eta 0:01:46\n",
      "   ------------------ --------------------- 93.6/203.0 MB 1.0 MB/s eta 0:01:45\n",
      "   ------------------ --------------------- 94.1/203.0 MB 1.1 MB/s eta 0:01:44\n",
      "   ------------------ --------------------- 94.4/203.0 MB 1.1 MB/s eta 0:01:43\n",
      "   ------------------ --------------------- 94.9/203.0 MB 1.1 MB/s eta 0:01:42\n",
      "   ------------------ --------------------- 95.4/203.0 MB 1.1 MB/s eta 0:01:40\n",
      "   ------------------ --------------------- 96.2/203.0 MB 1.1 MB/s eta 0:01:38\n",
      "   ------------------- -------------------- 96.5/203.0 MB 1.1 MB/s eta 0:01:38\n",
      "   ------------------- -------------------- 97.0/203.0 MB 1.1 MB/s eta 0:01:37\n",
      "   ------------------- -------------------- 97.5/203.0 MB 1.1 MB/s eta 0:01:36\n",
      "   ------------------- -------------------- 97.8/203.0 MB 1.1 MB/s eta 0:01:35\n",
      "   ------------------- -------------------- 98.0/203.0 MB 1.1 MB/s eta 0:01:35\n",
      "   ------------------- -------------------- 98.3/203.0 MB 1.1 MB/s eta 0:01:35\n",
      "   ------------------- -------------------- 98.8/203.0 MB 1.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 99.1/203.0 MB 1.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 99.4/203.0 MB 1.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 99.6/203.0 MB 1.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 99.9/203.0 MB 1.1 MB/s eta 0:01:33\n",
      "   ------------------- -------------------- 100.4/203.0 MB 1.1 MB/s eta 0:01:32\n",
      "   ------------------- -------------------- 100.7/203.0 MB 1.1 MB/s eta 0:01:32\n",
      "   ------------------- -------------------- 100.9/203.0 MB 1.1 MB/s eta 0:01:32\n",
      "   ------------------- -------------------- 101.4/203.0 MB 1.1 MB/s eta 0:01:31\n",
      "   -------------------- ------------------- 101.7/203.0 MB 1.1 MB/s eta 0:01:31\n",
      "   -------------------- ------------------- 102.0/203.0 MB 1.1 MB/s eta 0:01:31\n",
      "   -------------------- ------------------- 102.2/203.0 MB 1.1 MB/s eta 0:01:31\n",
      "   -------------------- ------------------- 102.8/203.0 MB 1.1 MB/s eta 0:01:29\n",
      "   -------------------- ------------------- 103.0/203.0 MB 1.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 103.0/203.0 MB 1.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 103.3/203.0 MB 1.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 103.3/203.0 MB 1.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 103.3/203.0 MB 1.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 103.3/203.0 MB 1.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 103.3/203.0 MB 1.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 103.3/203.0 MB 1.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 103.3/203.0 MB 1.1 MB/s eta 0:01:30\n",
      "   -------------------- ------------------- 103.5/203.0 MB 1.1 MB/s eta 0:01:35\n",
      "   -------------------- ------------------- 103.8/203.0 MB 1.2 MB/s eta 0:01:26\n",
      "   -------------------- ------------------- 104.1/203.0 MB 1.2 MB/s eta 0:01:26\n",
      "   -------------------- ------------------- 104.6/203.0 MB 1.2 MB/s eta 0:01:25\n",
      "   -------------------- ------------------- 104.9/203.0 MB 1.2 MB/s eta 0:01:24\n",
      "   -------------------- ------------------- 105.4/203.0 MB 1.2 MB/s eta 0:01:23\n",
      "   -------------------- ------------------- 105.6/203.0 MB 1.2 MB/s eta 0:01:23\n",
      "   -------------------- ------------------- 106.4/203.0 MB 1.2 MB/s eta 0:01:21\n",
      "   --------------------- ------------------ 106.7/203.0 MB 1.2 MB/s eta 0:01:21\n",
      "   --------------------- ------------------ 107.2/203.0 MB 1.2 MB/s eta 0:01:19\n",
      "   --------------------- ------------------ 108.0/203.0 MB 1.2 MB/s eta 0:01:18\n",
      "   --------------------- ------------------ 108.5/203.0 MB 1.2 MB/s eta 0:01:17\n",
      "   --------------------- ------------------ 109.1/203.0 MB 1.2 MB/s eta 0:01:16\n",
      "   --------------------- ------------------ 109.6/203.0 MB 1.3 MB/s eta 0:01:15\n",
      "   --------------------- ------------------ 110.1/203.0 MB 1.3 MB/s eta 0:01:14\n",
      "   --------------------- ------------------ 110.6/203.0 MB 1.4 MB/s eta 0:01:06\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   --------------------- ------------------ 111.4/203.0 MB 1.4 MB/s eta 0:01:04\n",
      "   ---------------------- ----------------- 111.9/203.0 MB 1.4 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 111.9/203.0 MB 1.4 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 112.2/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 112.2/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 112.5/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 112.5/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 112.5/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 112.7/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 112.7/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 113.0/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 113.0/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 113.2/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 113.2/203.0 MB 1.3 MB/s eta 0:01:08\n",
      "   ---------------------- ----------------- 113.5/203.0 MB 1.3 MB/s eta 0:01:09\n",
      "   ---------------------- ----------------- 113.5/203.0 MB 1.3 MB/s eta 0:01:09\n",
      "   ---------------------- ----------------- 113.8/203.0 MB 1.3 MB/s eta 0:01:10\n",
      "   ---------------------- ----------------- 113.8/203.0 MB 1.3 MB/s eta 0:01:10\n",
      "   ---------------------- ----------------- 113.8/203.0 MB 1.3 MB/s eta 0:01:10\n",
      "   ---------------------- ----------------- 113.8/203.0 MB 1.3 MB/s eta 0:01:10\n",
      "   ---------------------- ----------------- 113.8/203.0 MB 1.3 MB/s eta 0:01:10\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.0/203.0 MB 1.2 MB/s eta 0:01:13\n",
      "   ---------------------- ----------------- 114.3/203.0 MB 1.1 MB/s eta 0:01:24\n",
      "   ---------------------- ----------------- 114.6/203.0 MB 1.0 MB/s eta 0:01:25\n",
      "   ---------------------- ----------------- 114.8/203.0 MB 1.0 MB/s eta 0:01:25\n",
      "   ---------------------- ----------------- 115.1/203.0 MB 1.0 MB/s eta 0:01:24\n",
      "   ---------------------- ----------------- 115.6/203.0 MB 1.0 MB/s eta 0:01:24\n",
      "   ---------------------- ----------------- 115.9/203.0 MB 1.1 MB/s eta 0:01:23\n",
      "   ---------------------- ----------------- 116.4/203.0 MB 1.1 MB/s eta 0:01:23\n",
      "   ---------------------- ----------------- 116.7/203.0 MB 1.1 MB/s eta 0:01:22\n",
      "   ----------------------- ---------------- 116.9/203.0 MB 1.1 MB/s eta 0:01:21\n",
      "   ----------------------- ---------------- 116.9/203.0 MB 1.1 MB/s eta 0:01:21\n",
      "   ----------------------- ---------------- 116.9/203.0 MB 1.1 MB/s eta 0:01:21\n",
      "   ----------------------- ---------------- 116.9/203.0 MB 1.1 MB/s eta 0:01:21\n",
      "   ----------------------- ---------------- 117.2/203.0 MB 1.1 MB/s eta 0:01:20\n",
      "   ----------------------- ---------------- 117.2/203.0 MB 1.1 MB/s eta 0:01:20\n",
      "   ----------------------- ---------------- 117.2/203.0 MB 1.1 MB/s eta 0:01:20\n",
      "   ----------------------- ---------------- 117.7/203.0 MB 1.1 MB/s eta 0:01:20\n",
      "   ----------------------- ---------------- 118.0/203.0 MB 1.1 MB/s eta 0:01:18\n",
      "   ----------------------- ---------------- 118.2/203.0 MB 1.1 MB/s eta 0:01:18\n",
      "   ----------------------- ---------------- 118.8/203.0 MB 1.1 MB/s eta 0:01:17\n",
      "   ----------------------- ---------------- 119.0/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 119.3/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 119.5/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 119.8/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 119.8/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.1/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.3/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 120.6/203.0 MB 1.1 MB/s eta 0:01:16\n",
      "   ---------------------- --------------- 120.8/203.0 MB 980.7 kB/s eta 0:01:24\n",
      "   ---------------------- --------------- 120.8/203.0 MB 980.7 kB/s eta 0:01:24\n",
      "   ---------------------- --------------- 121.1/203.0 MB 963.2 kB/s eta 0:01:26\n",
      "   ---------------------- --------------- 121.4/203.0 MB 946.7 kB/s eta 0:01:27\n",
      "   ---------------------- --------------- 121.6/203.0 MB 937.9 kB/s eta 0:01:27\n",
      "   ---------------------- --------------- 121.9/203.0 MB 935.4 kB/s eta 0:01:27\n",
      "   ---------------------- --------------- 121.9/203.0 MB 935.4 kB/s eta 0:01:27\n",
      "   ---------------------- --------------- 122.2/203.0 MB 920.4 kB/s eta 0:01:28\n",
      "   ---------------------- --------------- 122.2/203.0 MB 920.4 kB/s eta 0:01:28\n",
      "   ---------------------- --------------- 122.2/203.0 MB 920.4 kB/s eta 0:01:28\n",
      "   ---------------------- --------------- 122.2/203.0 MB 920.4 kB/s eta 0:01:28\n",
      "   ---------------------- --------------- 122.2/203.0 MB 920.4 kB/s eta 0:01:28\n",
      "   ---------------------- --------------- 122.2/203.0 MB 920.4 kB/s eta 0:01:28\n",
      "   ---------------------- --------------- 122.2/203.0 MB 920.4 kB/s eta 0:01:28\n",
      "   ---------------------- --------------- 122.2/203.0 MB 920.4 kB/s eta 0:01:28\n",
      "   ---------------------- --------------- 122.4/203.0 MB 803.9 kB/s eta 0:01:41\n",
      "   ---------------------- --------------- 122.7/203.0 MB 796.4 kB/s eta 0:01:41\n",
      "   ---------------------- --------------- 122.7/203.0 MB 796.4 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 123.2/203.0 MB 791.0 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 123.2/203.0 MB 791.0 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 123.7/203.0 MB 786.4 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 124.0/203.0 MB 782.2 kB/s eta 0:01:42\n",
      "   ----------------------- -------------- 124.3/203.0 MB 786.9 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 124.5/203.0 MB 779.7 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 124.8/203.0 MB 773.8 kB/s eta 0:01:42\n",
      "   ----------------------- -------------- 125.0/203.0 MB 778.1 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 125.3/203.0 MB 772.6 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 125.6/203.0 MB 769.8 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 125.6/203.0 MB 769.8 kB/s eta 0:01:41\n",
      "   ----------------------- -------------- 125.8/203.0 MB 761.4 kB/s eta 0:01:42\n",
      "   ----------------------- -------------- 126.4/203.0 MB 769.4 kB/s eta 0:01:40\n",
      "   ----------------------- -------------- 126.6/203.0 MB 809.4 kB/s eta 0:01:35\n",
      "   ----------------------- -------------- 126.9/203.0 MB 812.8 kB/s eta 0:01:34\n",
      "   ----------------------- -------------- 127.4/203.0 MB 822.6 kB/s eta 0:01:32\n",
      "   ----------------------- -------------- 127.7/203.0 MB 828.1 kB/s eta 0:01:32\n",
      "   ----------------------- -------------- 127.9/203.0 MB 831.7 kB/s eta 0:01:31\n",
      "   ------------------------ ------------- 128.5/203.0 MB 840.6 kB/s eta 0:01:29\n",
      "   ------------------------ ------------- 128.7/203.0 MB 846.4 kB/s eta 0:01:28\n",
      "   ------------------------ ------------- 128.7/203.0 MB 846.4 kB/s eta 0:01:28\n",
      "   ------------------------ ------------- 128.7/203.0 MB 846.4 kB/s eta 0:01:28\n",
      "   ------------------------ ------------- 129.2/203.0 MB 831.4 kB/s eta 0:01:29\n",
      "   ------------------------ ------------- 129.5/203.0 MB 824.4 kB/s eta 0:01:30\n",
      "   ------------------------ ------------- 129.8/203.0 MB 821.8 kB/s eta 0:01:30\n",
      "   ------------------------ ------------- 130.0/203.0 MB 816.0 kB/s eta 0:01:30\n",
      "   ------------------------ ------------- 130.3/203.0 MB 813.5 kB/s eta 0:01:30\n",
      "   ------------------------ ------------- 130.5/203.0 MB 804.7 kB/s eta 0:01:31\n",
      "   ------------------------ ------------- 130.8/203.0 MB 796.8 kB/s eta 0:01:31\n",
      "   ------------------------ ------------- 131.1/203.0 MB 788.9 kB/s eta 0:01:32\n",
      "   ------------------------ ------------- 131.6/203.0 MB 778.5 kB/s eta 0:01:32\n",
      "   ------------------------ ------------- 131.9/203.0 MB 779.3 kB/s eta 0:01:32\n",
      "   ------------------------ ------------- 132.4/203.0 MB 770.6 kB/s eta 0:01:32\n",
      "   ------------------------ ------------- 132.9/203.0 MB 770.5 kB/s eta 0:01:32\n",
      "   ------------------------ ------------- 133.4/203.0 MB 770.5 kB/s eta 0:01:31\n",
      "   ------------------------- ------------ 134.0/203.0 MB 769.8 kB/s eta 0:01:30\n",
      "   ------------------------- ------------ 134.5/203.0 MB 769.8 kB/s eta 0:01:30\n",
      "   ------------------------- ------------ 135.0/203.0 MB 830.0 kB/s eta 0:01:22\n",
      "   ------------------------- ------------ 135.5/203.0 MB 841.6 kB/s eta 0:01:21\n",
      "   ------------------------- ------------ 136.1/203.0 MB 853.5 kB/s eta 0:01:19\n",
      "   ------------------------- ------------ 136.3/203.0 MB 858.9 kB/s eta 0:01:18\n",
      "   ------------------------- ------------ 136.8/203.0 MB 868.7 kB/s eta 0:01:17\n",
      "   ------------------------- ------------ 137.1/203.0 MB 873.5 kB/s eta 0:01:16\n",
      "   ------------------------- ------------ 137.6/203.0 MB 883.0 kB/s eta 0:01:15\n",
      "   ------------------------- ------------ 137.9/203.0 MB 884.9 kB/s eta 0:01:14\n",
      "   ------------------------- ------------ 138.1/203.0 MB 890.0 kB/s eta 0:01:13\n",
      "   ------------------------- ------------ 138.4/203.0 MB 893.1 kB/s eta 0:01:13\n",
      "   ------------------------- ------------ 138.7/203.0 MB 891.9 kB/s eta 0:01:13\n",
      "   ------------------------- ------------ 138.7/203.0 MB 891.9 kB/s eta 0:01:13\n",
      "   ------------------------- ------------ 138.7/203.0 MB 891.9 kB/s eta 0:01:13\n",
      "   ------------------------- ------------ 138.7/203.0 MB 891.9 kB/s eta 0:01:13\n",
      "   ------------------------- ------------ 138.7/203.0 MB 891.9 kB/s eta 0:01:13\n",
      "   ------------------------- ------------ 138.7/203.0 MB 891.9 kB/s eta 0:01:13\n",
      "   ------------------------- ------------ 138.7/203.0 MB 891.9 kB/s eta 0:01:13\n",
      "   -------------------------- ----------- 138.9/203.0 MB 867.3 kB/s eta 0:01:14\n",
      "   -------------------------- ----------- 139.2/203.0 MB 877.9 kB/s eta 0:01:13\n",
      "   -------------------------- ----------- 139.7/203.0 MB 888.1 kB/s eta 0:01:12\n",
      "   -------------------------- ----------- 140.2/203.0 MB 905.0 kB/s eta 0:01:10\n",
      "   -------------------------- ----------- 140.5/203.0 MB 911.0 kB/s eta 0:01:09\n",
      "   -------------------------- ----------- 141.0/203.0 MB 918.5 kB/s eta 0:01:08\n",
      "   -------------------------- ----------- 141.6/203.0 MB 933.1 kB/s eta 0:01:06\n",
      "   -------------------------- ----------- 142.1/203.0 MB 944.7 kB/s eta 0:01:05\n",
      "   -------------------------- ----------- 142.6/203.0 MB 983.7 kB/s eta 0:01:02\n",
      "   ---------------------------- ----------- 143.4/203.0 MB 1.0 MB/s eta 0:01:00\n",
      "   ---------------------------- ----------- 144.2/203.0 MB 1.0 MB/s eta 0:00:58\n",
      "   ---------------------------- ----------- 144.7/203.0 MB 1.0 MB/s eta 0:00:57\n",
      "   ---------------------------- ----------- 145.5/203.0 MB 1.1 MB/s eta 0:00:55\n",
      "   ---------------------------- ----------- 145.8/203.0 MB 1.2 MB/s eta 0:00:48\n",
      "   ---------------------------- ----------- 146.3/203.0 MB 1.2 MB/s eta 0:00:47\n",
      "   ---------------------------- ----------- 146.8/203.0 MB 1.2 MB/s eta 0:00:46\n",
      "   ----------------------------- ---------- 147.3/203.0 MB 1.2 MB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 147.8/203.0 MB 1.2 MB/s eta 0:00:45\n",
      "   ----------------------------- ---------- 148.1/203.0 MB 1.3 MB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 148.6/203.0 MB 1.3 MB/s eta 0:00:44\n",
      "   ----------------------------- ---------- 148.9/203.0 MB 1.3 MB/s eta 0:00:43\n",
      "   ----------------------------- ---------- 149.2/203.0 MB 1.3 MB/s eta 0:00:43\n",
      "   ----------------------------- ---------- 149.7/203.0 MB 1.3 MB/s eta 0:00:43\n",
      "   ----------------------------- ---------- 149.9/203.0 MB 1.3 MB/s eta 0:00:42\n",
      "   ----------------------------- ---------- 150.5/203.0 MB 1.3 MB/s eta 0:00:42\n",
      "   ----------------------------- ---------- 150.7/203.0 MB 1.3 MB/s eta 0:00:42\n",
      "   ----------------------------- ---------- 150.7/203.0 MB 1.3 MB/s eta 0:00:42\n",
      "   ----------------------------- ---------- 151.0/203.0 MB 1.3 MB/s eta 0:00:41\n",
      "   ----------------------------- ---------- 151.5/203.0 MB 1.3 MB/s eta 0:00:41\n",
      "   ----------------------------- ---------- 151.8/203.0 MB 1.3 MB/s eta 0:00:41\n",
      "   ----------------------------- ---------- 152.0/203.0 MB 1.3 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 152.6/203.0 MB 1.3 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 152.6/203.0 MB 1.3 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 152.6/203.0 MB 1.3 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 152.6/203.0 MB 1.3 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 152.8/203.0 MB 1.3 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 152.8/203.0 MB 1.3 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 152.8/203.0 MB 1.3 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 153.1/203.0 MB 1.2 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 153.1/203.0 MB 1.2 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 153.1/203.0 MB 1.2 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 153.1/203.0 MB 1.2 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 153.1/203.0 MB 1.2 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 153.4/203.0 MB 1.2 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 153.4/203.0 MB 1.2 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 153.6/203.0 MB 1.2 MB/s eta 0:00:41\n",
      "   ------------------------------ --------- 154.1/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 154.1/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 154.4/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 154.9/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 154.9/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.2/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.2/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.2/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 155.7/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 156.0/203.0 MB 1.1 MB/s eta 0:00:42\n",
      "   ------------------------------ --------- 156.2/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 156.2/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 156.5/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 156.8/203.0 MB 1.2 MB/s eta 0:00:40\n",
      "   ------------------------------ --------- 157.0/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------ --------- 157.0/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------ --------- 157.3/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------ --------- 157.3/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.5/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.5/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.8/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.8/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.8/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.8/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.8/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.8/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.8/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 157.8/203.0 MB 1.2 MB/s eta 0:00:39\n",
      "   ------------------------------- -------- 158.1/203.0 MB 1.1 MB/s eta 0:00:41\n",
      "   ------------------------------- -------- 158.1/203.0 MB 1.1 MB/s eta 0:00:41\n",
      "   ------------------------------- -------- 158.1/203.0 MB 1.1 MB/s eta 0:00:41\n",
      "   ------------------------------- -------- 158.3/203.0 MB 1.1 MB/s eta 0:00:42\n",
      "   ------------------------------- -------- 158.3/203.0 MB 1.1 MB/s eta 0:00:42\n",
      "   ------------------------------- -------- 158.3/203.0 MB 1.1 MB/s eta 0:00:42\n",
      "   ------------------------------- -------- 158.3/203.0 MB 1.1 MB/s eta 0:00:42\n",
      "   ------------------------------- -------- 158.6/203.0 MB 1.1 MB/s eta 0:00:42\n",
      "   ------------------------------- -------- 158.6/203.0 MB 1.1 MB/s eta 0:00:42\n",
      "   ------------------------------- -------- 158.6/203.0 MB 1.1 MB/s eta 0:00:42\n",
      "   ------------------------------- -------- 158.6/203.0 MB 1.1 MB/s eta 0:00:42\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ------------------------------- -------- 158.9/203.0 MB 1.0 MB/s eta 0:00:44\n",
      "   ----------------------------- -------- 159.1/203.0 MB 341.7 kB/s eta 0:02:09\n",
      "   ----------------------------- -------- 159.1/203.0 MB 341.7 kB/s eta 0:02:09\n",
      "   ----------------------------- -------- 159.1/203.0 MB 341.7 kB/s eta 0:02:09\n",
      "   ----------------------------- -------- 159.4/203.0 MB 315.6 kB/s eta 0:02:19\n",
      "   ----------------------------- -------- 159.4/203.0 MB 315.6 kB/s eta 0:02:19\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.6/203.0 MB 297.9 kB/s eta 0:02:26\n",
      "   ----------------------------- -------- 159.9/203.0 MB 222.0 kB/s eta 0:03:15\n",
      "   ----------------------------- -------- 159.9/203.0 MB 222.0 kB/s eta 0:03:15\n",
      "   ----------------------------- -------- 160.2/203.0 MB 219.6 kB/s eta 0:03:16\n",
      "   ----------------------------- -------- 160.2/203.0 MB 219.6 kB/s eta 0:03:16\n",
      "   ------------------------------ ------- 160.4/203.0 MB 210.4 kB/s eta 0:03:23\n",
      "   ------------------------------ ------- 160.4/203.0 MB 210.4 kB/s eta 0:03:23\n",
      "   ------------------------------ ------- 160.7/203.0 MB 210.0 kB/s eta 0:03:22\n",
      "   ------------------------------ ------- 161.0/203.0 MB 201.8 kB/s eta 0:03:29\n",
      "   ------------------------------ ------- 161.2/203.0 MB 202.3 kB/s eta 0:03:27\n",
      "   ------------------------------ ------- 161.5/203.0 MB 206.2 kB/s eta 0:03:22\n",
      "   ------------------------------ ------- 161.7/203.0 MB 214.1 kB/s eta 0:03:13\n",
      "   ------------------------------ ------- 162.0/203.0 MB 221.7 kB/s eta 0:03:06\n",
      "   ------------------------------ ------- 162.5/203.0 MB 236.3 kB/s eta 0:02:52\n",
      "   ------------------------------ ------- 162.5/203.0 MB 236.3 kB/s eta 0:02:52\n",
      "   ------------------------------ ------- 163.1/203.0 MB 257.7 kB/s eta 0:02:36\n",
      "   ------------------------------ ------- 163.1/203.0 MB 257.7 kB/s eta 0:02:36\n",
      "   ------------------------------ ------- 163.3/203.0 MB 264.6 kB/s eta 0:02:31\n",
      "   ------------------------------ ------- 163.6/203.0 MB 271.5 kB/s eta 0:02:26\n",
      "   ------------------------------ ------- 163.6/203.0 MB 271.5 kB/s eta 0:02:26\n",
      "   ------------------------------ ------- 163.8/203.0 MB 275.8 kB/s eta 0:02:23\n",
      "   ------------------------------ ------- 163.8/203.0 MB 275.8 kB/s eta 0:02:23\n",
      "   ------------------------------ ------- 164.1/203.0 MB 281.1 kB/s eta 0:02:19\n",
      "   ------------------------------ ------- 164.4/203.0 MB 302.6 kB/s eta 0:02:08\n",
      "   ------------------------------ ------- 164.4/203.0 MB 302.6 kB/s eta 0:02:08\n",
      "   ------------------------------ ------- 164.6/203.0 MB 308.6 kB/s eta 0:02:05\n",
      "   ------------------------------ ------- 164.9/203.0 MB 313.9 kB/s eta 0:02:02\n",
      "   ------------------------------ ------- 164.9/203.0 MB 313.9 kB/s eta 0:02:02\n",
      "   ------------------------------ ------- 165.2/203.0 MB 318.4 kB/s eta 0:02:00\n",
      "   ------------------------------ ------- 165.4/203.0 MB 324.7 kB/s eta 0:01:56\n",
      "   ------------------------------ ------- 165.4/203.0 MB 324.7 kB/s eta 0:01:56\n",
      "   ------------------------------- ------ 165.9/203.0 MB 337.9 kB/s eta 0:01:50\n",
      "   ------------------------------- ------ 166.2/203.0 MB 344.9 kB/s eta 0:01:47\n",
      "   ------------------------------- ------ 166.5/203.0 MB 351.4 kB/s eta 0:01:45\n",
      "   ------------------------------- ------ 166.7/203.0 MB 353.2 kB/s eta 0:01:43\n",
      "   ------------------------------- ------ 167.0/203.0 MB 360.3 kB/s eta 0:01:41\n",
      "   ------------------------------- ------ 167.2/203.0 MB 362.2 kB/s eta 0:01:39\n",
      "   ------------------------------- ------ 167.8/203.0 MB 376.1 kB/s eta 0:01:34\n",
      "   ------------------------------- ------ 168.0/203.0 MB 376.9 kB/s eta 0:01:33\n",
      "   ------------------------------- ------ 168.6/203.0 MB 385.9 kB/s eta 0:01:30\n",
      "   ------------------------------- ------ 168.6/203.0 MB 385.9 kB/s eta 0:01:30\n",
      "   ------------------------------- ------ 168.6/203.0 MB 385.9 kB/s eta 0:01:30\n",
      "   ------------------------------- ------ 168.6/203.0 MB 385.9 kB/s eta 0:01:30\n",
      "   ------------------------------- ------ 168.6/203.0 MB 385.9 kB/s eta 0:01:30\n",
      "   ------------------------------- ------ 168.6/203.0 MB 385.9 kB/s eta 0:01:30\n",
      "   ------------------------------- ------ 168.8/203.0 MB 369.7 kB/s eta 0:01:33\n",
      "   ------------------------------- ------ 168.8/203.0 MB 369.7 kB/s eta 0:01:33\n",
      "   ------------------------------- ------ 169.1/203.0 MB 384.2 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.1/203.0 MB 384.2 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.1/203.0 MB 384.2 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.3/203.0 MB 384.1 kB/s eta 0:01:28\n",
      "   ------------------------------- ------ 169.3/203.0 MB 384.1 kB/s eta 0:01:28\n",
      "   ------------------------------- ------ 169.3/203.0 MB 384.1 kB/s eta 0:01:28\n",
      "   ------------------------------- ------ 169.3/203.0 MB 384.1 kB/s eta 0:01:28\n",
      "   ------------------------------- ------ 169.3/203.0 MB 384.1 kB/s eta 0:01:28\n",
      "   ------------------------------- ------ 169.3/203.0 MB 384.1 kB/s eta 0:01:28\n",
      "   ------------------------------- ------ 169.3/203.0 MB 384.1 kB/s eta 0:01:28\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.6/203.0 MB 376.4 kB/s eta 0:01:29\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 169.9/203.0 MB 571.3 kB/s eta 0:00:59\n",
      "   ------------------------------- ------ 170.1/203.0 MB 500.5 kB/s eta 0:01:06\n",
      "   ------------------------------- ------ 170.4/203.0 MB 503.4 kB/s eta 0:01:05\n",
      "   ------------------------------- ------ 170.7/203.0 MB 510.1 kB/s eta 0:01:04\n",
      "   ------------------------------- ------ 170.9/203.0 MB 516.4 kB/s eta 0:01:03\n",
      "   -------------------------------- ----- 171.2/203.0 MB 525.0 kB/s eta 0:01:01\n",
      "   -------------------------------- ----- 171.4/203.0 MB 531.7 kB/s eta 0:01:00\n",
      "   -------------------------------- ----- 171.7/203.0 MB 540.1 kB/s eta 0:00:59\n",
      "   -------------------------------- ----- 172.0/203.0 MB 545.9 kB/s eta 0:00:57\n",
      "   -------------------------------- ----- 172.2/203.0 MB 549.7 kB/s eta 0:00:57\n",
      "   -------------------------------- ----- 172.5/203.0 MB 555.2 kB/s eta 0:00:56\n",
      "   -------------------------------- ----- 172.5/203.0 MB 555.2 kB/s eta 0:00:56\n",
      "   -------------------------------- ----- 172.8/203.0 MB 559.2 kB/s eta 0:00:55\n",
      "   -------------------------------- ----- 173.0/203.0 MB 564.2 kB/s eta 0:00:54\n",
      "   -------------------------------- ----- 173.3/203.0 MB 569.4 kB/s eta 0:00:53\n",
      "   -------------------------------- ----- 173.5/203.0 MB 572.8 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 173.5/203.0 MB 572.8 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 173.5/203.0 MB 572.8 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 173.5/203.0 MB 572.8 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 173.8/203.0 MB 567.7 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 173.8/203.0 MB 567.7 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 174.1/203.0 MB 566.5 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 174.3/203.0 MB 572.4 kB/s eta 0:00:51\n",
      "   -------------------------------- ----- 174.6/203.0 MB 577.8 kB/s eta 0:00:50\n",
      "   -------------------------------- ----- 174.9/203.0 MB 581.5 kB/s eta 0:00:49\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.1/203.0 MB 584.8 kB/s eta 0:00:48\n",
      "   -------------------------------- ----- 175.4/203.0 MB 543.8 kB/s eta 0:00:51\n",
      "   -------------------------------- ----- 175.4/203.0 MB 543.8 kB/s eta 0:00:51\n",
      "   -------------------------------- ----- 175.4/203.0 MB 543.8 kB/s eta 0:00:51\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.6/203.0 MB 537.2 kB/s eta 0:00:52\n",
      "   -------------------------------- ----- 175.9/203.0 MB 447.5 kB/s eta 0:01:01\n",
      "   -------------------------------- ----- 176.2/203.0 MB 439.0 kB/s eta 0:01:02\n",
      "   --------------------------------- ---- 176.4/203.0 MB 439.7 kB/s eta 0:01:01\n",
      "   --------------------------------- ---- 176.7/203.0 MB 440.1 kB/s eta 0:01:00\n",
      "   --------------------------------- ---- 176.9/203.0 MB 446.3 kB/s eta 0:00:59\n",
      "   --------------------------------- ---- 177.2/203.0 MB 451.0 kB/s eta 0:00:58\n",
      "   --------------------------------- ---- 177.7/203.0 MB 463.4 kB/s eta 0:00:55\n",
      "   --------------------------------- ---- 178.0/203.0 MB 467.0 kB/s eta 0:00:54\n",
      "   --------------------------------- ---- 178.5/203.0 MB 481.1 kB/s eta 0:00:51\n",
      "   --------------------------------- ---- 178.8/203.0 MB 484.1 kB/s eta 0:00:51\n",
      "   --------------------------------- ---- 179.3/203.0 MB 494.2 kB/s eta 0:00:49\n",
      "   --------------------------------- ---- 179.8/203.0 MB 507.3 kB/s eta 0:00:46\n",
      "   --------------------------------- ---- 180.1/203.0 MB 509.5 kB/s eta 0:00:46\n",
      "   --------------------------------- ---- 180.4/203.0 MB 515.8 kB/s eta 0:00:44\n",
      "   --------------------------------- ---- 180.9/203.0 MB 527.6 kB/s eta 0:00:43\n",
      "   --------------------------------- ---- 181.1/203.0 MB 533.6 kB/s eta 0:00:42\n",
      "   --------------------------------- ---- 181.7/203.0 MB 542.0 kB/s eta 0:00:40\n",
      "   ---------------------------------- --- 182.2/203.0 MB 551.4 kB/s eta 0:00:38\n",
      "   ---------------------------------- --- 182.7/203.0 MB 553.1 kB/s eta 0:00:37\n",
      "   ---------------------------------- --- 183.0/203.0 MB 559.5 kB/s eta 0:00:36\n",
      "   ---------------------------------- --- 183.8/203.0 MB 570.4 kB/s eta 0:00:34\n",
      "   ---------------------------------- --- 184.3/203.0 MB 578.2 kB/s eta 0:00:33\n",
      "   ---------------------------------- --- 184.8/203.0 MB 587.3 kB/s eta 0:00:32\n",
      "   ---------------------------------- --- 185.6/203.0 MB 597.0 kB/s eta 0:00:30\n",
      "   ---------------------------------- --- 186.1/203.0 MB 605.5 kB/s eta 0:00:28\n",
      "   ---------------------------------- --- 186.9/203.0 MB 621.7 kB/s eta 0:00:26\n",
      "   ----------------------------------- -- 187.4/203.0 MB 629.8 kB/s eta 0:00:25\n",
      "   ----------------------------------- -- 188.0/203.0 MB 660.6 kB/s eta 0:00:23\n",
      "   ----------------------------------- -- 188.7/203.0 MB 682.2 kB/s eta 0:00:21\n",
      "   ----------------------------------- -- 189.5/203.0 MB 703.1 kB/s eta 0:00:20\n",
      "   ----------------------------------- -- 190.1/203.0 MB 717.1 kB/s eta 0:00:19\n",
      "   ----------------------------------- -- 190.6/203.0 MB 729.8 kB/s eta 0:00:18\n",
      "   ----------------------------------- -- 191.4/203.0 MB 751.0 kB/s eta 0:00:16\n",
      "   ----------------------------------- -- 191.6/203.0 MB 757.8 kB/s eta 0:00:16\n",
      "   ----------------------------------- -- 192.2/203.0 MB 777.6 kB/s eta 0:00:15\n",
      "   ------------------------------------ - 192.4/203.0 MB 781.6 kB/s eta 0:00:14\n",
      "   ------------------------------------ - 192.7/203.0 MB 786.3 kB/s eta 0:00:14\n",
      "   ------------------------------------ - 193.2/203.0 MB 796.8 kB/s eta 0:00:13\n",
      "   ------------------------------------ - 193.7/203.0 MB 838.4 kB/s eta 0:00:12\n",
      "   ------------------------------------ - 194.0/203.0 MB 844.3 kB/s eta 0:00:11\n",
      "   ------------------------------------ - 194.2/203.0 MB 847.4 kB/s eta 0:00:11\n",
      "   ------------------------------------ - 194.8/203.0 MB 857.2 kB/s eta 0:00:10\n",
      "   ------------------------------------ - 195.0/203.0 MB 862.4 kB/s eta 0:00:10\n",
      "   ------------------------------------ - 195.3/203.0 MB 862.2 kB/s eta 0:00:09\n",
      "   ------------------------------------ - 195.8/203.0 MB 935.2 kB/s eta 0:00:08\n",
      "   ------------------------------------ - 196.1/203.0 MB 941.0 kB/s eta 0:00:08\n",
      "   ------------------------------------ - 196.3/203.0 MB 944.5 kB/s eta 0:00:08\n",
      "   ------------------------------------ - 196.9/203.0 MB 952.1 kB/s eta 0:00:07\n",
      "   ------------------------------------ - 197.1/203.0 MB 954.5 kB/s eta 0:00:07\n",
      "   ------------------------------------ - 197.4/203.0 MB 956.9 kB/s eta 0:00:06\n",
      "   ------------------------------------ - 197.7/203.0 MB 957.2 kB/s eta 0:00:06\n",
      "   ------------------------------------ - 197.7/203.0 MB 957.2 kB/s eta 0:00:06\n",
      "   -------------------------------------  197.9/203.0 MB 955.9 kB/s eta 0:00:06\n",
      "   -------------------------------------  198.2/203.0 MB 956.7 kB/s eta 0:00:06\n",
      "   -------------------------------------  198.7/203.0 MB 963.2 kB/s eta 0:00:05\n",
      "   ---------------------------------------  199.0/203.0 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------------  199.2/203.0 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------------  199.2/203.0 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------------  199.5/203.0 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------------  199.8/203.0 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------------  200.0/203.0 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------------------  200.5/203.0 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------------------  201.1/203.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  201.3/203.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  201.6/203.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  201.9/203.0 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  202.1/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.6/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 203.0/203.0 MB 1.0 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/6.2 MB 2.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.3/6.2 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.6/6.2 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.1/6.2 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.6/6.2 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.1/6.2 MB 2.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.7/6.2 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 3.9/6.2 MB 2.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 4.5/6.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.7/6.2 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.0/6.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.5/6.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.20.1-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.6 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.8/1.6 MB 2.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.3/1.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.5.1-cp312-cp312-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.8/2.4 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.3/2.4 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 262.1/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, fsspec, filelock, torch, torchvision, torchaudio\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.10.0 mpmath-1.3.0 sympy-1.13.1 torch-2.5.1 torchaudio-2.5.1 torchvision-0.20.1 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --timeout 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "01d10c41-bbdf-4aae-9119-a371637b19c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f82bfb9430>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch  # Importa la librería principal de PyTorch, que es utilizada para construir y entrenar modelos de redes neuronales, y realizar operaciones tensoriales.\n",
    "\n",
    "import torch.nn as nn  # Importa el submódulo `nn` de PyTorch, que contiene clases para construir redes neuronales, como capas (e.g., Linear, Conv2d) y modelos predefinidos.\n",
    "\n",
    "import torch.nn.functional as F  # Importa el submódulo `functional` de PyTorch, que contiene funciones útiles para redes neuronales, como activaciones (ReLU, Sigmoid) y funciones de pérdida (CrossEntropy, MSELoss).\n",
    "\n",
    "import torch.optim as optim  # Importa el submódulo `optim` de PyTorch, que proporciona varios optimizadores (como SGD, Adam) para actualizar los parámetros del modelo durante el entrenamiento.\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader  # Importa herramientas esenciales para trabajar con datos en PyTorch:\n",
    "# - `Dataset`: Es la clase base para crear conjuntos de datos personalizados, donde defines cómo acceder a los datos (implementando los métodos `__getitem__` y `__len__`).\n",
    "# - `DataLoader`: Es utilizado para cargar datos en lotes (batches) durante el entrenamiento de redes neuronales, facilitando el procesamiento eficiente de grandes volúmenes de datos.\n",
    "\n",
    "from collections import Counter  # Importa la clase `Counter` de la librería `collections`, que se utiliza para contar la frecuencia de elementos en una lista o conjunto de datos, útil en análisis de texto o para datos en general.\n",
    "\n",
    "import math  # Importa la librería estándar `math` que proporciona funciones matemáticas avanzadas como logaritmos, trigonometría, constantes matemáticas (por ejemplo, `pi`), etc.\n",
    "\n",
    "import numpy as np  # Importa la librería `numpy` para realizar operaciones eficientes con arrays multidimensionales y álgebra lineal.\n",
    "\n",
    "import re  # Importa la librería `re` para trabajar con expresiones regulares, que se utiliza para encontrar patrones y realizar búsquedas en texto.\n",
    "\n",
    "torch.manual_seed(23)  # Establece una semilla para la generación de números aleatorios en PyTorch, con el valor `23`. Esto asegura que los resultados sean reproducibles cada vez que se ejecute el código, garantizando consistencia en la inicialización de pesos y en el muestreo aleatorio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b530e45-19d9-48f6-be3d-ed27ee6ccddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Asigna el dispositivo de computación en el que se ejecutará el modelo. Si hay una GPU disponible (a través de CUDA), se utilizará la GPU ('cuda'). \n",
    "#Si no hay GPU disponible, se utilizará la CPU ('cpu').\n",
    "\n",
    "#print(device)  # Imprime el dispositivo que se ha asignado, es decir, 'cuda' si la GPU está disponible o 'cpu' si solo se puede usar la CPU.\n",
    "\n",
    "device = torch.device(\"cpu\")  # Asegúrate de usar la CPU\n",
    "model = model.to(device)  # Mueve el modelo a la CPU\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fce31f70-7fd6-4b7c-a214-18eb5d7ca357",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 50  # Define la longitud máxima de una secuencia de entrada (en este caso, 128 elementos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d2161d7-de1a-4093-9f2b-e3906ddea564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    La clase `PositionalEmbedding` genera una matriz de codificación posicional \n",
    "    que se añade a los embeddings de las palabras. Esta codificación posicional \n",
    "    permite que el modelo Transformer capture información sobre el orden de los tokens en la secuencia.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_seq_len = MAX_SEQ_LEN):\n",
    "        \"\"\"\n",
    "        Inicializa la codificación posicional.\n",
    "        Parámetros:\n",
    "        - d_model: La dimensionalidad del embedding de cada token (también usado en el Transformer).\n",
    "        - max_seq_len: La longitud máxima de la secuencia de entrada (por defecto, MAX_SEQ_LEN).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Crear la matriz de embeddings posicionales, con tamaño (max_seq_len, d_model)\n",
    "        self.pos_embed_matrix = torch.zeros(max_seq_len, d_model, device=device)\n",
    "        \n",
    "        # Generar las posiciones de los tokens, un tensor de tamaño (max_seq_len, 1)\n",
    "        token_pos = torch.arange(0, max_seq_len, dtype = torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Calcular el término de división para cada dimensión del embedding\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() \n",
    "                             * (-math.log(10000.0)/d_model))\n",
    "        \n",
    "        # Aplicar las funciones seno y coseno a las posiciones\n",
    "        self.pos_embed_matrix[:, 0::2] = torch.sin(token_pos * div_term)\n",
    "        self.pos_embed_matrix[:, 1::2] = torch.cos(token_pos * div_term)\n",
    "        \n",
    "        # Reorganizar la matriz para la suma posterior con el embedding de entrada\n",
    "        self.pos_embed_matrix = self.pos_embed_matrix.unsqueeze(0).transpose(0,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Realiza la operación de suma entre el embedding de entrada y la codificación posicional.\n",
    "        Parámetros:\n",
    "        - x: El tensor de entrada con forma (seq_len, batch_size, d_model).\n",
    "        \"\"\"\n",
    "        return x + self.pos_embed_matrix[:x.size(0), :]\n",
    "\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    La clase `MultiHeadAttention` implementa la atención multi-cabeza del Transformer. \n",
    "    La atención multi-cabeza permite que el modelo enfoque en diferentes partes de la secuencia \n",
    "    de entrada de forma simultánea.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model = 512, num_heads = 8):\n",
    "        \"\"\"\n",
    "        Inicializa la atención multi-cabeza.\n",
    "        Parámetros:\n",
    "        - d_model: Dimensionalidad del embedding de cada token.\n",
    "        - num_heads: Número de cabezas de atención.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Asegura que el tamaño del modelo sea divisible por el número de cabezas\n",
    "        assert d_model % num_heads == 0, 'Embedding size not compatible with num heads'\n",
    "        \n",
    "        # Dimensiones de la representación de cada cabeza\n",
    "        self.d_v = d_model // num_heads\n",
    "        self.d_k = self.d_v\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Definición de las matrices de transformación para Q (consulta), K (clave) y V (valor)\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask = None):\n",
    "        \"\"\"\n",
    "        Realiza la operación de atención multi-cabeza.\n",
    "        Parámetros:\n",
    "        - Q: Tensor de consultas con forma [batch_size, seq_len, d_model].\n",
    "        - K: Tensor de claves con forma [batch_size, seq_len, d_model].\n",
    "        - V: Tensor de valores con forma [batch_size, seq_len, d_model].\n",
    "        - mask: Máscara para evitar que ciertos tokens se vean durante la atención.\n",
    "        \"\"\"\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # Aplicar las transformaciones lineales a Q, K y V y luego dividirlas en varias cabezas\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Aplicar la atención de producto punto escalado\n",
    "        weighted_values, attention = self.scale_dot_product(Q, K, V, mask)\n",
    "        \n",
    "        # Reorganizar la salida de las cabezas de atención\n",
    "        weighted_values = weighted_values.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        \n",
    "        # Aplicar la transformación final\n",
    "        weighted_values = self.W_o(weighted_values)\n",
    "        \n",
    "        return weighted_values, attention\n",
    "        \n",
    "    def scale_dot_product(self, Q, K, V, mask = None):\n",
    "        \"\"\"\n",
    "        Realiza el producto punto escalado entre las consultas y las claves, \n",
    "        aplicando la máscara si es necesario.\n",
    "        Parámetros:\n",
    "        - Q: Consultas.\n",
    "        - K: Claves.\n",
    "        - V: Valores.\n",
    "        - mask: Máscara para aplicar.\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)  # Aplica la máscara\n",
    "        attention = F.softmax(scores, dim=-1)  # Aplica softmax\n",
    "        weighted_values = torch.matmul(attention, V)  # Calcula el valor ponderado\n",
    "        \n",
    "        return weighted_values, attention\n",
    "\n",
    "\n",
    "class PositionFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    La clase `PositionFeedForward` es una red feed-forward completamente conectada \n",
    "    que se aplica de forma independiente a cada posición de la secuencia.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        \"\"\"\n",
    "        Inicializa la red feed-forward.\n",
    "        Parámetros:\n",
    "        - d_model: Dimensionalidad del embedding de cada token.\n",
    "        - d_ff: Dimensionalidad de la capa interna de la red feed-forward.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)  # Capa lineal de entrada\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)  # Capa lineal de salida\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Realiza el paso hacia adelante de la red feed-forward.\n",
    "        Parámetros:\n",
    "        - x: Tensor de entrada con forma [batch_size, seq_len, d_model].\n",
    "        \"\"\"\n",
    "        return self.linear2(F.relu(self.linear1(x)))  # Activación ReLU\n",
    "\n",
    "\n",
    "class EncoderSubLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    La clase `EncoderSubLayer` define una subcapa del encoder del Transformer, \n",
    "    que consiste en una capa de atención seguida de una red feed-forward.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout = 0.1):\n",
    "        \"\"\"\n",
    "        Inicializa la subcapa del encoder.\n",
    "        Parámetros:\n",
    "        - d_model: Dimensionalidad del embedding de cada token.\n",
    "        - num_heads: Número de cabezas de atención.\n",
    "        - d_ff: Dimensionalidad de la red feed-forward.\n",
    "        - dropout: Tasa de dropout para regularización.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)  # Atención multi-cabeza\n",
    "        self.ffn = PositionFeedForward(d_model, d_ff)  # Red feed-forward\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # Normalización de capa\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)  # Dropout para regularización\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        \"\"\"\n",
    "        Realiza el paso hacia adelante de la subcapa del encoder.\n",
    "        Parámetros:\n",
    "        - x: Tensor de entrada.\n",
    "        - mask: Máscara para la atención.\n",
    "        \"\"\"\n",
    "        attention_score, _ = self.self_attn(x, x, x, mask)  # Atención de la entrada\n",
    "        x = x + self.dropout1(attention_score)  # Residual y dropout\n",
    "        x = self.norm1(x)  # Normalización\n",
    "        x = x + self.dropout2(self.ffn(x))  # Red feed-forward y residual\n",
    "        return self.norm2(x)  # Normalización final\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    La clase `Encoder` contiene varias capas de `EncoderSubLayer`, \n",
    "    cada una de las cuales realiza atención y procesamiento feed-forward.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Inicializa el encoder.\n",
    "        Parámetros:\n",
    "        - d_model: Dimensionalidad del embedding de cada token.\n",
    "        - num_heads: Número de cabezas de atención.\n",
    "        - d_ff: Dimensionalidad de la red feed-forward.\n",
    "        - num_layers: Número de capas en el encoder.\n",
    "        - dropout: Tasa de dropout.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)  # Normalización final del encoder\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Realiza el paso hacia adelante del encoder.\n",
    "        Parámetros:\n",
    "        - x: Tensor de entrada.\n",
    "        - mask: Máscara para la atención.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class DecoderSubLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    La clase `DecoderSubLayer` define una subcapa del decoder, que incluye\n",
    "    atención self-attention, cross-attention (atención entre encoder y decoder),\n",
    "    y una red feed-forward.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Inicializa la subcapa del decoder.\n",
    "        Parámetros:\n",
    "        - d_model: Dimensionalidad del embedding de cada token.\n",
    "        - num_heads: Número de cabezas de atención.\n",
    "        - d_ff: Dimensionalidad de la red feed-forward.\n",
    "        - dropout: Tasa de dropout.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)  # Atención self\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)  # Atención entre encoder y decoder\n",
    "        self.feed_forward = PositionFeedForward(d_model, d_ff)  # Red feed-forward\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # Normalización\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)  # Dropout\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, target_mask=None, encoder_mask=None):\n",
    "        \"\"\"\n",
    "        Realiza el paso hacia adelante de la subcapa del decoder.\n",
    "        Parámetros:\n",
    "        - x: Tensor de entrada (target).\n",
    "        - encoder_output: Salida del encoder (la que el decoder usa como contexto).\n",
    "        - target_mask: Máscara para la atención del target.\n",
    "        - encoder_mask: Máscara para la atención del encoder.\n",
    "        \"\"\"\n",
    "        attention_score, _ = self.self_attn(x, x, x, target_mask)  # Self-attention\n",
    "        x = x + self.dropout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        encoder_attn, _ = self.cross_attn(x, encoder_output, encoder_output, encoder_mask)  # Cross-attention\n",
    "        x = x + self.dropout2(encoder_attn)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        ff_output = self.feed_forward(x)  # Red feed-forward\n",
    "        x = x + self.dropout3(ff_output)\n",
    "        return self.norm3(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    La clase `Decoder` contiene varias capas de `DecoderSubLayer`, \n",
    "    cada una de las cuales realiza atención self-attention, cross-attention \n",
    "    y procesamiento feed-forward.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Inicializa el decoder.\n",
    "        Parámetros:\n",
    "        - d_model: Dimensionalidad del embedding de cada token.\n",
    "        - num_heads: Número de cabezas de atención.\n",
    "        - d_ff: Dimensionalidad de la red feed-forward.\n",
    "        - num_layers: Número de capas en el decoder.\n",
    "        - dropout: Tasa de dropout.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)  # Normalización final del decoder\n",
    "\n",
    "    def forward(self, x, encoder_output, target_mask, encoder_mask):\n",
    "        \"\"\"\n",
    "        Realiza el paso hacia adelante del decoder.\n",
    "        Parámetros:\n",
    "        - x: Tensor de entrada (target).\n",
    "        - encoder_output: Salida del encoder.\n",
    "        - target_mask: Máscara para la atención del target.\n",
    "        - encoder_mask: Máscara para la atención del encoder.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, target_mask, encoder_mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7d47fef4-c64b-4610-8181-109453077f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):  \n",
    "    # Definición de la clase Transformer, que hereda de nn.Module para construir un modelo de red neuronal.\n",
    "    # Esta clase representa la arquitectura completa del Transformer: un modelo de atención con encoder-decoder.\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers,\n",
    "                 input_vocab_size, target_vocab_size, \n",
    "                 max_len=MAX_SEQ_LEN, dropout=0.1):\n",
    "        # El método constructor inicializa los componentes del Transformer.\n",
    "        # Parámetros:\n",
    "        # - d_model: Dimensionalidad de los vectores de características.\n",
    "        # - num_heads: Número de cabezas de atención en cada capa de atención multi-cabeza.\n",
    "        # - d_ff: Dimensionalidad de la red completamente conectada en el modelo.\n",
    "        # - num_layers: Número de capas en el encoder y decoder.\n",
    "        # - input_vocab_size: Tamaño del vocabulario de entrada.\n",
    "        # - target_vocab_size: Tamaño del vocabulario de salida (target).\n",
    "        # - max_len: Longitud máxima de las secuencias de entrada y salida.\n",
    "        # - dropout: Tasa de dropout para regularización durante el entrenamiento.\n",
    "        \n",
    "        super().__init__()  # Llama al constructor de la clase base nn.Module.\n",
    "        \n",
    "        # Definición de las capas del modelo.\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)  \n",
    "        # Capa de embedding para la entrada (vocabulario de entrada -> d_model).\n",
    "        \n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        # Capa de embedding para la salida (vocabulario de salida -> d_model).\n",
    "        \n",
    "        self.pos_embedding = PositionalEmbedding(d_model, max_len)\n",
    "        # Capa para codificar las posiciones de los tokens en las secuencias de entrada y salida.\n",
    "        \n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        # Definición del encoder con el número de capas (num_layers), atención multi-cabeza (num_heads) y otros parámetros.\n",
    "        \n",
    "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        # Definición del decoder con los mismos parámetros que el encoder.\n",
    "        \n",
    "        self.output_layer = nn.Linear(d_model, target_vocab_size)\n",
    "        # Capa lineal que convierte la salida del decoder a las predicciones finales (tamaño vocabulario de salida).\n",
    "        \n",
    "    def forward(self, source, target):\n",
    "        # Método `forward` define cómo se realiza el paso hacia adelante a través del modelo.\n",
    "        # Parámetros:\n",
    "        # - source: Secuencia de entrada (source).\n",
    "        # - target: Secuencia de salida esperada (target).\n",
    "        \n",
    "        # Generación de las máscaras para el encoder y decoder.\n",
    "        source_mask, target_mask = self.mask(source, target)\n",
    "        \n",
    "        # Embedding y codificación posicional para la entrada (source).\n",
    "        source = self.encoder_embedding(source) * math.sqrt(self.encoder_embedding.embedding_dim)\n",
    "        # Realiza el embedding de la entrada y escala por la raíz cuadrada de la dimensión del embedding.\n",
    "        \n",
    "        source = self.pos_embedding(source)  \n",
    "        # Añade la codificación posicional a la entrada.\n",
    "        \n",
    "        # Paso del encoder.\n",
    "        encoder_output = self.encoder(source, source_mask)\n",
    "        \n",
    "        # Embedding y codificación posicional para la salida (target).\n",
    "        target = self.decoder_embedding(target) * math.sqrt(self.decoder_embedding.embedding_dim)\n",
    "        # Realiza el embedding de la salida y escala de manera similar.\n",
    "        \n",
    "        target = self.pos_embedding(target)  \n",
    "        # Añade la codificación posicional a la salida.\n",
    "        \n",
    "        # Paso del decoder.\n",
    "        output = self.decoder(target, encoder_output, target_mask, source_mask)\n",
    "        # El decoder recibe la salida del encoder y las máscaras correspondientes para generar la salida.\n",
    "        \n",
    "        return self.output_layer(output)\n",
    "        # La salida del decoder se pasa por una capa lineal que produce la predicción final para cada token en la secuencia de salida.\n",
    "\n",
    "    \n",
    "    def mask(self, source, target):\n",
    "        # Método para crear las máscaras para el encoder y decoder.\n",
    "        # Las máscaras se utilizan para evitar que el modelo vea tokens futuros durante el entrenamiento (en el caso del decoder).\n",
    "        \n",
    "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)  \n",
    "        # Máscara para la entrada (source): crea una máscara para los tokens no nulos (diferentes de 0).\n",
    "        \n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(2)  \n",
    "        # Máscara para la salida (target): crea una máscara similar para la secuencia de salida.\n",
    "        \n",
    "        size = target.size(1)  \n",
    "        # Obtiene el tamaño de la secuencia de salida.\n",
    "        \n",
    "        no_mask = torch.tril(torch.ones((1, size, size), device=device)).bool()  \n",
    "        # Crea una máscara triangular inferior para el decoder, asegurándose de que cada posición solo pueda \"ver\" las posiciones anteriores (para evitar mirar futuros tokens).\n",
    "        \n",
    "        target_mask = target_mask & no_mask\n",
    "        # Aplica la máscara triangular inferior a la máscara de la salida (target_mask).\n",
    "        \n",
    "        return source_mask, target_mask  \n",
    "        # Devuelve las máscaras para la entrada y la salida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2df51453-d98b-4b5c-a522-a092edb01309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de parámetros de entrada y salida para el modelo\n",
    "seq_len_source = 10  # Longitud de la secuencia de entrada (source).\n",
    "seq_len_target = 10  # Longitud de la secuencia de salida (target).\n",
    "batch_size = 2  # Número de ejemplos en cada lote (batch).\n",
    "input_vocab_size = 50  # Tamaño del vocabulario para la secuencia de entrada (source). 50\n",
    "target_vocab_size = 50  # Tamaño del vocabulario para la secuencia de salida (target). 50\n",
    "\n",
    "# Generación de las secuencias de entrada (source) y salida (target)\n",
    "# Las secuencias de entrada y salida son vectores de enteros que representan índices\n",
    "# de palabras en los respectivos vocabularios.\n",
    "# `torch.randint` genera números enteros aleatorios en el rango [1, vocab_size).\n",
    "source = torch.randint(1, input_vocab_size, (batch_size, seq_len_source))  # Secuencias de entrada (source)\n",
    "target = torch.randint(1, target_vocab_size, (batch_size, seq_len_target))  # Secuencias de salida (target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "76e5908d-f8b0-4e65-a337-76abf8b4c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de hiperparámetros para el modelo Transformer\n",
    "d_model = 512 # Dimensionalidad de los vectores de características (embeddings) de entrada y salida.  #512\n",
    "               # Este valor define el tamaño de la representación interna de cada token.\n",
    "num_heads = 8  # Número de cabezas en la atención multi-cabeza (multi-head attention).\n",
    "               # Esto permite al modelo aprender diferentes representaciones para cada token en distintas subespacios de características.\n",
    "d_ff = 2048  # Tamaño de la capa de alimentación directa (feed-forward layer) interna. #2048\n",
    "             # Es el número de neuronas en la capa oculta de la red de alimentación directa.\n",
    "num_layers = 6  # Número de capas en el encoder y decoder del Transformer.\n",
    "               # A mayor número de capas, mayor capacidad de modelado, pero también mayor complejidad computacional.\n",
    "\n",
    "# Inicialización del modelo Transformer\n",
    "# Se crea una instancia del modelo Transformer con los hiperparámetros definidos previamente.\n",
    "# Este modelo es capaz de procesar secuencias de entrada y salida usando un mecanismo de atención\n",
    "# multi-cabeza y una red de alimentación directa.\n",
    "model = Transformer(d_model, num_heads, d_ff, num_layers,\n",
    "                  input_vocab_size, target_vocab_size, \n",
    "                  max_len=MAX_SEQ_LEN, dropout=0.1)\n",
    "\n",
    "# Mover el modelo al dispositivo adecuado (CPU o GPU)\n",
    "# Esto garantiza que el modelo se ejecute en el dispositivo donde la variable `device` haya sido configurada (puede ser 'cuda' si hay GPU disponible o 'cpu' si no).\n",
    "model = model.to(device)\n",
    "\n",
    "# Mover las secuencias de entrada (source) y salida (target) al mismo dispositivo que el modelo.\n",
    "# Esto asegura que las secuencias de entrada y salida estén en el dispositivo correcto para el procesamiento\n",
    "# durante la etapa de entrenamiento o inferencia.\n",
    "source = source.to(device)  # Mueve las secuencias de entrada (source) al dispositivo\n",
    "target = target.to(device)  # Mueve las secuencias de salida (target) al dispositivo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eb9d67dd-563f-4eaa-8b4c-18458e9ac36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(source, target)  # Realiza una pasada hacia adelante a través del modelo Transformer, generando las predicciones de salida para cada token en la secuencia 'target' basada en las secuencias de entrada 'source'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "19715a6f-eef6-4e01-966e-50f060ff45d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.shape torch.Size([2, 10, 50])\n"
     ]
    }
   ],
   "source": [
    "# Expected output shape -> [batch, seq_len_target, target_vocab_size] i.e. [2, 10, 50]\n",
    "print(f'output.shape {output.shape}')  # Imprime las dimensiones del tensor de salida, mostrando la forma de la salida del modelo (batch_size, seq_len_target, target_vocab_size).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1894bfc-c9f5-4a15-9498-a1b69d09ee8e",
   "metadata": {},
   "source": [
    "<h1>Translation Eng-Span</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bd170b79-77c5-47d2-be87-adc26d50e96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0                                                 1        2   \\\n",
      "0  1276                              Let's try something.     2481   \n",
      "1  1277                            I have to go to sleep.     2482   \n",
      "2  1280  Today is June 18th and it is Muiriel's birthday!     2485   \n",
      "3  1280  Today is June 18th and it is Muiriel's birthday!  1130137   \n",
      "4  1282                                Muiriel is 20 now.     2487   \n",
      "\n",
      "                                                  3    4    5    6    7    8   \\\n",
      "0                                  ¡Intentemos algo!  NaN  NaN  NaN  NaN  NaN   \n",
      "1                           Tengo que irme a dormir.  NaN  NaN  NaN  NaN  NaN   \n",
      "2  ¡Hoy es 18 de junio y es el cumpleaños de Muir...  NaN  NaN  NaN  NaN  NaN   \n",
      "3  ¡Hoy es el 18 de junio y es el cumpleaños de M...  NaN  NaN  NaN  NaN  NaN   \n",
      "4                      Ahora, Muiriel tiene 20 años.  NaN  NaN  NaN  NaN  NaN   \n",
      "\n",
      "    9    10  \n",
      "0  NaN  NaN  \n",
      "1  NaN  NaN  \n",
      "2  NaN  NaN  \n",
      "3  NaN  NaN  \n",
      "4  NaN  NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Definimos la ruta del archivo CSV que contiene las frases en inglés y español\n",
    "PATH = 'ing-esp.csv'  # Asegúrate de que la ruta sea correcta\n",
    "\n",
    "# Intentamos leer el archivo CSV especificando el delimitador correcto (punto y coma ';')\n",
    "# Leemos solo las columnas que nos interesan (1 para inglés y 3 para español)\n",
    "df = pd.read_csv(PATH, sep=';', encoding='ISO-8859-1', header=None, dtype=str, low_memory=False, on_bad_lines='skip')\n",
    "\n",
    "# Verificamos las primeras filas para entender la estructura del DataFrame\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "deddd76c-c886-4389-b5f9-e8319e015393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos solo las columnas 1 (inglés) y 3 (español)\n",
    "eng_spa_cols = df.iloc[:, [1, 3]]\n",
    "\n",
    "# Eliminamos cualquier columna vacía (si existiera alguna columna extra)\n",
    "eng_spa_cols = eng_spa_cols.dropna(axis=1, how='all')\n",
    "\n",
    "# Calculamos la longitud de las frases en inglés\n",
    "eng_spa_cols['length'] = eng_spa_cols.iloc[:, 0].str.len()\n",
    "\n",
    "# Ordenamos por la longitud de las frases en inglés\n",
    "eng_spa_cols = eng_spa_cols.sort_values(by='length')\n",
    "\n",
    "# Eliminamos la columna 'length' ya que no es necesaria para el archivo final\n",
    "eng_spa_cols = eng_spa_cols.drop(columns=['length'])\n",
    "\n",
    "# Especificamos la ruta de salida para el archivo\n",
    "output_file_path = 'textoIngleEspaniol.txt'\n",
    "\n",
    "# Guardamos el DataFrame resultante en un archivo de texto (tabulado y sin índice ni encabezados)\n",
    "eng_spa_cols.to_csv(output_file_path, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0a0f803a-ce96-4fa5-af94-a4dce6ba38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'textoIngleEspaniol.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2bf8a06d-d2fe-4445-b05b-0fcb1a60cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrimos el archivo en modo de lectura ('r') y con codificación 'utf-8' para manejar correctamente los caracteres especiales.\n",
    "# La variable 'PATH' debe contener la ruta del archivo que estamos leyendo.\n",
    "with open(PATH, 'r', encoding='utf-8') as f:\n",
    "    # Leemos todas las líneas del archivo y las almacenamos en una lista llamada 'lines'.\n",
    "    # Cada elemento de la lista 'lines' será una cadena de texto que corresponde a una línea del archivo.\n",
    "    lines = f.readlines()  # `readlines()` lee todo el contenido del archivo y lo divide por líneas.\n",
    "\n",
    "# Ahora procesamos cada línea en 'lines', y por cada línea:\n",
    "# 1. Eliminamos cualquier espacio en blanco o salto de línea extra con 'strip()'.\n",
    "# 2. Dividimos la línea en dos partes usando el delimitador de tabulación ('\\t') con 'split()'.\n",
    "#    Esto creará una lista con dos elementos: la primera frase (en inglés) y la segunda (en español).\n",
    "# 3. Filtramos las líneas que no contienen el delimitador '\\t', asegurándonos de que solo procesamos las líneas con pares de frases.\n",
    "eng_spa_pairs = [line.strip().split('\\t') for line in lines if '\\t' in line]\n",
    "\n",
    "# 'eng_spa_pairs' será ahora una lista de listas donde cada sublista contiene dos elementos:\n",
    "# 1. La primera posición es la frase en inglés.\n",
    "# 2. La segunda posición es la traducción en español.\n",
    "# Esto se utilizará para tareas como entrenamiento de un modelo de traducción, donde cada par de frases (inglés, español) es un ejemplo de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e7fda871-a9f4-47dd-807e-003c1d56cf49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['So?', '¿Y?'],\n",
       " ['OK.', '¡Órale!'],\n",
       " ['Hi!', '¡Hola!'],\n",
       " ['Go.', 'Ve.'],\n",
       " ['Hi.', '¡Hola!'],\n",
       " ['No.', 'No.'],\n",
       " ['Go.', 'Vete.'],\n",
       " ['Ah!', '¡Anda!'],\n",
       " ['Go.', 'Váyase.'],\n",
       " ['No!', '¡No!']]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora que hemos procesado las líneas y tenemos 'eng_spa_pairs', vamos a tomar los primeros 10 pares de frases\n",
    "# para verificar que el procesamiento de los datos se haya realizado correctamente.\n",
    "# Esto es útil para inspeccionar una pequeña muestra de los datos antes de utilizarla para entrenar un modelo.\n",
    "eng_spa_pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eab40ddf-cf89-4632-805b-5e3e348b08d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So?', 'OK.', 'Hi!', 'Go.', 'Hi.', 'No.', 'Go.', 'Ah!', 'Go.', 'No!']\n",
      "['¿Y?', '¡Órale!', '¡Hola!', 'Ve.', '¡Hola!', 'No.', 'Vete.', '¡Anda!', 'Váyase.', '¡No!']\n"
     ]
    }
   ],
   "source": [
    "# Primero, verificamos que cada par de frases tiene exactamente dos elementos\n",
    "# Para evitar que se generen errores de índice fuera de rango\n",
    "eng_spa_pairs = [pair for pair in eng_spa_pairs if len(pair) == 2]\n",
    "\n",
    "# Ahora podemos proceder a extraer las frases en inglés y español sin problema\n",
    "eng_sentences = [pair[0] for pair in eng_spa_pairs]  # Frases en inglés (primer elemento de cada par)\n",
    "spa_sentences = [pair[1] for pair in eng_spa_pairs]  # Frases en español (segundo elemento de cada par)\n",
    "\n",
    "# Verificamos los primeros 10 elementos de las listas para asegurarnos de que todo esté correcto\n",
    "print(eng_sentences[:10])\n",
    "print(spa_sentences[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9d39a6de-4567-4f88-a2b1-a66cb39c65c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # 1. Convertir todo el texto a minúsculas y eliminar espacios al principio y al final.\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    # 2. Reemplazar múltiples espacios seguidos por un solo espacio.\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    \n",
    "    # 3. Normalizar las vocales con tilde a su forma sin tilde (por ejemplo, \"á\" a \"a\").\n",
    "    sentence = re.sub(r\"[á]+\", \"a\", sentence)\n",
    "    sentence = re.sub(r\"[é]+\", \"e\", sentence)\n",
    "    sentence = re.sub(r\"[í]+\", \"i\", sentence)\n",
    "    sentence = re.sub(r\"[ó]+\", \"o\", sentence)\n",
    "    sentence = re.sub(r\"[ú]+\", \"u\", sentence)\n",
    "    \n",
    "    # 4. Eliminar todos los caracteres que no son letras del alfabeto (como signos de puntuación, números, etc.)\n",
    "    sentence = re.sub(r\"[^a-z]+\", \" \", sentence)\n",
    "    \n",
    "    # 5. Volver a eliminar cualquier espacio extra al principio o al final.\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    # 6. Añadir las etiquetas <sos> al principio y <eos> al final de la frase.\n",
    "    sentence = '<sos> ' + sentence + ' <eos>'\n",
    "    \n",
    "    # 7. Devolver la frase procesada.\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c0eb545a-8e60-483f-ab05-e69423e4e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = '¿Hola @ cómo estás? 123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7466cdd8-0076-44b1-a769-89f768fdd60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Hola @ cómo estás? 123\n",
      "<sos> hola como estas <eos>\n"
     ]
    }
   ],
   "source": [
    "# Imprime la variable 's1' en su forma original.\n",
    "# La variable 's1' contiene una cadena de texto (string) que puede tener diferentes caracteres especiales, espacios extras, o incluso signos de puntuación.\n",
    "# Al utilizar 'print(s1)', estamos visualizando cómo es la cadena antes de cualquier tipo de preprocesamiento.\n",
    "print(s1)\n",
    "\n",
    "# Aplica la función 'preprocess_sentence' a la cadena 's1' y luego imprime el resultado.\n",
    "# La función 'preprocess_sentence' toma la cadena 's1', la procesa para convertirla a minúsculas, \n",
    "# eliminar caracteres no alfabéticos, eliminar espacios innecesarios, normalizar las vocales con tildes, y agregar etiquetas especiales al inicio y fin.\n",
    "# El resultado es una versión más \"limpia\" de la cadena original, que es más adecuada para tareas de procesamiento de lenguaje natural.\n",
    "# Se imprime la cadena procesada para que se pueda comparar con la forma original de 's1'.\n",
    "print(preprocess_sentence(s1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "662c8bb3-afce-460e-9775-8e31619a4c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica la función 'preprocess_sentence' a cada una de las frases en la lista 'eng_sentences'.\n",
    "# La lista 'eng_sentences' contiene frases en inglés, y queremos preprocesar cada una de esas frases antes de usarlas en un modelo de procesamiento de lenguaje.\n",
    "# Esto asegura que todas las frases en inglés sean limpiadas, normalizadas y formateadas de manera consistente (por ejemplo, convirtiéndolas a minúsculas, eliminando caracteres especiales y agregando las etiquetas <sos> y <eos>).\n",
    "# Usamos una lista por comprensión para aplicar la función 'preprocess_sentence' a cada frase de 'eng_sentences'.\n",
    "eng_sentences = [preprocess_sentence(sentence) for sentence in eng_sentences]\n",
    "\n",
    "# Aplica la misma función 'preprocess_sentence' a cada una de las frases en la lista 'spa_sentences'.\n",
    "# La lista 'spa_sentences' contiene frases en español, y de igual forma, necesitamos preprocesarlas para hacerlas aptas para su uso en el modelo.\n",
    "# Al igual que en el caso de las frases en inglés, se aplica la función de preprocesamiento a cada frase en 'spa_sentences' para limpiarlas y estructurarlas correctamente.\n",
    "spa_sentences = [preprocess_sentence(sentence) for sentence in spa_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "161af0b2-1dc9-4dbc-bc29-8a55f4cb7b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> y <eos>',\n",
       " '<sos> orale <eos>',\n",
       " '<sos> hola <eos>',\n",
       " '<sos> ve <eos>',\n",
       " '<sos> hola <eos>',\n",
       " '<sos> no <eos>',\n",
       " '<sos> vete <eos>',\n",
       " '<sos> anda <eos>',\n",
       " '<sos> vayase <eos>',\n",
       " '<sos> no <eos>']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spa_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "66ddff21-f2d6-4610-a15b-71031d528aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la función 'build_vocab' que se encarga de construir el vocabulario\n",
    "# a partir de una lista de oraciones. El vocabulario es esencial para convertir\n",
    "# las palabras en sus correspondientes índices (y viceversa), lo cual es necesario\n",
    "# para trabajar con modelos de aprendizaje automático.\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    # 'sentences' es una lista de oraciones (frases) que puede contener texto en inglés, español\n",
    "    # o cualquier otro idioma, dependiendo del contexto de uso de la función.\n",
    "    \n",
    "    # 1. Se extraen todas las palabras de todas las oraciones, de manera que cada palabra\n",
    "    #    se convierte en un único elemento en una lista. Para ello, primero recorremos cada\n",
    "    #    oración, luego separamos cada oración en palabras usando 'split()', y agregamos\n",
    "    #    todas las palabras a la lista 'words'.\n",
    "    words = [word for sentence in sentences for word in sentence.split()]\n",
    "    \n",
    "    # 2. Usamos un objeto 'Counter' para contar cuántas veces aparece cada palabra en la lista 'words'.\n",
    "    #    'word_count' es un diccionario que contiene como claves las palabras, y como valores,\n",
    "    #    la cantidad de veces que cada palabra aparece en todas las oraciones.\n",
    "    word_count = Counter(words)\n",
    "    \n",
    "    # 3. Ordenamos el diccionario de frecuencias 'word_count' de mayor a menor, es decir,\n",
    "    #    las palabras más frecuentes aparecerán primero en la lista ordenada.\n",
    "    #    'sorted_word_counts' es una lista de tuplas donde el primer elemento es la palabra\n",
    "    #    y el segundo es el conteo de ocurrencias de esa palabra.\n",
    "    sorted_word_counts = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # 4. Creamos el diccionario 'word2idx' que mapea cada palabra a un índice único.\n",
    "    #    El índice comienza en 2 porque los índices 0 y 1 se reservan para los tokens especiales '<pad>' y '<unk>'.\n",
    "    #    Usamos 'enumerate' para asignar un índice a cada palabra ordenada, comenzando desde el índice 2.\n",
    "    word2idx = {word: idx for idx, (word, _) in enumerate(sorted_word_counts, 2)}\n",
    "    \n",
    "    # 5. Asignamos los índices 0 y 1 a los tokens especiales:\n",
    "    #    - '<pad>': Se usa para rellenar secuencias a un tamaño constante (padding).\n",
    "    #    - '<unk>': Se usa para representar palabras desconocidas (out-of-vocabulary).\n",
    "    word2idx['<pad>'] = 0\n",
    "    word2idx['<unk>'] = 1\n",
    "    \n",
    "    # 6. Creamos un diccionario 'idx2word' que es el inverso de 'word2idx', es decir,\n",
    "    #    mapea índices a palabras. Esto nos permite recuperar una palabra a partir de su índice.\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    \n",
    "    # 7. Finalmente, la función devuelve dos diccionarios:\n",
    "    #    - 'word2idx': Mapea palabras a índices (para usar en el modelo).\n",
    "    #    - 'idx2word': Mapea índices a palabras (útil para la interpretación de resultados).\n",
    "    return word2idx, idx2word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "487e6fed-b683-4c2a-8c8f-f5646947a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos los vocabularios para las frases en inglés y español usando la función 'build_vocab'.\n",
    "# 'build_vocab' toma como entrada una lista de oraciones y devuelve dos diccionarios:\n",
    "# - 'word2idx': Mapea las palabras a índices numéricos.\n",
    "# - 'idx2word': Mapea los índices numéricos a palabras.\n",
    "# En este caso, estamos construyendo los vocabularios para las frases en inglés ('eng_sentences') y español ('spa_sentences').\n",
    "\n",
    "# Construcción del vocabulario para las frases en inglés.\n",
    "eng_word2idx, eng_idx2word = build_vocab(eng_sentences)\n",
    "# - 'eng_word2idx' será un diccionario que asigna un índice único a cada palabra en las frases en inglés.\n",
    "# - 'eng_idx2word' será el diccionario inverso, que permite obtener la palabra a partir de su índice.\n",
    "\n",
    "# Construcción del vocabulario para las frases en español.\n",
    "spa_word2idx, spa_idx2word = build_vocab(spa_sentences)\n",
    "# - 'spa_word2idx' será un diccionario similar al anterior, pero para las frases en español.\n",
    "# - 'spa_idx2word' será el diccionario inverso de 'spa_word2idx', para convertir índices en palabras.\n",
    "\n",
    "# Determinamos el tamaño del vocabulario de inglés y español (número de palabras únicas en cada idioma).\n",
    "eng_vocab_size = len(eng_word2idx)\n",
    "# - 'eng_vocab_size' almacenará el número total de palabras únicas (incluyendo los tokens especiales <pad>, <unk>) en el vocabulario de inglés.\n",
    "\n",
    "spa_vocab_size = len(spa_word2idx)\n",
    "# - 'spa_vocab_size' almacenará el número total de palabras únicas (incluyendo los tokens especiales <pad>, <unk>) en el vocabulario de español.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4bdb6cbb-1c65-4d4c-9a5b-7bdab3deeb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27464 46617\n"
     ]
    }
   ],
   "source": [
    "print(eng_vocab_size, spa_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9338a663-7d34-4292-8957-69e0f98a2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la clase 'EngSpaDataset' que hereda de 'Dataset'.\n",
    "# Esta clase se utiliza para crear un conjunto de datos personalizado (dataset) en PyTorch.\n",
    "# El objetivo de esta clase es proporcionar una forma estructurada de acceder a las frases en inglés y español\n",
    "# y convertir las palabras en índices numéricos usando los vocabularios previamente construidos.\n",
    "\n",
    "class EngSpaDataset(Dataset):\n",
    "    # El constructor (__init__) se encarga de inicializar el conjunto de datos.\n",
    "    def __init__(self, eng_sentences, spa_sentences, eng_word2idx, spa_word2idx):\n",
    "        # 'eng_sentences' son las frases en inglés.\n",
    "        # 'spa_sentences' son las frases en español.\n",
    "        # 'eng_word2idx' es el diccionario que mapea las palabras en inglés a índices.\n",
    "        # 'spa_word2idx' es el diccionario que mapea las palabras en español a índices.\n",
    "        self.eng_sentences = eng_sentences  # Almacenamos las frases en inglés.\n",
    "        self.spa_sentences = spa_sentences  # Almacenamos las frases en español.\n",
    "        self.eng_word2idx = eng_word2idx  # Almacenamos el vocabulario de inglés.\n",
    "        self.spa_word2idx = spa_word2idx  # Almacenamos el vocabulario de español.\n",
    "        \n",
    "    # El método __len__ devuelve el tamaño del conjunto de datos.\n",
    "    # En este caso, el número de oraciones en inglés (y en español, que son iguales).\n",
    "    def __len__(self):\n",
    "        # Devuelve la cantidad de frases en el conjunto de datos.\n",
    "        return len(self.eng_sentences)\n",
    "    \n",
    "    # El método __getitem__ obtiene un par de frases (inglés, español) dado un índice 'idx'.\n",
    "    # Este método será utilizado por el DataLoader de PyTorch para acceder a los datos de manera eficiente.\n",
    "    def __getitem__(self, idx):\n",
    "        # 'eng_sentence' es la oración en inglés en el índice 'idx'.\n",
    "        eng_sentence = self.eng_sentences[idx]\n",
    "        # 'spa_sentence' es la oración en español en el índice 'idx'.\n",
    "        spa_sentence = self.spa_sentences[idx]\n",
    "        \n",
    "        # Convertimos la oración en inglés en una lista de índices, donde cada palabra se reemplaza por su índice en el vocabulario.\n",
    "        # Si una palabra no se encuentra en el vocabulario, se utiliza el índice del token desconocido (<unk>).\n",
    "        eng_idxs = [self.eng_word2idx.get(word, self.eng_word2idx['<unk>']) for word in eng_sentence.split()]\n",
    "        \n",
    "        # Convertimos la oración en español en una lista de índices, de manera similar a lo hecho con el inglés.\n",
    "        # Si una palabra no se encuentra en el vocabulario, se utiliza el índice del token desconocido (<unk>).\n",
    "        spa_idxs = [self.spa_word2idx.get(word, self.spa_word2idx['<unk>']) for word in spa_sentence.split()]\n",
    "        \n",
    "        # Retornamos las oraciones convertidas en índices como tensores de PyTorch.\n",
    "        # Los tensores son estructuras de datos que PyTorch puede procesar.\n",
    "        return torch.tensor(eng_idxs), torch.tensor(spa_idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7e0b638f-2d59-441b-a8d2-b2e516d803a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función 'collate_fn', que es utilizada para procesar un lote de datos en el DataLoader.\n",
    "# Esta función se encarga de manejar el procesamiento de datos dentro del lote antes de ser alimentados al modelo.\n",
    "# Principalmente, realiza dos tareas importantes: \n",
    "# - Recortar o limitar las secuencias a una longitud máxima.\n",
    "# - Realizar padding (relleno) para asegurar que todas las secuencias dentro del lote tengan la misma longitud.\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 'batch' es una lista de tuplas que contienen las oraciones en inglés y español (en índices).\n",
    "    # Cada elemento en 'batch' es una tupla con una secuencia en inglés y una secuencia en español.\n",
    "    \n",
    "    # 'eng_batch' y 'spa_batch' contienen las secuencias de inglés y español respectivamente.\n",
    "    # Usamos zip(*batch) para separar las oraciones de inglés y español en dos listas separadas.\n",
    "    eng_batch, spa_batch = zip(*batch)\n",
    "\n",
    "    # Para cada secuencia en inglés, recortamos a 'MAX_SEQ_LEN' y usamos '.clone().detach()' para evitar que los cambios\n",
    "    # en el tensor afecten al original. Esto también asegura que el tensor es independiente de cualquier gráfico de computación.\n",
    "    eng_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in eng_batch]\n",
    "    \n",
    "    # Para cada secuencia en español, también la recortamos a 'MAX_SEQ_LEN' y la clonamos para desvincularla.\n",
    "    spa_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in spa_batch]\n",
    "    \n",
    "    # Usamos 'torch.nn.utils.rnn.pad_sequence' para rellenar las secuencias en inglés con ceros (padding_value=0) \n",
    "    # hasta la longitud máxima en el lote. 'batch_first=True' asegura que la dimensión del batch sea la primera dimensión.\n",
    "    eng_batch = torch.nn.utils.rnn.pad_sequence(eng_batch, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Hacemos lo mismo para las secuencias en español, asegurando que tengan el mismo tamaño en el lote.\n",
    "    spa_batch = torch.nn.utils.rnn.pad_sequence(spa_batch, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Retornamos el lote de secuencias en inglés y español después de haber realizado el padding.\n",
    "    return eng_batch, spa_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ee01557c-dddd-43fa-9c81-519fae3c1288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función 'train', que se encarga de entrenar el modelo durante un número determinado de épocas.\n",
    "# El entrenamiento se realiza usando el optimizador y la función de pérdida proporcionados.\n",
    "# Durante cada época, el modelo se ajusta a los datos de entrenamiento en función de la pérdida y el gradiente calculados.\n",
    "# La función recibe los siguientes parámetros:\n",
    "# - 'model': El modelo que estamos entrenando.\n",
    "# - 'dataloader': El DataLoader que proporciona los lotes de datos (eng_batch, spa_batch).\n",
    "# - 'loss_function': La función de pérdida que se utilizará para calcular el error entre la predicción del modelo y la salida esperada.\n",
    "# - 'optimiser': El optimizador que ajustará los parámetros del modelo.\n",
    "# - 'epochs': El número de épocas durante las cuales el modelo será entrenado.\n",
    "\n",
    "def train(model, dataloader, loss_function, optimiser, epochs):\n",
    "    # Configuramos el modelo en modo de entrenamiento.\n",
    "    # 'model.train()' indica que el modelo se encuentra en el estado de entrenamiento, activando comportamientos como el dropout.\n",
    "    model.train()\n",
    "    \n",
    "    # Recorremos las épocas del entrenamiento.\n",
    "    for epoch in range(epochs):\n",
    "        # Inicializamos una variable para acumular la pérdida total de la época.\n",
    "        total_loss = 0 \n",
    "        \n",
    "        # Iteramos sobre los lotes del DataLoader.\n",
    "        # En cada iteración, 'eng_batch' es el lote de secuencias en inglés y 'spa_batch' es el lote de secuencias en español.\n",
    "        for i, (eng_batch, spa_batch) in enumerate(dataloader):\n",
    "            # Movemos los lotes al dispositivo (CPU o GPU) donde se encuentra el modelo.\n",
    "            eng_batch = eng_batch.to(device)\n",
    "            spa_batch = spa_batch.to(device)\n",
    "            \n",
    "            # Preprocesamiento del decoder:\n",
    "            # Separamos el lote de salida del español en dos partes:\n",
    "            # - 'target_input' es la entrada al decoder (todas las palabras excepto el último token de cada secuencia).\n",
    "            # - 'target_output' es la salida esperada del decoder (todas las palabras excepto el primero de cada secuencia).\n",
    "            target_input = spa_batch[:, :-1]\n",
    "            target_output = spa_batch[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            # Ponemos a cero los gradientes acumulados de las iteraciones anteriores para evitar que se acumulen en el siguiente paso.\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            # Realizamos una pasada hacia adelante a través del modelo con el batch de inglés y el input del decoder.\n",
    "            output = model(eng_batch, target_input)\n",
    "            \n",
    "            # Aplanamos la salida del modelo para que tenga la forma necesaria para calcular la pérdida.\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            \n",
    "            # Calculamos la pérdida entre la salida del modelo y la salida esperada (target_output).\n",
    "            loss = loss_function(output, target_output)\n",
    "            \n",
    "            # Calculamos los gradientes a partir de la pérdida.\n",
    "            loss.backward()\n",
    "            \n",
    "            # Actualizamos los parámetros del modelo usando el optimizador.\n",
    "            optimiser.step()\n",
    "            \n",
    "            # Sumamos la pérdida de esta iteración a la pérdida total.\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Calculamos la pérdida promedio de la época.\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        \n",
    "        # Imprimimos la pérdida promedio por cada época.\n",
    "        print(f'Epoch: {epoch}/{epochs}, Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "06a0b778-9467-4f53-a95a-c16bce30c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el tamaño del lote (batch) para el entrenamiento. En este caso, el tamaño del lote es 64.\n",
    "BATCH_SIZE = 8 #64\n",
    "\n",
    "# Creamos una instancia del dataset que contiene las frases en inglés y español, junto con sus respectivos diccionarios de índices (eng_word2idx y spa_word2idx).\n",
    "# 'EngSpaDataset' es una clase que hereda de 'Dataset' y está diseñada para gestionar pares de frases en inglés y español,\n",
    "# así como la conversión de esas frases en listas de índices de palabras (usando los diccionarios 'eng_word2idx' y 'spa_word2idx').\n",
    "dataset = EngSpaDataset(eng_sentences, spa_sentences, eng_word2idx, spa_word2idx)\n",
    "\n",
    "# Creamos un DataLoader, que es una herramienta que se encarga de cargar los datos en lotes para el entrenamiento.\n",
    "# 'DataLoader' toma como entrada el dataset, el tamaño del lote (BATCH_SIZE), si se deben barajar los datos (shuffle=True),\n",
    "# y una función personalizada de 'collate_fn' que prepara los lotes para el entrenamiento.\n",
    "# El DataLoader permite iterar sobre el dataset en pequeños lotes, facilitando el entrenamiento del modelo.\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1a3ec03f-2e62-475f-82b5-dec0adc537ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una instancia del modelo Transformer utilizando los hiperparámetros especificados.\n",
    "\n",
    "# 'd_model=512': Establece la dimensionalidad de los vectores de características en el modelo Transformer. \n",
    "# Esto significa que cada token será representado por un vector de 512 dimensiones, lo que ayuda al modelo a capturar relaciones complejas entre las palabras. \n",
    "# Un valor comúnmente usado es 512, que proporciona un buen equilibrio entre poder de representación y eficiencia computacional.\n",
    "\n",
    "# 'num_heads=8': Define el número de \"cabezas\" en la atención multi-cabeza (multi-head attention). \n",
    "# Cada cabeza de atención aprende una representación diferente de cada token, permitiendo al modelo capturar distintas relaciones y patrones. \n",
    "# En este caso, el modelo tendrá 8 cabezas de atención, lo que mejora su capacidad de modelado.\n",
    "\n",
    "# 'd_ff=2048': Establece el tamaño de la capa de alimentación directa (feed-forward layer). \n",
    "# Esta capa toma las salidas de la atención multi-cabeza y las transforma. El valor de 2048 significa que la capa tendrá 2048 neuronas, lo que aumenta la capacidad del modelo para aprender transformaciones complejas.\n",
    "\n",
    "# 'num_layers=6': Define el número de capas tanto en el encoder como en el decoder. \n",
    "# Esto indica cuántas veces se aplicará la atención multi-cabeza y la red de alimentación directa en cada uno de estos componentes. \n",
    "# Con 6 capas, el modelo tendrá una capacidad mayor de capturar patrones complejos, aunque también aumenta el costo computacional.\n",
    "\n",
    "# 'input_vocab_size=eng_vocab_size': Especifica el tamaño del vocabulario de entrada, que corresponde al número de palabras únicas en las frases en inglés. \n",
    "# 'eng_vocab_size' es el tamaño del vocabulario de las frases en inglés, y este valor se utiliza para definir la capa de embedding de entrada del modelo.\n",
    "\n",
    "# 'target_vocab_size=spa_vocab_size': Define el tamaño del vocabulario de salida, correspondiente al número de palabras únicas en las frases en español. \n",
    "# 'spa_vocab_size' es el tamaño del vocabulario de las frases en español, y se usa para la capa de embedding en el decoder, así como para la capa de salida del modelo.\n",
    "\n",
    "# 'max_len=MAX_SEQ_LEN': Establece la longitud máxima de las secuencias que el modelo puede procesar. \n",
    "# 'MAX_SEQ_LEN' es una constante que define cuántos tokens (palabras o subpalabras) como máximo el modelo aceptará en las secuencias de entrada y salida. \n",
    "# Esto asegura que el modelo pueda manejar secuencias de longitud variable sin quedar limitado.\n",
    "\n",
    "# 'dropout=0.1': Define la tasa de dropout que se aplica para la regularización durante el entrenamiento. \n",
    "# En este caso, el valor de 0.1 significa que el modelo ignorará aleatoriamente el 10% de las conexiones durante el entrenamiento para prevenir el sobreajuste (overfitting).\n",
    "\n",
    "model = Transformer(d_model=128, num_heads=4, d_ff=512, num_layers=2, #d_model=512  d_ff=2048 num_layer=6\n",
    "                    input_vocab_size=eng_vocab_size, target_vocab_size=spa_vocab_size,\n",
    "                    max_len=MAX_SEQ_LEN, dropout=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f7a6bfa1-2cf2-4d1b-89fe-bd8d981b2db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movemos el modelo al dispositivo especificado (CPU o GPU). Esto asegura que las operaciones del modelo se realicen en el dispositivo que se haya configurado previamente (por ejemplo, 'cuda' si hay una GPU disponible o 'cpu' si no).\n",
    "# El dispositivo se define antes en el código y se usa para asegurar que tanto el modelo como los datos estén en el mismo lugar para ser procesados.\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Definimos la función de pérdida (loss function). \n",
    "# En este caso, utilizamos 'CrossEntropyLoss', que es adecuada para tareas de clasificación múltiple como la traducción de secuencias.\n",
    "# 'ignore_index=0' le indica a la función de pérdida que ignore las posiciones de los tokens de padding, es decir, los índices con valor 0.\n",
    "# Esto es importante porque el padding no debe afectar al cálculo de la pérdida, ya que no corresponde a una palabra o token real.\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Definimos el optimizador que se usará durante el entrenamiento. En este caso, usamos el optimizador 'Adam', que es un optimizador muy popular y eficiente.\n",
    "# 'model.parameters()' obtiene todos los parámetros del modelo que se actualizarán durante el entrenamiento.\n",
    "# 'lr=0.0001' especifica la tasa de aprendizaje (learning rate), que controla qué tan rápido el optimizador ajusta los parámetros durante el entrenamiento. \n",
    "# En este caso, la tasa de aprendizaje es pequeña para hacer ajustes finos en los parámetros durante el entrenamiento.\n",
    "\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bfde355d-9e15-4a9b-9842-d6c2472c5820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Batch 0/33266 - Loss: 10.9242\n",
      "Batch 100/33266 - Loss: 9.0613\n",
      "Batch 200/33266 - Loss: 7.3750\n",
      "Batch 300/33266 - Loss: 7.1300\n",
      "Batch 400/33266 - Loss: 7.0763\n",
      "Batch 500/33266 - Loss: 6.1902\n",
      "Batch 600/33266 - Loss: 5.9182\n",
      "Batch 700/33266 - Loss: 7.1199\n",
      "Batch 800/33266 - Loss: 6.4075\n",
      "Batch 900/33266 - Loss: 6.2202\n",
      "Batch 1000/33266 - Loss: 6.1031\n",
      "Batch 1100/33266 - Loss: 6.1848\n",
      "Batch 1200/33266 - Loss: 5.7866\n",
      "Batch 1300/33266 - Loss: 6.1444\n",
      "Batch 1400/33266 - Loss: 5.4407\n",
      "Batch 1500/33266 - Loss: 5.0448\n",
      "Batch 1600/33266 - Loss: 6.5918\n",
      "Batch 1700/33266 - Loss: 5.5889\n",
      "Batch 1800/33266 - Loss: 5.7957\n",
      "Batch 1900/33266 - Loss: 5.4905\n",
      "Batch 2000/33266 - Loss: 6.2380\n",
      "Batch 2100/33266 - Loss: 6.4696\n",
      "Batch 2200/33266 - Loss: 6.1172\n",
      "Batch 2300/33266 - Loss: 4.9554\n",
      "Batch 2400/33266 - Loss: 5.5033\n",
      "Batch 2500/33266 - Loss: 5.2688\n",
      "Batch 2600/33266 - Loss: 5.8008\n",
      "Batch 2700/33266 - Loss: 5.4825\n",
      "Batch 2800/33266 - Loss: 6.4351\n",
      "Batch 2900/33266 - Loss: 5.6467\n",
      "Batch 3000/33266 - Loss: 5.2075\n",
      "Batch 3100/33266 - Loss: 5.8220\n",
      "Batch 3200/33266 - Loss: 5.7164\n",
      "Batch 3300/33266 - Loss: 5.6002\n",
      "Batch 3400/33266 - Loss: 4.8707\n",
      "Batch 3500/33266 - Loss: 5.1638\n",
      "Batch 3600/33266 - Loss: 5.5422\n",
      "Batch 3700/33266 - Loss: 5.9154\n",
      "Batch 3800/33266 - Loss: 5.1926\n",
      "Batch 3900/33266 - Loss: 5.9067\n",
      "Batch 4000/33266 - Loss: 5.4142\n",
      "Batch 4100/33266 - Loss: 5.4570\n",
      "Batch 4200/33266 - Loss: 5.1392\n",
      "Batch 4300/33266 - Loss: 5.0570\n",
      "Batch 4400/33266 - Loss: 5.3275\n",
      "Batch 4500/33266 - Loss: 5.1167\n",
      "Batch 4600/33266 - Loss: 5.2351\n",
      "Batch 4700/33266 - Loss: 5.2183\n",
      "Batch 4800/33266 - Loss: 5.0930\n",
      "Batch 4900/33266 - Loss: 5.0306\n",
      "Batch 5000/33266 - Loss: 5.5261\n",
      "Batch 5100/33266 - Loss: 5.7092\n",
      "Batch 5200/33266 - Loss: 5.8981\n",
      "Batch 5300/33266 - Loss: 5.3928\n",
      "Batch 5400/33266 - Loss: 4.7973\n",
      "Batch 5500/33266 - Loss: 5.1518\n",
      "Batch 5600/33266 - Loss: 4.8559\n",
      "Batch 5700/33266 - Loss: 6.3391\n",
      "Batch 5800/33266 - Loss: 5.3425\n",
      "Batch 5900/33266 - Loss: 5.0656\n",
      "Batch 6000/33266 - Loss: 5.9761\n",
      "Batch 6100/33266 - Loss: 5.2252\n",
      "Batch 6200/33266 - Loss: 5.1177\n",
      "Batch 6300/33266 - Loss: 4.9031\n",
      "Batch 6400/33266 - Loss: 5.1666\n",
      "Batch 6500/33266 - Loss: 5.0896\n",
      "Batch 6600/33266 - Loss: 4.9666\n",
      "Batch 6700/33266 - Loss: 5.4438\n",
      "Batch 6800/33266 - Loss: 4.8637\n",
      "Batch 6900/33266 - Loss: 4.6188\n",
      "Batch 7000/33266 - Loss: 5.7294\n",
      "Batch 7100/33266 - Loss: 5.4286\n",
      "Batch 7200/33266 - Loss: 5.0366\n",
      "Batch 7300/33266 - Loss: 4.8344\n",
      "Batch 7400/33266 - Loss: 4.7574\n",
      "Batch 7500/33266 - Loss: 4.6973\n",
      "Batch 7600/33266 - Loss: 5.1413\n",
      "Batch 7700/33266 - Loss: 4.8341\n",
      "Batch 7800/33266 - Loss: 6.0383\n",
      "Batch 7900/33266 - Loss: 4.4321\n",
      "Batch 8000/33266 - Loss: 5.5566\n",
      "Batch 8100/33266 - Loss: 4.1759\n",
      "Batch 8200/33266 - Loss: 4.4813\n",
      "Batch 8300/33266 - Loss: 4.8629\n",
      "Batch 8400/33266 - Loss: 4.5626\n",
      "Batch 8500/33266 - Loss: 5.3841\n",
      "Batch 8600/33266 - Loss: 4.6528\n",
      "Batch 8700/33266 - Loss: 5.2755\n",
      "Batch 8800/33266 - Loss: 4.3647\n",
      "Batch 8900/33266 - Loss: 4.9445\n",
      "Batch 9000/33266 - Loss: 4.1803\n",
      "Batch 9100/33266 - Loss: 5.4052\n",
      "Batch 9200/33266 - Loss: 4.5618\n",
      "Batch 9300/33266 - Loss: 4.5552\n",
      "Batch 9400/33266 - Loss: 5.5408\n",
      "Batch 9500/33266 - Loss: 5.1460\n",
      "Batch 9600/33266 - Loss: 4.5498\n",
      "Batch 9700/33266 - Loss: 5.0316\n",
      "Batch 9800/33266 - Loss: 5.0452\n",
      "Batch 9900/33266 - Loss: 5.3746\n",
      "Batch 10000/33266 - Loss: 4.6481\n",
      "Batch 10100/33266 - Loss: 5.1540\n",
      "Batch 10200/33266 - Loss: 4.4006\n",
      "Batch 10300/33266 - Loss: 4.6448\n",
      "Batch 10400/33266 - Loss: 4.8185\n",
      "Batch 10500/33266 - Loss: 4.8562\n",
      "Batch 10600/33266 - Loss: 5.4934\n",
      "Batch 10700/33266 - Loss: 4.2330\n",
      "Batch 10800/33266 - Loss: 5.0893\n",
      "Batch 10900/33266 - Loss: 4.2054\n",
      "Batch 11000/33266 - Loss: 4.2888\n",
      "Batch 11100/33266 - Loss: 5.9249\n",
      "Batch 11200/33266 - Loss: 4.7505\n",
      "Batch 11300/33266 - Loss: 4.0647\n",
      "Batch 11400/33266 - Loss: 5.2136\n",
      "Batch 11500/33266 - Loss: 3.7803\n",
      "Batch 11600/33266 - Loss: 5.1869\n",
      "Batch 11700/33266 - Loss: 4.4215\n",
      "Batch 11800/33266 - Loss: 5.8088\n",
      "Batch 11900/33266 - Loss: 4.3178\n",
      "Batch 12000/33266 - Loss: 4.1515\n",
      "Batch 12100/33266 - Loss: 5.0245\n",
      "Batch 12200/33266 - Loss: 5.4094\n",
      "Batch 12300/33266 - Loss: 5.5038\n",
      "Batch 12400/33266 - Loss: 5.0863\n",
      "Batch 12500/33266 - Loss: 4.3810\n",
      "Batch 12600/33266 - Loss: 4.5812\n",
      "Batch 12700/33266 - Loss: 4.6105\n",
      "Batch 12800/33266 - Loss: 3.8589\n",
      "Batch 12900/33266 - Loss: 6.1425\n",
      "Batch 13000/33266 - Loss: 3.8066\n",
      "Batch 13100/33266 - Loss: 4.2351\n",
      "Batch 13200/33266 - Loss: 4.5833\n",
      "Batch 13300/33266 - Loss: 4.2852\n",
      "Batch 13400/33266 - Loss: 4.1323\n",
      "Batch 13500/33266 - Loss: 5.7407\n",
      "Batch 13600/33266 - Loss: 4.4800\n",
      "Batch 13700/33266 - Loss: 3.7078\n",
      "Batch 13800/33266 - Loss: 3.4125\n",
      "Batch 13900/33266 - Loss: 4.4853\n",
      "Batch 14000/33266 - Loss: 4.3335\n",
      "Batch 14100/33266 - Loss: 4.1109\n",
      "Batch 14200/33266 - Loss: 3.9883\n",
      "Batch 14300/33266 - Loss: 3.9653\n",
      "Batch 14400/33266 - Loss: 5.2154\n",
      "Batch 14500/33266 - Loss: 4.3175\n",
      "Batch 14600/33266 - Loss: 5.4086\n",
      "Batch 14700/33266 - Loss: 4.8271\n",
      "Batch 14800/33266 - Loss: 6.0086\n",
      "Batch 14900/33266 - Loss: 3.5555\n",
      "Batch 15000/33266 - Loss: 4.6535\n",
      "Batch 15100/33266 - Loss: 3.8412\n",
      "Batch 15200/33266 - Loss: 3.8333\n",
      "Batch 15300/33266 - Loss: 3.5921\n",
      "Batch 15400/33266 - Loss: 4.2289\n",
      "Batch 15500/33266 - Loss: 3.8843\n",
      "Batch 15600/33266 - Loss: 3.4569\n",
      "Batch 15700/33266 - Loss: 5.3978\n",
      "Batch 15800/33266 - Loss: 3.0660\n",
      "Batch 15900/33266 - Loss: 5.0264\n",
      "Batch 16000/33266 - Loss: 4.5776\n",
      "Batch 16100/33266 - Loss: 4.0998\n",
      "Batch 16200/33266 - Loss: 4.4768\n",
      "Batch 16300/33266 - Loss: 5.0496\n",
      "Batch 16400/33266 - Loss: 3.8810\n",
      "Batch 16500/33266 - Loss: 4.3422\n",
      "Batch 16600/33266 - Loss: 4.9451\n",
      "Batch 16700/33266 - Loss: 5.2204\n",
      "Batch 16800/33266 - Loss: 4.6950\n",
      "Batch 16900/33266 - Loss: 5.0993\n",
      "Batch 17000/33266 - Loss: 4.6659\n",
      "Batch 17100/33266 - Loss: 4.0153\n",
      "Batch 17200/33266 - Loss: 4.7819\n",
      "Batch 17300/33266 - Loss: 4.2658\n",
      "Batch 17400/33266 - Loss: 3.7114\n",
      "Batch 17500/33266 - Loss: 4.6079\n",
      "Batch 17600/33266 - Loss: 4.8000\n",
      "Batch 17700/33266 - Loss: 5.0859\n",
      "Batch 17800/33266 - Loss: 4.4061\n",
      "Batch 17900/33266 - Loss: 4.2886\n",
      "Batch 18000/33266 - Loss: 3.2138\n",
      "Batch 18100/33266 - Loss: 4.0271\n",
      "Batch 18200/33266 - Loss: 4.6446\n",
      "Batch 18300/33266 - Loss: 3.7706\n",
      "Batch 18400/33266 - Loss: 4.7983\n",
      "Batch 18500/33266 - Loss: 3.5656\n",
      "Batch 18600/33266 - Loss: 4.5711\n",
      "Batch 18700/33266 - Loss: 3.6740\n",
      "Batch 18800/33266 - Loss: 4.2053\n",
      "Batch 18900/33266 - Loss: 2.8906\n",
      "Batch 19000/33266 - Loss: 4.3853\n",
      "Batch 19100/33266 - Loss: 3.7011\n",
      "Batch 19200/33266 - Loss: 4.8757\n",
      "Batch 19300/33266 - Loss: 4.3975\n",
      "Batch 19400/33266 - Loss: 4.2949\n",
      "Batch 19500/33266 - Loss: 4.7105\n",
      "Batch 19600/33266 - Loss: 4.4971\n",
      "Batch 19700/33266 - Loss: 3.9354\n",
      "Batch 19800/33266 - Loss: 4.4281\n",
      "Batch 19900/33266 - Loss: 4.3868\n",
      "Batch 20000/33266 - Loss: 3.5845\n",
      "Batch 20100/33266 - Loss: 2.4904\n",
      "Batch 20200/33266 - Loss: 4.5337\n",
      "Batch 20300/33266 - Loss: 4.9815\n",
      "Batch 20400/33266 - Loss: 4.1484\n",
      "Batch 20500/33266 - Loss: 4.5510\n",
      "Batch 20600/33266 - Loss: 3.1121\n",
      "Batch 20700/33266 - Loss: 4.5199\n",
      "Batch 20800/33266 - Loss: 3.0317\n",
      "Batch 20900/33266 - Loss: 4.4116\n",
      "Batch 21000/33266 - Loss: 4.2578\n",
      "Batch 21100/33266 - Loss: 3.7864\n",
      "Batch 21200/33266 - Loss: 4.4395\n",
      "Batch 21300/33266 - Loss: 3.9796\n",
      "Batch 21400/33266 - Loss: 4.1444\n",
      "Batch 21500/33266 - Loss: 4.9070\n",
      "Batch 21600/33266 - Loss: 3.9066\n",
      "Batch 21700/33266 - Loss: 4.1370\n",
      "Batch 21800/33266 - Loss: 3.8498\n",
      "Batch 21900/33266 - Loss: 2.8835\n",
      "Batch 22000/33266 - Loss: 5.1905\n",
      "Batch 22100/33266 - Loss: 3.6422\n",
      "Batch 22200/33266 - Loss: 3.7276\n",
      "Batch 22300/33266 - Loss: 3.7537\n",
      "Batch 22400/33266 - Loss: 5.3437\n",
      "Batch 22500/33266 - Loss: 4.6122\n",
      "Batch 22600/33266 - Loss: 3.9800\n",
      "Batch 22700/33266 - Loss: 2.7570\n",
      "Batch 22800/33266 - Loss: 4.0051\n",
      "Batch 22900/33266 - Loss: 3.1108\n",
      "Batch 23000/33266 - Loss: 4.4285\n",
      "Batch 23100/33266 - Loss: 3.9274\n",
      "Batch 23200/33266 - Loss: 4.6021\n",
      "Batch 23300/33266 - Loss: 3.9103\n",
      "Batch 23400/33266 - Loss: 4.1949\n",
      "Batch 23500/33266 - Loss: 4.0632\n",
      "Batch 23600/33266 - Loss: 4.3681\n",
      "Batch 23700/33266 - Loss: 4.3355\n",
      "Batch 23800/33266 - Loss: 3.1346\n",
      "Batch 23900/33266 - Loss: 5.0669\n",
      "Batch 24000/33266 - Loss: 4.9711\n",
      "Batch 24100/33266 - Loss: 3.1368\n",
      "Batch 24200/33266 - Loss: 3.9228\n",
      "Batch 24300/33266 - Loss: 3.3223\n",
      "Batch 24400/33266 - Loss: 3.9458\n",
      "Batch 24500/33266 - Loss: 3.8800\n",
      "Batch 24600/33266 - Loss: 4.0092\n",
      "Batch 24700/33266 - Loss: 3.3291\n",
      "Batch 24800/33266 - Loss: 4.0785\n",
      "Batch 24900/33266 - Loss: 2.9680\n",
      "Batch 25000/33266 - Loss: 4.1317\n",
      "Batch 25100/33266 - Loss: 3.4548\n",
      "Batch 25200/33266 - Loss: 2.7869\n",
      "Batch 25300/33266 - Loss: 3.7979\n",
      "Batch 25400/33266 - Loss: 4.0043\n",
      "Batch 25500/33266 - Loss: 4.2140\n",
      "Batch 25600/33266 - Loss: 4.5097\n",
      "Batch 25700/33266 - Loss: 3.9252\n",
      "Batch 25800/33266 - Loss: 3.7887\n",
      "Batch 25900/33266 - Loss: 3.7451\n",
      "Batch 26000/33266 - Loss: 4.2032\n",
      "Batch 26100/33266 - Loss: 3.7433\n",
      "Batch 26200/33266 - Loss: 5.1716\n",
      "Batch 26300/33266 - Loss: 4.7967\n",
      "Batch 26400/33266 - Loss: 3.7495\n",
      "Batch 26500/33266 - Loss: 3.3747\n",
      "Batch 26600/33266 - Loss: 4.0147\n",
      "Batch 26700/33266 - Loss: 3.2216\n",
      "Batch 26800/33266 - Loss: 3.0907\n",
      "Batch 26900/33266 - Loss: 3.1976\n",
      "Batch 27000/33266 - Loss: 3.6027\n",
      "Batch 27100/33266 - Loss: 3.3241\n",
      "Batch 27200/33266 - Loss: 3.5510\n",
      "Batch 27300/33266 - Loss: 4.7882\n",
      "Batch 27400/33266 - Loss: 3.0103\n",
      "Batch 27500/33266 - Loss: 4.0238\n",
      "Batch 27600/33266 - Loss: 4.0328\n",
      "Batch 27700/33266 - Loss: 3.8695\n",
      "Batch 27800/33266 - Loss: 3.9663\n",
      "Batch 27900/33266 - Loss: 3.4728\n",
      "Batch 28000/33266 - Loss: 4.1686\n",
      "Batch 28100/33266 - Loss: 2.7097\n",
      "Batch 28200/33266 - Loss: 3.2380\n",
      "Batch 28300/33266 - Loss: 3.2572\n",
      "Batch 28400/33266 - Loss: 3.3992\n",
      "Batch 28500/33266 - Loss: 4.1329\n",
      "Batch 28600/33266 - Loss: 4.7049\n",
      "Batch 28700/33266 - Loss: 3.5707\n",
      "Batch 28800/33266 - Loss: 3.8655\n",
      "Batch 28900/33266 - Loss: 4.2714\n",
      "Batch 29000/33266 - Loss: 2.6656\n",
      "Batch 29100/33266 - Loss: 4.4692\n",
      "Batch 29200/33266 - Loss: 3.5652\n",
      "Batch 29300/33266 - Loss: 4.1863\n",
      "Batch 29400/33266 - Loss: 4.1076\n",
      "Batch 29500/33266 - Loss: 5.6849\n",
      "Batch 29600/33266 - Loss: 4.4027\n",
      "Batch 29700/33266 - Loss: 2.7001\n",
      "Batch 29800/33266 - Loss: 4.7466\n",
      "Batch 29900/33266 - Loss: 2.9333\n",
      "Batch 30000/33266 - Loss: 4.0224\n",
      "Batch 30100/33266 - Loss: 3.7753\n",
      "Batch 30200/33266 - Loss: 3.5964\n",
      "Batch 30300/33266 - Loss: 4.4378\n",
      "Batch 30400/33266 - Loss: 3.9412\n",
      "Batch 30500/33266 - Loss: 3.4480\n",
      "Batch 30600/33266 - Loss: 3.7300\n",
      "Batch 30700/33266 - Loss: 4.1584\n",
      "Batch 30800/33266 - Loss: 4.0348\n",
      "Batch 30900/33266 - Loss: 2.9859\n",
      "Batch 31000/33266 - Loss: 4.0990\n",
      "Batch 31100/33266 - Loss: 3.4624\n",
      "Batch 31200/33266 - Loss: 4.4864\n",
      "Batch 31300/33266 - Loss: 3.5909\n",
      "Batch 31400/33266 - Loss: 3.8617\n",
      "Batch 31500/33266 - Loss: 2.6276\n",
      "Batch 31600/33266 - Loss: 3.9063\n",
      "Batch 31700/33266 - Loss: 3.8467\n",
      "Batch 31800/33266 - Loss: 2.8090\n",
      "Batch 31900/33266 - Loss: 4.1608\n",
      "Batch 32000/33266 - Loss: 4.3763\n",
      "Batch 32100/33266 - Loss: 3.1793\n",
      "Batch 32200/33266 - Loss: 3.0790\n",
      "Batch 32300/33266 - Loss: 3.1710\n",
      "Batch 32400/33266 - Loss: 4.6785\n",
      "Batch 32500/33266 - Loss: 3.4673\n",
      "Batch 32600/33266 - Loss: 3.7445\n",
      "Batch 32700/33266 - Loss: 3.4031\n",
      "Batch 32800/33266 - Loss: 2.6746\n",
      "Batch 32900/33266 - Loss: 3.0234\n",
      "Batch 33000/33266 - Loss: 4.5992\n",
      "Batch 33100/33266 - Loss: 3.5419\n",
      "Batch 33200/33266 - Loss: 4.2512\n",
      "End of Epoch 1 - Avg Loss: 4.4769\n",
      "Epoch 2/2\n",
      "Batch 0/33266 - Loss: 4.2512\n",
      "Batch 100/33266 - Loss: 3.3244\n",
      "Batch 200/33266 - Loss: 5.5101\n",
      "Batch 300/33266 - Loss: 3.7010\n",
      "Batch 400/33266 - Loss: 2.8420\n",
      "Batch 500/33266 - Loss: 3.8399\n",
      "Batch 600/33266 - Loss: 3.1281\n",
      "Batch 700/33266 - Loss: 3.5935\n",
      "Batch 800/33266 - Loss: 2.6578\n",
      "Batch 900/33266 - Loss: 3.9879\n",
      "Batch 1000/33266 - Loss: 4.1459\n",
      "Batch 1100/33266 - Loss: 3.5390\n",
      "Batch 1200/33266 - Loss: 5.0442\n",
      "Batch 1300/33266 - Loss: 2.7790\n",
      "Batch 1400/33266 - Loss: 3.1313\n",
      "Batch 1500/33266 - Loss: 3.7094\n",
      "Batch 1600/33266 - Loss: 2.8959\n",
      "Batch 1700/33266 - Loss: 3.1525\n",
      "Batch 1800/33266 - Loss: 3.6002\n",
      "Batch 1900/33266 - Loss: 2.9278\n",
      "Batch 2000/33266 - Loss: 4.4629\n",
      "Batch 2100/33266 - Loss: 2.6107\n",
      "Batch 2200/33266 - Loss: 4.4832\n",
      "Batch 2300/33266 - Loss: 4.1575\n",
      "Batch 2400/33266 - Loss: 3.4802\n",
      "Batch 2500/33266 - Loss: 3.6071\n",
      "Batch 2600/33266 - Loss: 2.8290\n",
      "Batch 2700/33266 - Loss: 3.3719\n",
      "Batch 2800/33266 - Loss: 2.7764\n",
      "Batch 2900/33266 - Loss: 3.9324\n",
      "Batch 3000/33266 - Loss: 3.7244\n",
      "Batch 3100/33266 - Loss: 3.7348\n",
      "Batch 3200/33266 - Loss: 4.3014\n",
      "Batch 3300/33266 - Loss: 4.0706\n",
      "Batch 3400/33266 - Loss: 3.9373\n",
      "Batch 3500/33266 - Loss: 3.1874\n",
      "Batch 3600/33266 - Loss: 3.9465\n",
      "Batch 3700/33266 - Loss: 3.2670\n",
      "Batch 3800/33266 - Loss: 3.5322\n",
      "Batch 3900/33266 - Loss: 3.3868\n",
      "Batch 4000/33266 - Loss: 2.8973\n",
      "Batch 4100/33266 - Loss: 3.1377\n",
      "Batch 4200/33266 - Loss: 3.7823\n",
      "Batch 4300/33266 - Loss: 3.6606\n",
      "Batch 4400/33266 - Loss: 2.7615\n",
      "Batch 4500/33266 - Loss: 3.2922\n",
      "Batch 4600/33266 - Loss: 2.9936\n",
      "Batch 4700/33266 - Loss: 4.7306\n",
      "Batch 4800/33266 - Loss: 2.8205\n",
      "Batch 4900/33266 - Loss: 3.3535\n",
      "Batch 5000/33266 - Loss: 3.7682\n",
      "Batch 5100/33266 - Loss: 4.0891\n",
      "Batch 5200/33266 - Loss: 3.8238\n",
      "Batch 5300/33266 - Loss: 2.7501\n",
      "Batch 5400/33266 - Loss: 3.2538\n",
      "Batch 5500/33266 - Loss: 4.3641\n",
      "Batch 5600/33266 - Loss: 4.4501\n",
      "Batch 5700/33266 - Loss: 3.6142\n",
      "Batch 5800/33266 - Loss: 3.1178\n",
      "Batch 5900/33266 - Loss: 4.2399\n",
      "Batch 6000/33266 - Loss: 3.6664\n",
      "Batch 6100/33266 - Loss: 3.5829\n",
      "Batch 6200/33266 - Loss: 3.8116\n",
      "Batch 6300/33266 - Loss: 4.5293\n",
      "Batch 6400/33266 - Loss: 3.5207\n",
      "Batch 6500/33266 - Loss: 3.2525\n",
      "Batch 6600/33266 - Loss: 2.9350\n",
      "Batch 6700/33266 - Loss: 2.2124\n",
      "Batch 6800/33266 - Loss: 2.4979\n",
      "Batch 6900/33266 - Loss: 3.1909\n",
      "Batch 7000/33266 - Loss: 4.4852\n",
      "Batch 7100/33266 - Loss: 2.9757\n",
      "Batch 7200/33266 - Loss: 2.9686\n",
      "Batch 7300/33266 - Loss: 3.3195\n",
      "Batch 7400/33266 - Loss: 3.3453\n",
      "Batch 7500/33266 - Loss: 3.7022\n",
      "Batch 7600/33266 - Loss: 3.1226\n",
      "Batch 7700/33266 - Loss: 3.6116\n",
      "Batch 7800/33266 - Loss: 4.1223\n",
      "Batch 7900/33266 - Loss: 3.3845\n",
      "Batch 8000/33266 - Loss: 4.0101\n",
      "Batch 8100/33266 - Loss: 2.4417\n",
      "Batch 8200/33266 - Loss: 3.2204\n",
      "Batch 8300/33266 - Loss: 3.3111\n",
      "Batch 8400/33266 - Loss: 4.0225\n",
      "Batch 8500/33266 - Loss: 4.1002\n",
      "Batch 8600/33266 - Loss: 3.1211\n",
      "Batch 8700/33266 - Loss: 3.4977\n",
      "Batch 8800/33266 - Loss: 2.6448\n",
      "Batch 8900/33266 - Loss: 3.4294\n",
      "Batch 9000/33266 - Loss: 4.2368\n",
      "Batch 9100/33266 - Loss: 3.1086\n",
      "Batch 9200/33266 - Loss: 3.8282\n",
      "Batch 9300/33266 - Loss: 3.4999\n",
      "Batch 9400/33266 - Loss: 4.3517\n",
      "Batch 9500/33266 - Loss: 2.6648\n",
      "Batch 9600/33266 - Loss: 1.9395\n",
      "Batch 9700/33266 - Loss: 3.4006\n",
      "Batch 9800/33266 - Loss: 3.6096\n",
      "Batch 9900/33266 - Loss: 2.7679\n",
      "Batch 10000/33266 - Loss: 3.1261\n",
      "Batch 10100/33266 - Loss: 3.3046\n",
      "Batch 10200/33266 - Loss: 3.6061\n",
      "Batch 10300/33266 - Loss: 3.2740\n",
      "Batch 10400/33266 - Loss: 3.2702\n",
      "Batch 10500/33266 - Loss: 4.2660\n",
      "Batch 10600/33266 - Loss: 3.2495\n",
      "Batch 10700/33266 - Loss: 3.6649\n",
      "Batch 10800/33266 - Loss: 3.4634\n",
      "Batch 10900/33266 - Loss: 3.6068\n",
      "Batch 11000/33266 - Loss: 3.5028\n",
      "Batch 11100/33266 - Loss: 3.8587\n",
      "Batch 11200/33266 - Loss: 3.3099\n",
      "Batch 11300/33266 - Loss: 3.2844\n",
      "Batch 11400/33266 - Loss: 2.9759\n",
      "Batch 11500/33266 - Loss: 3.4918\n",
      "Batch 11600/33266 - Loss: 3.0511\n",
      "Batch 11700/33266 - Loss: 2.4156\n",
      "Batch 11800/33266 - Loss: 3.1610\n",
      "Batch 11900/33266 - Loss: 3.1791\n",
      "Batch 12000/33266 - Loss: 4.2503\n",
      "Batch 12100/33266 - Loss: 3.2256\n",
      "Batch 12200/33266 - Loss: 3.2251\n",
      "Batch 12300/33266 - Loss: 3.1789\n",
      "Batch 12400/33266 - Loss: 3.4597\n",
      "Batch 12500/33266 - Loss: 3.7460\n",
      "Batch 12600/33266 - Loss: 2.9115\n",
      "Batch 12700/33266 - Loss: 2.5665\n",
      "Batch 12800/33266 - Loss: 2.8835\n",
      "Batch 12900/33266 - Loss: 3.6349\n",
      "Batch 13000/33266 - Loss: 2.5187\n",
      "Batch 13100/33266 - Loss: 2.5472\n",
      "Batch 13200/33266 - Loss: 5.2902\n",
      "Batch 13300/33266 - Loss: 2.9692\n",
      "Batch 13400/33266 - Loss: 3.1096\n",
      "Batch 13500/33266 - Loss: 3.5300\n",
      "Batch 13600/33266 - Loss: 2.8035\n",
      "Batch 13700/33266 - Loss: 3.2632\n",
      "Batch 13800/33266 - Loss: 3.9149\n",
      "Batch 13900/33266 - Loss: 3.6204\n",
      "Batch 14000/33266 - Loss: 3.2633\n",
      "Batch 14100/33266 - Loss: 2.9251\n",
      "Batch 14200/33266 - Loss: 2.9604\n",
      "Batch 14300/33266 - Loss: 3.1739\n",
      "Batch 14400/33266 - Loss: 5.0089\n",
      "Batch 14500/33266 - Loss: 2.7014\n",
      "Batch 14600/33266 - Loss: 3.1245\n",
      "Batch 14700/33266 - Loss: 3.3687\n",
      "Batch 14800/33266 - Loss: 3.3033\n",
      "Batch 14900/33266 - Loss: 2.5519\n",
      "Batch 15000/33266 - Loss: 3.7523\n",
      "Batch 15100/33266 - Loss: 4.0454\n",
      "Batch 15200/33266 - Loss: 2.7663\n",
      "Batch 15300/33266 - Loss: 3.5987\n",
      "Batch 15400/33266 - Loss: 2.3461\n",
      "Batch 15500/33266 - Loss: 2.5224\n",
      "Batch 15600/33266 - Loss: 2.4761\n",
      "Batch 15700/33266 - Loss: 2.6540\n",
      "Batch 15800/33266 - Loss: 4.1067\n",
      "Batch 15900/33266 - Loss: 3.7253\n",
      "Batch 16000/33266 - Loss: 4.4283\n",
      "Batch 16100/33266 - Loss: 2.6977\n",
      "Batch 16200/33266 - Loss: 2.7528\n",
      "Batch 16300/33266 - Loss: 2.5235\n",
      "Batch 16400/33266 - Loss: 3.7821\n",
      "Batch 16500/33266 - Loss: 4.2161\n",
      "Batch 16600/33266 - Loss: 3.6516\n",
      "Batch 16700/33266 - Loss: 3.2391\n",
      "Batch 16800/33266 - Loss: 3.3311\n",
      "Batch 16900/33266 - Loss: 4.2765\n",
      "Batch 17000/33266 - Loss: 2.6369\n",
      "Batch 17100/33266 - Loss: 3.9147\n",
      "Batch 17200/33266 - Loss: 3.4421\n",
      "Batch 17300/33266 - Loss: 4.1124\n",
      "Batch 17400/33266 - Loss: 3.2527\n",
      "Batch 17500/33266 - Loss: 1.7901\n",
      "Batch 17600/33266 - Loss: 2.3826\n",
      "Batch 17700/33266 - Loss: 3.7381\n",
      "Batch 17800/33266 - Loss: 3.4891\n",
      "Batch 17900/33266 - Loss: 2.9223\n",
      "Batch 18000/33266 - Loss: 2.7489\n",
      "Batch 18100/33266 - Loss: 2.8127\n",
      "Batch 18200/33266 - Loss: 4.1986\n",
      "Batch 18300/33266 - Loss: 3.5988\n",
      "Batch 18400/33266 - Loss: 2.6746\n",
      "Batch 18500/33266 - Loss: 3.4755\n",
      "Batch 18600/33266 - Loss: 4.2796\n",
      "Batch 18700/33266 - Loss: 3.1193\n",
      "Batch 18800/33266 - Loss: 3.0834\n",
      "Batch 18900/33266 - Loss: 3.3852\n",
      "Batch 19000/33266 - Loss: 3.6255\n",
      "Batch 19100/33266 - Loss: 4.1166\n",
      "Batch 19200/33266 - Loss: 3.1697\n",
      "Batch 19300/33266 - Loss: 3.4145\n",
      "Batch 19400/33266 - Loss: 3.5460\n",
      "Batch 19500/33266 - Loss: 2.3424\n",
      "Batch 19600/33266 - Loss: 2.2514\n",
      "Batch 19700/33266 - Loss: 4.0143\n",
      "Batch 19800/33266 - Loss: 2.8787\n",
      "Batch 19900/33266 - Loss: 2.4280\n",
      "Batch 20000/33266 - Loss: 2.8041\n",
      "Batch 20100/33266 - Loss: 3.7328\n",
      "Batch 20200/33266 - Loss: 2.7188\n",
      "Batch 20300/33266 - Loss: 3.2487\n",
      "Batch 20400/33266 - Loss: 4.2294\n",
      "Batch 20500/33266 - Loss: 3.2484\n",
      "Batch 20600/33266 - Loss: 4.3698\n",
      "Batch 20700/33266 - Loss: 2.5107\n",
      "Batch 20800/33266 - Loss: 3.6662\n",
      "Batch 20900/33266 - Loss: 4.4182\n",
      "Batch 21000/33266 - Loss: 2.7067\n",
      "Batch 21100/33266 - Loss: 4.5053\n",
      "Batch 21200/33266 - Loss: 2.1420\n",
      "Batch 21300/33266 - Loss: 3.4325\n",
      "Batch 21400/33266 - Loss: 3.5905\n",
      "Batch 21500/33266 - Loss: 2.0966\n",
      "Batch 21600/33266 - Loss: 3.2598\n",
      "Batch 21700/33266 - Loss: 3.0953\n",
      "Batch 21800/33266 - Loss: 4.4855\n",
      "Batch 21900/33266 - Loss: 2.6490\n",
      "Batch 22000/33266 - Loss: 2.6673\n",
      "Batch 22100/33266 - Loss: 3.7201\n",
      "Batch 22200/33266 - Loss: 3.4110\n",
      "Batch 22300/33266 - Loss: 3.9044\n",
      "Batch 22400/33266 - Loss: 3.1149\n",
      "Batch 22500/33266 - Loss: 4.2822\n",
      "Batch 22600/33266 - Loss: 2.7678\n",
      "Batch 22700/33266 - Loss: 4.0992\n",
      "Batch 22800/33266 - Loss: 3.3347\n",
      "Batch 22900/33266 - Loss: 3.0883\n",
      "Batch 23000/33266 - Loss: 2.9064\n",
      "Batch 23100/33266 - Loss: 2.9788\n",
      "Batch 23200/33266 - Loss: 2.0643\n",
      "Batch 23300/33266 - Loss: 3.6093\n",
      "Batch 23400/33266 - Loss: 2.1845\n",
      "Batch 23500/33266 - Loss: 2.3766\n",
      "Batch 23600/33266 - Loss: 3.5146\n",
      "Batch 23700/33266 - Loss: 2.6296\n",
      "Batch 23800/33266 - Loss: 3.7824\n",
      "Batch 23900/33266 - Loss: 3.9140\n",
      "Batch 24000/33266 - Loss: 2.8170\n",
      "Batch 24100/33266 - Loss: 2.4890\n",
      "Batch 24200/33266 - Loss: 3.0707\n",
      "Batch 24300/33266 - Loss: 2.4126\n",
      "Batch 24400/33266 - Loss: 2.9238\n",
      "Batch 24500/33266 - Loss: 3.2914\n",
      "Batch 24600/33266 - Loss: 3.5639\n",
      "Batch 24700/33266 - Loss: 2.4436\n",
      "Batch 24800/33266 - Loss: 1.8938\n",
      "Batch 24900/33266 - Loss: 3.6021\n",
      "Batch 25000/33266 - Loss: 3.6931\n",
      "Batch 25100/33266 - Loss: 3.0639\n",
      "Batch 25200/33266 - Loss: 3.7878\n",
      "Batch 25300/33266 - Loss: 2.5699\n",
      "Batch 25400/33266 - Loss: 3.5339\n",
      "Batch 25500/33266 - Loss: 3.2771\n",
      "Batch 25600/33266 - Loss: 2.9204\n",
      "Batch 25700/33266 - Loss: 2.6753\n",
      "Batch 25800/33266 - Loss: 3.6282\n",
      "Batch 25900/33266 - Loss: 2.3605\n",
      "Batch 26000/33266 - Loss: 3.5607\n",
      "Batch 26100/33266 - Loss: 3.3652\n",
      "Batch 26200/33266 - Loss: 2.4182\n",
      "Batch 26300/33266 - Loss: 3.1297\n",
      "Batch 26400/33266 - Loss: 3.2077\n",
      "Batch 26500/33266 - Loss: 3.2733\n",
      "Batch 26600/33266 - Loss: 3.1533\n",
      "Batch 26700/33266 - Loss: 2.9704\n",
      "Batch 26800/33266 - Loss: 4.3376\n",
      "Batch 26900/33266 - Loss: 2.6845\n",
      "Batch 27000/33266 - Loss: 3.9499\n",
      "Batch 27100/33266 - Loss: 3.2503\n",
      "Batch 27200/33266 - Loss: 2.8427\n",
      "Batch 27300/33266 - Loss: 2.5486\n",
      "Batch 27400/33266 - Loss: 3.5972\n",
      "Batch 27500/33266 - Loss: 3.6543\n",
      "Batch 27600/33266 - Loss: 3.3523\n",
      "Batch 27700/33266 - Loss: 3.1898\n",
      "Batch 27800/33266 - Loss: 2.4554\n",
      "Batch 27900/33266 - Loss: 4.2816\n",
      "Batch 28000/33266 - Loss: 4.4562\n",
      "Batch 28100/33266 - Loss: 1.8860\n",
      "Batch 28200/33266 - Loss: 3.4804\n",
      "Batch 28300/33266 - Loss: 2.7594\n",
      "Batch 28400/33266 - Loss: 2.4654\n",
      "Batch 28500/33266 - Loss: 1.5487\n",
      "Batch 28600/33266 - Loss: 2.4993\n",
      "Batch 28700/33266 - Loss: 2.9801\n",
      "Batch 28800/33266 - Loss: 3.2617\n",
      "Batch 28900/33266 - Loss: 2.6745\n",
      "Batch 29000/33266 - Loss: 3.5986\n",
      "Batch 29100/33266 - Loss: 3.1429\n",
      "Batch 29200/33266 - Loss: 2.5423\n",
      "Batch 29300/33266 - Loss: 2.8467\n",
      "Batch 29400/33266 - Loss: 3.1509\n",
      "Batch 29500/33266 - Loss: 2.7556\n",
      "Batch 29600/33266 - Loss: 3.6188\n",
      "Batch 29700/33266 - Loss: 2.8943\n",
      "Batch 29800/33266 - Loss: 2.8716\n",
      "Batch 29900/33266 - Loss: 3.1239\n",
      "Batch 30000/33266 - Loss: 3.6688\n",
      "Batch 30100/33266 - Loss: 4.2156\n",
      "Batch 30200/33266 - Loss: 2.1599\n",
      "Batch 30300/33266 - Loss: 3.3416\n",
      "Batch 30400/33266 - Loss: 2.2538\n",
      "Batch 30500/33266 - Loss: 2.9186\n",
      "Batch 30600/33266 - Loss: 3.5388\n",
      "Batch 30700/33266 - Loss: 3.5926\n",
      "Batch 30800/33266 - Loss: 3.5566\n",
      "Batch 30900/33266 - Loss: 2.5231\n",
      "Batch 31000/33266 - Loss: 3.4954\n",
      "Batch 31100/33266 - Loss: 3.0878\n",
      "Batch 31200/33266 - Loss: 3.9850\n",
      "Batch 31300/33266 - Loss: 3.5618\n",
      "Batch 31400/33266 - Loss: 2.2646\n",
      "Batch 31500/33266 - Loss: 3.0573\n",
      "Batch 31600/33266 - Loss: 3.1530\n",
      "Batch 31700/33266 - Loss: 3.5295\n",
      "Batch 31800/33266 - Loss: 2.6930\n",
      "Batch 31900/33266 - Loss: 2.6409\n",
      "Batch 32000/33266 - Loss: 2.3609\n",
      "Batch 32100/33266 - Loss: 3.3305\n",
      "Batch 32200/33266 - Loss: 3.0395\n",
      "Batch 32300/33266 - Loss: 1.8596\n",
      "Batch 32400/33266 - Loss: 3.5967\n",
      "Batch 32500/33266 - Loss: 3.5422\n",
      "Batch 32600/33266 - Loss: 3.8453\n",
      "Batch 32700/33266 - Loss: 2.1611\n",
      "Batch 32800/33266 - Loss: 3.7025\n",
      "Batch 32900/33266 - Loss: 3.4809\n",
      "Batch 33000/33266 - Loss: 3.1461\n",
      "Batch 33100/33266 - Loss: 4.2457\n",
      "Batch 33200/33266 - Loss: 2.9945\n",
      "End of Epoch 2 - Avg Loss: 3.2997\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función 'train' para entrenar el modelo. El entrenamiento se realiza utilizando los siguientes parámetros:\n",
    "# - model: El modelo Transformer previamente definido que se entrenará.\n",
    "# - dataloader: El DataLoader que proporciona los datos en mini-lotes (batches) durante el entrenamiento.\n",
    "# - loss_function: La función de pérdida que se utilizará para calcular la diferencia entre las predicciones del modelo y las etiquetas verdaderas.\n",
    "# - optimiser: El optimizador que actualiza los parámetros del modelo en función de la retropropagación (backpropagation).\n",
    "# - epochs: El número de épocas de entrenamiento. En este caso, entrenaremos el modelo durante 10 épocas.\n",
    "# Durante cada época, el modelo será alimentado con los datos de entrada, y se calcularán y actualizarán los parámetros del modelo en base a la pérdida.\n",
    "\n",
    "train(model, dataloader, loss_function, optimiser, epochs=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b4ffe033-ab15-4ead-b191-1b299e68d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función convierte una oración (sentence) en una lista de índices usando el diccionario 'word2idx'.\n",
    "# La función toma cada palabra en la oración, la busca en el diccionario 'word2idx', y devuelve su índice correspondiente.\n",
    "# Si una palabra no está en el diccionario, se utiliza el índice de la palabra desconocida ('<unk>').\n",
    "def sentence_to_indices(sentence, word2idx):\n",
    "    return [word2idx.get(word, word2idx['<unk>']) for word in sentence.split()]\n",
    "\n",
    "# Esta función convierte una lista de índices (indices) en una oración (sentence) usando el diccionario 'idx2word'.\n",
    "# La función toma cada índice en la lista y lo busca en 'idx2word' para obtener la palabra correspondiente.\n",
    "# Los índices que corresponden al token de relleno ('<pad>') se ignoran.\n",
    "def indices_to_sentence(indices, idx2word):\n",
    "    return ' '.join([idx2word[idx] for idx in indices if idx in idx2word and idx2word[idx] != '<pad>'])\n",
    "\n",
    "# Esta función realiza la traducción de una oración de inglés a español utilizando el modelo Transformer.\n",
    "# La función toma como entrada la oración en inglés, las palabras en inglés y español mapeadas a índices ('eng_word2idx' y 'spa_idx2word'),\n",
    "# y produce la traducción en español.\n",
    "# La traducción se realiza en modo de evaluación (sin entrenamiento) y sigue los siguientes pasos:\n",
    "# 1. Preprocesar la oración de entrada en inglés.\n",
    "# 2. Convertir la oración de inglés en una lista de índices (sentence_to_indices).\n",
    "# 3. Inicializar el tensor de la entrada y el tensor de destino con el token <sos> (Start of Sentence).\n",
    "# 4. Realizar una inferencia paso a paso para generar cada palabra en la traducción, deteniéndose cuando se alcanza el token <eos> (End of Sentence).\n",
    "# 5. Convertir los índices generados a palabras en español utilizando el diccionario 'spa_idx2word'.\n",
    "def translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n",
    "    model.eval()  # Ponemos el modelo en modo de evaluación (sin actualizaciones de parámetros).\n",
    "    \n",
    "    # Preprocesamos la oración de entrada (minúsculas, eliminación de espacios innecesarios, etc.).\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    # Convertimos la oración de entrada en índices utilizando el diccionario de inglés.\n",
    "    input_indices = sentence_to_indices(sentence, eng_word2idx)\n",
    "    \n",
    "    # Convertimos los índices a un tensor y lo movemos al dispositivo (CPU o GPU).\n",
    "    input_tensor = torch.tensor(input_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    # Inicializamos el tensor de destino con el token de inicio de oración (<sos>).\n",
    "    tgt_indices = [spa_word2idx['<sos>']]\n",
    "    tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():  # Desactivamos el cálculo de gradientes, ya que estamos en modo de inferencia.\n",
    "        # Generamos la traducción palabra por palabra, hasta alcanzar el máximo de longitud o el token <eos>.\n",
    "        for _ in range(max_len):\n",
    "            # Ejecutamos el modelo para predecir la siguiente palabra en la secuencia.\n",
    "            output = model(input_tensor, tgt_tensor)\n",
    "            \n",
    "            # Eliminamos la dimensión adicional de la salida y obtenemos la predicción para el siguiente token.\n",
    "            output = output.squeeze(0)\n",
    "            \n",
    "            # Elegimos el token con la probabilidad más alta (máximo valor en la última dimensión).\n",
    "            next_token = output.argmax(dim=-1)[-1].item()\n",
    "            \n",
    "            # Añadimos el siguiente token al tensor de destino.\n",
    "            tgt_indices.append(next_token)\n",
    "            \n",
    "            # Actualizamos el tensor de destino con la secuencia generada hasta ahora.\n",
    "            tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Si el token de salida es <eos>, terminamos la traducción.\n",
    "            if next_token == spa_word2idx['<eos>']:\n",
    "                break\n",
    "\n",
    "    # Convertimos los índices generados a palabras en español y devolvemos la traducción como una cadena de texto.\n",
    "    return indices_to_sentence(tgt_indices, spa_idx2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a16f7ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función evalúa las traducciones de un conjunto de oraciones de prueba utilizando el modelo Transformer.\n",
    "# Toma un modelo entrenado, un conjunto de oraciones en inglés, y los diccionarios que mapean palabras a índices para inglés y español.\n",
    "# La función imprime las traducciones generadas para cada oración de prueba.\n",
    "# Para cada oración:\n",
    "# 1. Se traduce utilizando la función 'translate_sentence'.\n",
    "# 2. Se imprime la oración original (en inglés) y la traducción generada (en español).\n",
    "def evaluate_translations(model, sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n",
    "    for sentence in sentences:\n",
    "        # Se traduce cada oración del conjunto de oraciones de prueba utilizando la función 'translate_sentence'.\n",
    "        translation = translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len, device)\n",
    "        \n",
    "        # Imprime la oración original en inglés y su traducción en español.\n",
    "        print(f'Input sentence: {sentence}')\n",
    "        print(f'Traducción: {translation}')\n",
    "        print()\n",
    "\n",
    "# Estas son algunas oraciones de ejemplo para evaluar el modelo.\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",  # Ejemplo 1\n",
    "    \"I am learning artificial intelligence.\",  # Ejemplo 2\n",
    "    \"Artificial intelligence is great.\",  # Ejemplo 3\n",
    "    \"Good night!\"  # Ejemplo 4\n",
    "]\n",
    "\n",
    "# Se asume que el modelo ha sido entrenado y cargado previamente.\n",
    "# Definimos el dispositivo de ejecución del modelo, que puede ser 'cuda' si hay una GPU disponible, o 'cpu' si no la hay.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Mover el modelo al dispositivo adecuado (CPU o GPU).\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a748f7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Hello, how are you?\n",
      "Traducción: <sos> como estas hola <eos>\n",
      "\n",
      "Input sentence: I am learning artificial intelligence.\n",
      "Traducción: <sos> estoy aprendiendo inteligencia inteligencia inteligencia <eos>\n",
      "\n",
      "Input sentence: Artificial intelligence is great.\n",
      "Traducción: <sos> la inteligencia es muy grande <eos>\n",
      "\n",
      "Input sentence: Good night!\n",
      "Traducción: <sos> buenas noches <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate translations\n",
    "evaluate_translations(model, test_sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b3347",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291f357",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
